<!doctype html>
<html>
    <head>
        <title>MB500_SKIP1</title>
        <meta charset='utf-8'/>
        <style>
body {
  background-color: black;
  color: white;
  margin-left: 25%;
  margin-right: 25%;
  font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
}

div{
  color: white;
}

img{
  margin-top: auto;
  margin-bottom: auto;
  max-width: 80%;

}

p{
  font-size: large;
  /* display: inline; */
}

li{
font-size: large;
}

.scrollmenu {
  overflow: auto;
  max-height: 55vh;
  white-space: nowrap;
  text-align: left;
}

table {
  border-collapse: collapse;
  width: 100%;
}

/* Style for table header cells */
th {
  border: 1px solid #ccc;
  padding: 8px;
  text-align: center;
}


/* Style for table body cells */
td {
  border: 1px solid #ccc;
  padding: 8px;
  text-align: center;
}
</style>
    </head>
    <body>
<p>Okay, let's break down these concepts from the text. It's all about creating a solid foundation for developing within the Dynamics 365 for Finance and Operations ecosystem.</p>
<p>First, we talked about VHDs, or Virtual Hard Disks. Think of a VHD as a completely separate computer within your computer. It's a file that acts like a physical hard drive, allowing you to install an entire operating system and applications, like Dynamics 365, inside it. This is super helpful because it lets you create a dedicated environment for development or testing without messing with your main system. It keeps things clean and contained. You can download these VHDs directly from Microsoft's Lifecycle Services (LCS) portal. LCS, in general, is a crucial cloud-based tool that helps manage the entire application lifecycle of Dynamics 365 projects. It's where you handle deployments, updates, and, in this case, get those handy VHDs.</p>
<p>To get a VHD up and running, we dive into the world of Hyper-V. This is Microsoft's built-in technology for creating and managing virtual machines on Windows. Basically, Hyper-V is what allows your computer to host that separate virtual environment within the VHD. You'll need to enable it in your Windows settings, and then, through the Hyper-V Manager, you can create a new virtual machine, pointing it to your downloaded VHD.</p>
<p>When setting up your virtual machine within Hyper-V, there are a few key things to consider. You'll give it a name, choose a generation (usually Generation 1 is fine), and allocate memory. Memory is especially important for Dynamics 365 development. The text recommends at least 8GB, but honestly, more is better to keep things running smoothly. You also set up networking, typically using the "Default Switch" option, so your virtual machine can access the internet.</p>
<p>Once your VHD environment is up and running, the document talks about essential tools like Visual Studio and SQL Server Management Studio. Visual Studio is your primary development environment for Dynamics 365. It's where you'll write code, design forms, and manage your projects. SQL Server Management Studio, on the other hand, is for interacting with the database. You can use it to query data, troubleshoot issues, and even restore databases from your production or testing environments. By the way, when restoring databases, it is best practice to restore them to a non-production environment like a Tier 2 environment first. This avoids any issues with your live production data.</p>
<p>Updating your VHD is another crucial aspect. Just like any software, Dynamics 365 receives updates and new versions. To keep your development environment current, you can download update packages from Lifecycle Services and apply them to your VHD. The process is similar to deploying code packages in other Dynamics 365 development environments, so if you're familiar with that, you'll feel right at home. This will usually involve running a specific command (AXUpdateInstaller.exe devinstall) and then compiling and syncing within Visual Studio.</p>
<p>Now, let's shift gears and talk about how you organize your development work in Visual Studio. This is where projects and solutions come into play. A project is like a folder that holds all the related code, forms, and other elements for a specific part of your Dynamics 365 customization. A solution, in turn, is a container for multiple projects. Think of it as a master folder that groups together all the projects related to a larger application or feature set.</p>
<p>When you're building your code, Visual Studio offers a few options: "build," "rebuild," and "clean." A "build" compiles only the changes you've made since the last build, making it faster. A "rebuild" compiles everything from scratch, which can be helpful if you're running into issues or want to ensure a clean slate. "Clean" removes any previous build outputs, getting your project ready for a fresh build.</p>
<p>Labels are another important concept for Dynamics 365 development, especially if you're working with multiple languages. A label is basically a placeholder for text that appears in the user interface. Instead of hardcoding text directly into your code or forms, you create labels in separate label files. Each label has an ID and a corresponding text value for each language you support.</p>
<p>Label files are usually organized by module or area within Dynamics 365, such as one for Accounts Payable, one for Accounts Receivable, and so on. The text recommends creating one label file for each language you're using. To add labels, you can use the Label File Wizard in Visual Studio, which guides you through the process. The naming convention for label files and IDs is up to you and your team's development policies, but clarity is key.</p>
<p>Using labels in your code or forms is straightforward. You can either copy and paste them directly from the label file or use a special syntax (like @MyLabelFile:CustName) to reference them. The language used for a label will typically depend on the user's settings, but you can also specify a particular language if needed.</p>
<p>Finally, the Application Explorer in Visual Studio is your central hub for managing all the elements in your Dynamics 365 development environment. It's where you can browse, search, and modify things like tables, forms, classes, and, of course, label files.</p>
<p>Alright, let's continue our conversation and explore some more foundational concepts in Dynamics 365 development. We're moving now into the core structures and building blocks you'll be using within Visual Studio, specifically within the Application Object Tree (AOT).</p>
<p>Think of the AOT as the central repository for all the objects that make up your Dynamics 365 application. It's where you'll find everything from tables and forms to classes and workflows, all organized in a hierarchical structure. This is where metadata comes into play. Metadata is essentially data that describes other data. In the context of Dynamics 365, metadata defines the structure and behavior of your application's objects.</p>
<p>The text highlights several key metadata types. Extended Data Types (EDTs) are like customized data types built upon the basic ones. For instance, you could create an EDT called "Color" based on the "string" data type. This gives you a more descriptive way to represent colors in your code. You also mentioned Enums, which are perfect for representing a fixed set of values, like the days of the week. Tables are exactly what they sound like—structures for organizing data into rows and columns. Forms are the user interface elements that people interact with. Then you have Data Entities, which provide a simplified, abstract way of working with data, particularly useful for integrations and data import/export.</p>
<p>Now, within the Application Explorer, you have two primary ways of viewing the AOT: model view and classic view. Model view is useful when you're working with objects related to a specific model. If you recall, a model is a way of grouping related customizations and extensions. Classic view, on the other hand, organizes objects by their type, which is great when you know you need, say, a form but don't remember which model it's in.</p>
<p>If you need to find something specific in the vastness of the AOT, you can use filters. This involves entering the field name, a colon, a space, and then the value you're searching for. There's also a handy "Refresh" button to ensure you're seeing the most up-to-date information.</p>
<p>Now, when it comes to applying your code changes to a non-development environment, we get into the world of deployment packages. A deployment package is essentially a bundle of your customizations that can be reliably applied to other environments, like testing or production. You can create these packages directly within Visual Studio and then upload them to Lifecycle Services (LCS) for deployment. Azure DevOps is used in this process. This is Microsoft's cloud based platform for collaboration among developers. It integrates directly with Visual Studio and helps you manage things like builds, code repositories, and testing. When you queue a build in Azure DevOps, it will compile your code and, if successful, create a deployable package (or "AX Deployable Runtime") that you can then take to LCS.</p>
<p>Database synchronization is another critical step. When you add or modify elements, you need to synchronize the database so that it knows about these changes. You can do this directly from the Dynamics 365 menu in Visual Studio or set it to happen automatically after a successful build. Without synchronization, you might run into SQL errors or see inconsistencies between your code and the actual database structure.</p>
<p>The Element Designer is the tool within Visual Studio where you'll spend a lot of time creating and modifying elements. It provides a visual, hierarchical view of your element's structure and allows you to drag and drop components, set properties, and even write code. Forms, in particular, have a specialized layout in the Element Designer, including a Form Designer, a Normal Element Designer, and a Pattern Designer to help you maintain consistency with existing Dynamics 365 user interface patterns.</p>
<p>Now, let's talk about the basics of the X++ programming language. X++ is an object-oriented language, similar to C# or Java, and it's the primary language for developing in Dynamics 365 for Finance and Operations. One key difference from C# is that X++ is case-insensitive.</p>
<p>The text covered some fundamental data types in X++. We have the usual suspects like "boolean", "int", "real" (for decimal numbers), and "str" (for strings). There's also "int64" for larger integer values. Dates are handled by the "date" type, although the text recommends using "utcDateTime" instead, as it combines date, time, and time zone information. "GUID" is for globally unique identifiers. "TimeOfday" represents the number of seconds since midnight.</p>
<p>Composite data types include arrays (lists of items of the same type), containers (which can hold a mix of different types), classes (blueprints for objects), delegates (which allow you to call multiple subscriber methods at once), and even tables themselves, which can be used as data types within your code.</p>
<p>Extended Data Types (EDTs) are user-defined types built upon the basic ones, as we touched upon earlier. They offer a way to create more meaningful and reusable type definitions. For example, you could create an EDT called "VehicleColor" based on the "string" type and set properties like string length or a default value.</p>
<p>Variables in X++ are declared with a data type and can be placed anywhere within a code block, not just at the beginning.</p>
<p>Okay, let's dive deeper into the world of X++ programming, focusing on variables, operators, classes, and how we interact with the database.</p>
<p>In X++, just like in many other programming languages, variables are used to store data. You have member variables (also called global variables) that are accessible throughout an entire class instance, and you have local variables that are only accessible within the specific method where they're declared. When you declare a variable, you specify its data type, give it a name (following camelCase conventions), and optionally assign it an initial value. The text emphasizes the importance of using labels for string values, which is crucial for localization and maintainability. This means that rather than hardcoding text like "Customer Name" directly into your code, you would use a label, making it easier to translate that text into different languages later.</p>
<p>Operators in X++ are used to perform calculations and comparisons. Assignment operators change the value of a variable, like the basic "=" or the compound "+=", "-=", which add or subtract a value from the variable. Arithmetic operators perform mathematical operations, such as addition ("+"), subtraction ("-"), multiplication ("*"), division ("/"), and some more specialized ones like "div" for integer division and "mod" for the remainder of an integer division. Relational operators are used for comparisons, like checking if two values are equal ("=="), not equal ("!="), greater than ("&gt;"), less than ("&lt;"), and so on. The "like" operator is specific to SQL statements and is used for pattern matching within strings.</p>
<p>The ternary operator is a concise way to write an if-else statement in a single line. It takes three operands: a condition, a value to return if the condition is true, and a value to return if the condition is false. The example "result = (x==5) ? "Yes" : "No";" demonstrates this perfectly. If "x" is equal to 5, the "result" variable gets the value "Yes"; otherwise, it gets "No".</p>
<p>Now, let's talk about classes. In X++, classes are the blueprints for creating objects. They define the data (variables) and behavior (methods) of those objects. The class declaration specifies the class name, member variables, and modifiers. While all X++ classes are technically public, you can control the accessibility of individual methods using modifiers like "private", "protected", and "public". "private" methods are only accessible within the same class, "protected" methods can be accessed within the same class and its subclasses, and "public" methods can be called from anywhere.</p>
<p>Classes need to be instantiated before they can be used. This means creating a specific object based on the class blueprint. The example shows how to create an instance of a "TruckLoad" class within a "Truck" class.</p>
<p>Methods within a class can be either instance methods or static methods. Instance methods operate on a specific object instance and can access both the instance's data and static data. Static methods, on the other hand, don't require an object instance and can only access static data. The "main" method is a special type of static method that can be called directly from a menu option.</p>
<p>The method signature defines the method's name, return type (or "void" if it doesn't return anything), modifiers, and parameters. The method body contains the actual code that gets executed. The example "public real ReturnCalculation(real _expression1)" shows a public method that returns a "real" value and takes a "real" parameter.</p>
<p>Collection classes in X++ provide ways to store and manage groups of data. We're talking about things like arrays, lists, maps, sets, and structs. Arrays are ordered collections of elements of the same type. Lists are similar but can be more efficient for adding or removing elements. Maps store key-value pairs, where each key is unique and maps to a specific value. Sets are like lists but only allow unique values. Structs are used to group together related data of different types.</p>
<p>The examples provided illustrate how to use these collection classes. For instance, the "List" example shows how to add elements to the beginning or end of a list and then iterate through them using either a "ListIterator" or a "ListEnumerator". The "Map" example demonstrates how to insert key-value pairs, check if a key exists, and look up a value based on its key. The "Set" example shows how to add unique values to a set and check if a value is already present.</p>
<p>Now, let's move on to interacting with the database using SQL statements within X++. You can use "select" statements to retrieve data, specifying the table, fields, and conditions (using the "where" clause). You can also order the results using "order by" and group them using "group by". The "forUpdate" option in a "select" statement indicates that you intend to modify the selected records.</p>
<p>Joins are used to combine data from multiple tables based on a related field. The example shows how to join "SalesLine" and "SalesTable" based on the "SalesId" field.</p>
<p>A "while select" statement loops through all records that match the specified criteria, allowing you to process each record individually. The example demonstrates how to update the "CreditMax" for all customers whose account number starts with "US".</p>
<p>Insert methods are used to add new records to a table. It is essential to use "ttsBegin" and "ttsCommit" around your insert statements to ensure that the operation is treated as a single transaction. This means that either all the changes within the transaction are saved, or none of them are. Similarly, update methods modify existing records. You need to use "select forUpdate" to lock the records you intend to update and wrap the update statements within a "ttsBegin" and "ttsCommit" block. The "update_recordset" statement allows you to update multiple records at once efficiently.</p>
<p>Let's continue our exploration of X++ and delve into conditional statements, iterative statements (loops), exception handling, and a few other important concepts.</p>
<p>We talked about how "ttsBegin" and "ttsCommit" are essential for database operations like inserts and updates. They ensure that your changes are treated as a single transaction. Well, delete operations follow the same principle. To remove a record, you use a "delete" method, and just like with inserts and updates, you need to wrap it in a "ttsBegin" and "ttsCommit" block. The example shows how to delete a customer record where the "firstname" is 'Johnny'. The "doDelete()" method allows you to bypass the standard delete logic if you need to customize the deletion process. Also, the "delete_from" statement is handy when you want to delete multiple records at once.</p>
<p>Now, let's move on to conditional statements. These are fundamental programming constructs that allow your code to make decisions and execute different blocks of code based on certain conditions. X++ offers the standard "if", "if...else", and "switch" statements, as well as the ternary operator, which we discussed earlier.</p>
<p>An "if" statement checks if a condition is true and, if so, executes the code within its block. The example "if (x == 5)" demonstrates this. If "x" equals 5, the code inside the curly braces will run.</p>
<p>The "if...else" statement provides an alternative path to execute if the condition in the "if" statement is false. The example shows that if "x" is not equal to 5, the code within the "else" block will run.</p>
<p>You can use logical operators like "&amp;&amp;" (AND) and "||" (OR) within your conditions to check multiple expressions. You can also nest "if" statements within each other, but the text suggests that if you find yourself nesting too many "if" statements, a "switch" statement might be a better choice.</p>
<p>A "switch" statement is useful when you have a variable that could have several different values, and you want to execute different code for each value. The example shows a "switch" statement that checks the value of "x". If "x" is 5, it runs the code under "case 5". If "x" is 10, it runs the code under "case 10". If "x" is neither 5 nor 10, it runs the code under "default". The "break" statement is crucial within each "case" to prevent the code from "falling through" to the next case.</p>
<p>Iterative statements, or loops, allow you to repeat a block of code multiple times. X++ provides "while", "do...while", and "for" loops.</p>
<p>A "while" loop continues to execute as long as its condition is true. The example shows a loop that iterates through a container and prints each element.</p>
<p>A "do...while" loop is similar to a "while" loop, but the condition is checked at the end of the loop instead of the beginning. This means that the code within a "do...while" loop will always execute at least once. The example demonstrates a scenario where you might want to do something at least once, like getting parameters for a report.</p>
<p>A "for" loop is often used when you know how many times you want to iterate, or when you're working with collections that have a fixed number of elements, like arrays. The example shows a "for" loop that iterates through an array and prints each element.</p>
<p>The "break" statement can be used to exit a loop prematurely, and the "continue" statement can be used to skip the rest of the current iteration and move to the next one.</p>
<p>Exception handling is how your code deals with errors. The "throw" statement is used to signal an error. The example shows how to throw an error message using the "error" method. The "try...catch" statement allows you to handle exceptions gracefully. The code within the "try" block is executed, and if an exception occurs, the code within the corresponding "catch" block is run. The example demonstrates how to catch a "Numeric" exception and a generic exception. The "retry" statement, which can be used within a "catch" block, allows you to jump back to the beginning of the "try" block. The "finally" block, if present, will always be executed, regardless of whether an exception occurred or not.</p>
<p>The "Message" API offers more granular control over user messages, allowing you to add, remove, and manage them beyond the standard save boundaries. The "Message::AddAction()" method enables embedding actions within messages, such as linking to a specific menu item.</p>
<p>It is crucial to understand how exceptions behave within transactions. If an exception occurs inside a "ttsBegin - ttsCommit" block, the transaction is automatically aborted. Also, any "catch" statements within that same transaction block will not handle the exception. Instead, the exception will be caught by the nearest "catch" statement outside the transaction block.</p>
<p>Constant values, defined using the "const" keyword, are variables whose values cannot be changed after they are initialized.</p>
<p>Global functions in X++ are implemented as static methods within the "Global" class. They can be called from anywhere in your application without needing to specify the class name. To create a new global function, you would create a code extension of the "Global" class and add your static method.</p>
<p>X++ provides a wide range of built-in runtime functions for common tasks, such as mathematical operations, string manipulation, date operations, and more. These functions can significantly simplify your code.</p>
<p>Finally, X++ code can seamlessly interact with .NET libraries. You can add references to C# projects or other .NET assemblies within your Visual Studio solution, and the system will handle the deployment of those assemblies to the cloud.</p>
<p>Object-Oriented Programming (OOP) is a paradigm centered around objects, which have both data (attributes) and behavior (methods). We will discuss this further in our next interaction!</p>
<p>Alright, let's break down these more advanced concepts related to object-oriented programming (OOP) in the context of X++ and Dynamics 365 development.</p>
<p>We're now talking about the core principles of OOP: defining objects, their relationships, and how they interact. In Dynamics 365, you're working with X++, which is an object-oriented language. This means you'll be creating and manipulating objects that represent real-world entities or abstract concepts within your application.</p>
<p>Inheritance is a fundamental OOP concept that allows you to create new classes (child classes or subclasses) based on existing classes (parent classes or superclasses). The child class inherits the attributes (data) and methods (behavior) of the parent class, and it can also add its own unique attributes and methods or modify the inherited ones. The example provided illustrates this with a "Vehicle" parent class and "Car", "Bus", and "Truck" child classes. Each child class inherits the basic characteristics of a vehicle (like height and width) but can also have its own specific attributes (like the number of passengers in a "Car").</p>
<p>Abstract classes are a special type of class that cannot be instantiated directly. They serve as templates or blueprints for other classes. An abstract class may contain abstract methods, which are methods without a concrete implementation. Any class that inherits from an abstract class must provide an implementation for all its abstract methods. The example shows an abstract "Vehicle" class with an abstract "operate()" method. The "Car" class, which extends "Vehicle", must provide a concrete implementation of "operate()".</p>
<p>Attributes in X++ are used to add metadata to classes and methods. They provide additional information about the element they are attached to. The "SysObsoleteAttribute" is a good example. It can be used to mark classes or methods that are no longer recommended for use, and it can trigger warnings or errors during compilation. You can also create your own custom attribute classes by extending the "SysAttribute" class. The example demonstrates a "PracticeAttribute" class that can be used to decorate other classes or methods with specific instructions or information.</p>
<p>Chain of Command (CoC) is a powerful feature in X++ that allows you to extend the functionality of existing methods without modifying the original code. This is crucial in Dynamics 365 because you often need to customize standard functionality without directly altering the base code (which is generally not allowed or recommended). With CoC, you create an extension class (with a "_Extension" suffix) and use the "ExtensionOf" attribute to link it to the class you're extending. Within your extension class, you can then wrap the original method, adding your custom logic before or after the original code by using the "next" keyword. This calls the next method in the chain, eventually leading to the original method. The example shows how to extend the "doSomething()" method of an "ExampleClass".</p>
<p>It's important to note that not all methods can be wrapped using CoC. Methods marked as "final", "private", or those with "[Hookable(false)]" or "[Wrappable(false)]" attributes cannot be extended using CoC.</p>
<p>Scope in X++ determines where a variable or method can be accessed. Instance variables (class fields) have a broad scope, being accessible from any method within the class and its subclasses (if declared as "public" or "protected"). Local variables have a narrower scope, being limited to the method in which they are declared. Parameters are essentially local variables that receive their values when the method is called.</p>
<p>Access identifiers ("public", "protected", and "private") control the visibility and accessibility of methods and variables. "public" elements are accessible from anywhere, "protected" elements are accessible within the same class and its subclasses, and "private" elements are only accessible within the same class.</p>
<p>The "internal" keyword, similar to its C# counterpart, restricts access to the model where the element is defined.</p>
<p>When passing variables as parameters to methods, it's important to understand the concept of "call by value" versus "call by reference." In X++, the default is "call by value," meaning that a copy of the variable's value is passed to the method. Any changes made to the parameter within the method do not affect the original variable outside the method.</p>
<p>Interfaces define a contract that classes can implement. They specify a set of public instance methods that a class must implement if it implements the interface. The example shows an "IDrivable" interface and an "Automobile" class that implements it. The "is" keyword can be used to check if an object implements a specific interface.</p>
<p>Okay, let's discuss Application Lifecycle Management (ALM) methodologies and their relevance to Dynamics 365 for Finance and Operations implementations.</p>
<p>ALM is essentially the entire lifespan of an application, from its initial conception to its eventual retirement. It encompasses everything from requirements gathering and design to development, testing, deployment, maintenance, and eventual decommissioning. The goal of ALM is to streamline and optimize this process, improving efficiency, collaboration, and the overall quality of the software.</p>
<p>The text highlights three common ALM methodologies: Waterfall, Agile, and Spiral. Each has its strengths and weaknesses, and the best choice depends on the specific project and organizational context.</p>
<p><strong>Waterfall</strong> is a traditional, sequential approach. Think of it like a series of steps cascading down, just like a waterfall. Each phase of the project (requirements, design, implementation, testing, maintenance) is completed in order before moving on to the next. This methodology is characterized by thorough documentation, clear deliverables, and formal reviews and approvals at each stage. Waterfall works best for projects with well-defined, stable requirements where the scope is unlikely to change significantly. It's less suited for complex projects or those where requirements are uncertain or likely to evolve.</p>
<p><strong>Agile</strong>, on the other hand, is an iterative and incremental approach. Instead of a linear sequence, Agile projects are broken down into smaller cycles, often called "sprints." Each sprint typically lasts one or two weeks and focuses on delivering a specific set of features or functionalities. This allows for continuous feedback and adaptation throughout the project lifecycle. Agile is well-suited for projects where requirements are unclear or expected to change, where rapid delivery is a priority, and where close collaboration between developers, testers, and business stakeholders is essential. However, Agile can be more challenging to manage due to its iterative nature and the need for constant communication and coordination.</p>
<p><strong>Spiral</strong> combines elements of both Waterfall and Agile. It emphasizes risk assessment and is often used for large, complex projects. A Spiral project proceeds in a series of cycles, each involving planning, risk analysis, prototyping, and evaluation. This iterative approach allows for early identification and mitigation of potential risks. The Spiral methodology is suitable for projects where risk management is critical but can be resource-intensive due to the repeated cycles of planning and evaluation.</p>
<p>Now, let's shift our focus to models in Dynamics 365 development. A model is essentially a container for a set of related elements, such as metadata and source code, that define a customization or extension to the standard application. Models are packaged into deployable units that can be applied to different environments.</p>
<p>Models are crucial for organizing and managing customizations. They allow you to group related elements together, making it easier to track, manage, and deploy changes. You can create models in Visual Studio, and there are two main types: those that are deployed in their own package and those that are part of an existing package. Models in their own package are generally recommended for new customizations, as they provide better isolation and simplify deployment. Models within an existing package are typically used for overlaying, which is modifying standard code directly. However, overlaying is generally discouraged in favor of extension-based development.</p>
<p>When planning your development process, you need to consider how you'll manage builds, testing, and quality control. This involves defining your build process, determining which environments you'll need (development, testing, UAT, production, etc.), and deciding how much testing is required (unit testing, regression testing, performance testing).</p>
<p>Source control is another critical aspect of development, especially when multiple developers are working on the same project. Azure DevOps, which integrates seamlessly with Dynamics 365, provides source control capabilities using either Team Foundation Version Control (TFVC) or Git. TFVC is a centralized version control system, while Git is distributed. Git is generally the recommended choice for new projects due to its flexibility and branching capabilities.</p>
<p>Upgrades, updates, and hotfixes are inevitable parts of the application lifecycle. Upgrades typically involve moving to a new major version of the product, while updates are smaller, more frequent releases that include bug fixes, performance improvements, and new features. Hotfixes are patches that address specific issues.</p>
<p>When planning for upgrades or updates, it's essential to understand the changes being introduced and their potential impact on your customizations. Microsoft provides release plans that detail the new features and changes in each release. It's also crucial to have a testing strategy in place to ensure that your customizations continue to function correctly after an upgrade or update.</p>
<p>Risk management is another essential aspect of ALM. Upgrades and updates can introduce risks, such as downtime, compatibility issues, or unexpected behavior. Identifying potential risks, assessing their likelihood and impact, and developing mitigation strategies are crucial for a successful implementation.</p>
<p>Let's continue our discussion and focus on Extended Data Types (EDTs), base enumerations (enums), and tables within the context of Dynamics 365 for Finance and Operations development.</p>
<p>EDTs and enums are important data types that help improve code readability, maintainability, and consistency. EDTs extend primitive data types (like "string", "integer", "date") by adding custom properties and behaviors. Enums, on the other hand, define a set of named literals, essentially creating a list of predefined values.</p>
<p>The text highlights several benefits of using EDTs. They make code easier to read because you can use descriptive names for your data types (like "CustomerName" instead of just "string"). They promote consistency because the properties you set on an EDT are applied to all instances of that type throughout the system. EDTs also support inheritance, meaning you can create hierarchies of EDTs where child EDTs inherit properties from their parent EDTs.</p>
<p>Base enums are lists of literals, each with an associated numerical value. They are stored in the database as numbers, which saves space. Enums can be used in X++ code like integers, but they are displayed to the user in the interface with their corresponding label. You create base enums in Visual Studio by adding a new "Base Enum" item to your project.</p>
<p>The examples provided illustrate how enums can be displayed in the user interface, such as a drop-down list (like the "NoYes" enum) or as a set of buttons.</p>
<p>Properties are essential for both EDTs and enums. They allow you to customize the appearance and behavior of these data types. For enums, you can set properties like the label (which is displayed to the user) and the name of each element.</p>
<p>Now, let's talk about tables. Tables are the fundamental building blocks for storing data in Dynamics 365. They are similar to tables in a relational database or spreadsheets, with rows representing records and columns representing fields.</p>
<p>You create tables in Visual Studio by adding a new "Table" item to your project. The Table Designer window allows you to define the structure of your table, including its fields, field groups, indexes, relations, and other components.</p>
<p>Fields are added to a table, and they are usually based on EDTs or base enums. This is where the benefits of EDTs become apparent. By using an existing EDT for a field, you ensure consistency with other parts of the system that use the same EDT.</p>
<p>When you create a new table or field, it's important to populate its properties. For tables, some key properties include the table type (regular or temporary), name, label, primary index, cluster index, configuration key, and whether it supports inheritance. For fields, you typically set the properties at the EDT or base enum level rather than within the table itself.</p>
<p>Let's delve into the specifics of managing fields, field groups, indexes, relations, table methods, and views within Dynamics 365 for Finance and Operations.</p>
<p>When you're creating a table, you'll be adding fields to it, and these fields are often based on EDTs or base enums. Field groups allow you to organize related fields together. This not only improves the structure of your table but also enhances the user interface. When you use a field group in a form, any changes you make to the field group (like adding a new field or modifying a property) will automatically be reflected in the form.</p>
<p>Indexes are crucial for optimizing data retrieval. They allow the system to quickly locate specific records based on the indexed fields. When creating an index, you need to carefully choose the fields to include and their order. The order should go from the most granular (most unique values) to the least granular (fewest unique values). There are three main types of indexes: primary, clustered, and non-clustered.</p>
<ul>
<li><strong>Primary Index:</strong> Uniquely identifies each record in a table.</li>
<li><strong>Clustered Index:</strong> Determines the physical order in which data is stored in the table.</li>
<li><strong>Non-clustered Index:</strong>  Provides a separate structure for quickly looking up data based on specific columns.</li>
</ul>
<p>Relations define how tables are connected to each other. They are essential for maintaining data integrity and enabling features like lookups and joins. When creating a relation, you specify the related table, the type of relationship (normal, field fixed, related field fixed, or foreign key), and the fields involved in the relation.</p>
<p>Delete actions define what happens when a record in a related table is deleted. The options are:</p>
<ul>
<li><strong>Restricted:</strong> Prevents deletion if related records exist.</li>
<li><strong>Cascade:</strong> Deletes related records automatically.</li>
<li><strong>CascadeRestricted:</strong> A combination of restricted and cascade, depending on the parent table.</li>
</ul>
<p>Table methods are pieces of code associated with a table that can be executed when certain events occur, like when a record is inserted, updated, or deleted. Standard table methods include "initValue()", "insert()", "modifiedField()", "validateField()", and "validateWrite()". You can override these methods to add custom logic. For example, you might use "initValue()" to set default values for fields when a new record is created, or "validateWrite()" to enforce data validation rules before a record is saved.</p>
<p>The "find()" method is a common custom method that retrieves a specific record based on a primary key or other unique identifier.</p>
<p>Chain of Command (CoC) allows you to extend table methods by adding your code before or after the original method's logic. This is done by creating an extension class and using the "next" keyword to call the next method in the chain.</p>
<p>Views are similar to tables in that they present data in a tabular format, but they are based on underlying queries. They can combine data from multiple tables and provide a simplified or filtered view of the data. Views are useful for reporting, forms, and other scenarios where you need to present data in a specific way.</p>
<p>You create views in Visual Studio, and like tables, they have fields, field groups, indexes, relations, and methods. Computed columns are a special type of field in a view that are calculated on the database using T-SQL. You define the calculation logic for a computed column in a static method within the view.</p>
<p>When working with views, it's important to understand the performance implications. Views that involve complex joins or calculations can impact performance, so it's essential to design them carefully.</p>
<p>Let's break down the remaining concepts related to extending queries, table maps, and the lab exercise involving table creation.</p>
<p>When you extend a query, you can modify certain aspects of it, like adding new data sources, fields, grouping, ordering, or having clauses. However, you cannot remove existing components from the original query. This is important to keep in mind when designing your extensions. Also, setting the "Dynamic Fields" property to "Yes" on a data source will automatically add new fields to the query, often eliminating the need for an extension just to add fields.</p>
<p>Table maps are a bit different. They define a common interface for working with multiple related tables. You can define mappings between fields in the map and fields in the underlying tables. However, you cannot create extensions for table maps directly. Instead, you need to work with the individual tables that are part of the map. You can also use interface classes to encapsulate specific logic for each table within the map.</p>
<p>The example provided demonstrates the creation of an abstract interface class ("MyCustVendMapInterface") and a concrete implementation class ("MyCustVendMapMyCustTable"). This pattern allows you to define common methods in the interface and then provide table-specific implementations in the concrete classes.</p>
<p>Now, let's look at the lab exercise. It demonstrates the fundamental steps of creating a new table in Visual Studio and adding fields to it:</p>
<ol>
<li><strong>Create a new project:</strong> This sets up the container for your development work.</li>
<li><strong>Add a new table item:</strong> This creates the table definition within your project.</li>
<li><strong>Set table properties:</strong>  This configures the basic characteristics of your table, such as its label.</li>
<li><strong>Add fields:</strong> This defines the data elements that your table will store. You'll typically drag and drop existing EDTs or base enums from the Application Explorer or your project into the Fields node of your table.</li>
<li><strong>Create field groups:</strong> Field groups, while not demonstrated in the lab instructions, are a good practice for organizing related fields within your table, as we discussed earlier.</li>
</ol>
<p>The lab guides you through using existing elements from the AOT (like EDTs and base enums) when creating your table. This is a best practice because it promotes consistency and reusability.</p>
<p>Let's quickly revisit the questions that were posed in the previous interactions and provide the answers based on our discussion:</p>
<p><strong>Question 1:</strong> You're working on a project that has undefined requirements and new requirements are expected to occur during the project. What methodology would best suit this type of project?</p>
<p><strong>Answer:</strong> Agile. Agile's iterative nature and focus on continuous feedback make it ideal for projects with evolving requirements.</p>
<p><strong>Question 2:</strong> A new developer is joining the project team. You, as the project manager, must request a new environment for the developer to use. What kind of environment should you request?</p>
<p><strong>Answer:</strong> A Tier 1 environment. Tier 1 environments are single-box environments designed for development purposes.</p>
<p><strong>Question 3:</strong> How are base enums represented and stored in the database?</p>
<p><strong>Answer:</strong> A numerical value. Base enums are stored as integers in the database, which saves space.</p>
<p><strong>Question 4:</strong> We recommend that you use EDTs whenever possible in coding for finance and operations apps. Which of these statements is not a benefit of using EDTs?</p>
<p><strong>Answer:</strong> EDTs are more easily compiled. While EDTs offer many benefits like reusability, readability, and inheritance, they don't necessarily compile more easily than other data types.</p>
<p><strong>Question 5:</strong> A base enumeration can be viewed in multiple ways in the user interface of finance and operations apps. Which of the following is not an example of how a base enum can be viewed in the user interface?</p>
<p><strong>Answer:</strong> A date field. Base enums are typically represented as option buttons, slider bars, or drop-down menus.</p>
<p><strong>Question 6:</strong> What methods can you extend to use Chain of Command?</p>
<p><strong>Answer:</strong> Protected and public methods. CoC allows you to extend the logic of protected and public methods, but not private methods or those marked as "final" or with specific "hookable" or "wrappable" attributes.</p>
<p><strong>Question 7:</strong> Which of the following statements is NOT true about variable declarations?</p>
<p><strong>Answer:</strong> You can declare a variable after the variable is used. Variables must be declared before they are used in your code.</p>
<p><strong>Question 8:</strong> You need a conditional statement that checks if a variable equals In progress, Completed, or Canceled. Different code needs to run, depending on what the variable equals. A default block of code needs to run if the variable doesn't equal any of the values. What conditional statement is most appropriate?</p>
<p><strong>Answer:</strong> Switch statement. A switch statement is ideal for checking a variable against multiple possible values and executing different code blocks accordingly.</p>
<p><strong>Question 9:</strong> An error is sometimes thrown in a certain try…catch block. You know that the issue can be fixed within the code during the error handling code block. What statement could you use when the error is found to run the code again?</p>
<p><strong>Answer:</strong> The retry statement could be used in the catch statement to rerun the try block.</p>
<p><strong>Question 10:</strong> You want to call a new global function to use in an application that someone else created by using extensions. What recommended method should you use?</p>
<p><strong>Answer:</strong> Call it by using only the method name. Global functions can be called directly by their name without needing to specify the class name.</p>
<p><strong>Question 11:</strong> You created a new form with a grid that lists records. The grid has a virtual field that should save the value for each record temporarily, and you want to ensure that you’re using the correct collection class. What is the recommended collection class?</p>
<p><strong>Answer:</strong> Map collection class. Maps are useful for storing temporary data associated with records, especially in forms.</p>
<p>Let's continue our exploration of forms, focusing on form patterns, data sources, controls, and form methods within Dynamics 365 for Finance and Operations.</p>
<p>Form patterns are pre-defined templates that provide a consistent structure and layout for forms. They help ensure that forms are user-friendly, follow best practices, and are easier to maintain. While not mandatory, using form patterns is highly recommended.</p>
<p>The text highlights several common form patterns, including:</p>
<ul>
<li><strong>Simple List:</strong> For displaying a list of records with a small number of fields.</li>
<li><strong>Simple List and Details:</strong> Similar to Simple List but also includes a details section for viewing more information about a selected record.</li>
<li><strong>Simple Details:</strong>  For displaying a simple set of fields, often in view mode.</li>
<li><strong>Master Details:</strong> Used for entering and viewing data organized into FastTabs.</li>
<li><strong>Details Transaction:</strong> Presents data in a header and lines format.</li>
<li><strong>List Page:</strong> Optimized for browsing records and taking actions on them.</li>
<li><strong>Table of Contents:</strong> Used for setup and configuration forms.</li>
<li><strong>Workspace:</strong> Creates operational workspaces that group related tasks and pages.</li>
<li><strong>Wizard:</strong> Guides the user through a multi-step process.</li>
</ul>
<p>The Form Designer window in Visual Studio is where you create and manage forms. It has several panes:</p>
<ul>
<li><strong>Left Pane:</strong>  Displays the form's structure, including data sources, methods, and events.</li>
<li><strong>Right Pane:</strong> Used for applying patterns, adding controls, and setting properties.</li>
<li><strong>Bottom Pane:</strong> Shows the pattern design and a preview of the form.</li>
</ul>
<p>Adding a data source to a form is essential for displaying data. You can drag and drop tables or queries from the Application Explorer or Solution Explorer into the Data Sources node of your form.</p>
<p>Once you have a data source, you can add controls like grids, fields, and groups to your form. Grids display data in a tabular format, fields display individual data elements, and groups organize related controls.</p>
<p>Form methods are pieces of code associated with a form that execute when certain events occur, like when the form is opened, closed, or when a record is saved or deleted. Standard form methods include "init()", "close()", "closeCancel()", "closeOK()", and "run()". You can override these methods to add custom logic.</p>
<p>Data source methods are similar to form methods but are specific to a particular data source on the form. Some common data source methods are "init()", "active()", "validateWrite()", and "validateDelete()".</p>
<p>Here are some examples of how you might use these methods:</p>
<ul>
<li><strong>Form "init()":</strong> To initialize variables, set default values, or modify the query used by a data source.</li>
<li><strong>Form "close()":</strong> To clean up resources or perform actions before the form closes.</li>
<li><strong>Data source "init()":</strong> To set filters or modify the query for a specific data source.</li>
<li><strong>Data source "active()":</strong> To enable or disable controls based on the currently selected record.</li>
<li><strong>Data source "validateWrite()":</strong> To validate data before it's saved.</li>
<li><strong>Data source "validateDelete()":</strong> To prevent deletion of records based on certain conditions.</li>
</ul>
<p>Chain of Command (CoC) can also be used to extend form and data source methods, allowing you to add custom logic before or after the standard method's execution.</p>
<p>Let's dive deeper into form methods, menu items, menus, and performance optimization within Dynamics 365 for Finance and Operations.</p>
<p>We discussed several standard form methods, including "init()", "close()", "closeCancel()", "closeOK()", and "run()". Now, we're adding "executeQuery()" to the mix. This method is crucial for refreshing the data displayed on a form. While "init()" runs once when the form is opened, "executeQuery()" runs whenever the form needs to refresh its data, such as when filters are applied.</p>
<p>The example demonstrates how to override "executeQuery()" to filter the data based on the "TrainingType" field. This is a common technique for controlling what data is displayed on a form based on specific criteria.</p>
<p>We also covered "initValue()", which is used to set default values for fields when a new record is created. The example shows how to set the "TrainingDate" to the current date.</p>
<p>The "write()" method is called when a record is saved. The example illustrates how to update the "Price" field based on values from related tables.</p>
<p>The "delete()" method is executed when a record is deleted. The example demonstrates updating a related table ("TrainerTable") to reflect the deletion.</p>
<p>Moving on to data source field methods, we have "modified()", "validate()", and "lookup()".</p>
<ul>
<li>"modified()": Triggered when a field's value is changed. The example shows how to clear the "Venue" field if "TrainingType" is set to "Online".</li>
<li>"validate()": Used to validate the value entered in a field. The example demonstrates restricting the "Participants" field to a maximum value of 20.</li>
<li>"lookup()": Called when a lookup is performed on a field. The example shows how to filter the lookup for "TrainerID" to only show technical trainers.</li>
</ul>
<p>Chain of Command (CoC) is a powerful mechanism for extending form methods, data source methods, data source field methods, and form control methods. The examples demonstrate how to use CoC with the "next" keyword to add custom logic before or after the standard method's execution.</p>
<p>Menu items are essential for user navigation. They provide links to forms, reports, or batch jobs. There are three main types:</p>
<ul>
<li><strong>Display:</strong> Open forms or dialogs.</li>
<li><strong>Output:</strong> Generate reports.</li>
<li><strong>Action:</strong> Perform actions, like creating or updating records.</li>
</ul>
<p>You can extend existing menu items to change their labels, help text, or the object they point to.</p>
<p>Menus are containers for menu items. They define the structure of the navigation pane in the user interface. You can create new menus in Visual Studio and add menu item references, separators, submenus, or tiles to them.</p>
<p>Testing form functionality is crucial before deployment. You can set a form as the startup object and run it in a browser to test its behavior and data connections.</p>
<p>Performance optimization is also important. Browser-based tools like Fiddler and Chrome Developer Tools, along with Trace Parser and Performance Timer, can help you diagnose and address performance issues.</p>
<p>Okay, let's break down what's going on in this scenario, focusing on the core concepts involved in creating forms and using classes within the development environment of Microsoft Dynamics 365 Finance and Operations.</p>
<p>Imagine you're a developer tasked with building a simple data entry screen for a Fleet Management company. They need a straightforward way to input customer information. This is a common real-world task you'd encounter, and Microsoft Dynamics 365 provides a set of tools to accomplish it.</p>
<p>First, we're dealing with <strong>forms</strong>. In Dynamics 365, forms are essentially the user interface elements that people interact with. Think of them as the digital equivalent of paper forms, but way more powerful. You use forms to view, enter, and edit data. These forms are built within Visual Studio, which is Microsoft's integrated development environment (IDE). It's like a sophisticated toolbox that provides everything you need to construct these digital forms.</p>
<p>The text describes creating a new form called "FleetCustomersForm". This form will be linked to the "CustTable", which, as the name suggests, is a database table that stores customer information. Dynamics 365 is built on a database, and tables are where the data is stored. It is important to note that "CustTable" is a standard table that exists out-of-the-box in Dynamics 365 Finance and Operations. Tables have fields (like columns in a spreadsheet) that hold specific pieces of information, such as customer name, account number, and so on.</p>
<p>Now, to make form creation easier, Dynamics 365 uses <strong>form patterns</strong>. These are like templates or blueprints that provide a predefined structure and layout for your form. The scenario mentions the "Simple List" pattern. This pattern is commonly used when you want to display a list of records, like a list of customers, and allow basic operations like adding, deleting, and editing them.</p>
<p>When you apply a pattern, you're essentially telling Dynamics 365, "I want my form to look and behave like this standard template." This ensures consistency across the application and saves you a lot of time compared to building everything from scratch.</p>
<p>The text goes into details about adding components to the form, such as an "ActionPane", "Group", and "Grid". These are standard UI elements within Dynamics 365 forms:</p>
<ul>
<li><strong>ActionPane:</strong> Think of this as a toolbar at the top of the form where you have buttons for actions like "New," "Delete," "Save," etc.</li>
<li><strong>Group:</strong> This is a way to organize related fields or controls together on the form. For instance, you might have a group for "Customer Details" and another for "Contact Information."</li>
<li><strong>Grid:</strong> This is where the list of records is displayed, similar to a table or a spreadsheet view.</li>
</ul>
<p>The developer in the scenario is adding these elements and configuring their properties. For example, they're setting the name of the "ActionPane" to "ActionPane" and the name of the "Group" to "Filters."</p>
<p>Then comes the part where you link the form to the actual data. This is done by specifying the <strong>data source</strong>, which in this case is the "CustTable". By doing this, you're telling the form, "Hey, the data you're going to display and work with is coming from this table." The example then involves dragging specific fields from "CustTable" (like "Party", "SalesGroup", "CustGroup", and "AccountNum") onto the "MainGrid". This means that these fields will be visible and editable in the grid on the form.</p>
<p>After building the form, the developer sets it as the <strong>startup object</strong>. This means that when you run the project, this particular form will be launched automatically. It's like designating the main entry point for testing your form. Pressing "Ctrl+F5" starts the project, and the newly created form opens in a browser window, allowing you to interact with it and even add new records.</p>
<p>Now, let's touch on the concept of <strong>classes</strong>. Think of classes as blueprints for creating objects. An object, in this context, is a self-contained unit that holds both data and the code that operates on that data. Classes are a fundamental concept in object-oriented programming.</p>
<p>In Dynamics 365, you often use classes to add custom logic or behavior to your application. For instance, you might create a class to perform complex calculations, validate data, or interact with external systems. The text mentions <strong>runnable classes</strong>, which are a special type of class that can be executed directly. They have a "main" method, which serves as the entry point for execution. Runnable classes are often used for batch jobs, scheduled tasks, or running custom processes in Dynamics 365 Finance and Operations.</p>
<p>The concept of <strong>caching</strong> is introduced, specifically <strong>SysGlobalCache</strong> and <strong>SysGlobalObjectCache</strong>. Caching is a technique used to store frequently accessed data in a temporary location (the cache) for faster retrieval. Think of it like keeping a copy of your most-used tools close at hand instead of having to go to the toolbox every time.</p>
<ul>
<li><strong>SysGlobalCache:</strong> This is a cache that is specific to a user's session. It's like a personal cache that only that user can access. The system automatically cleans it up when the user's session ends.</li>
<li><strong>SysGlobalObjectCache:</strong> This is a server-wide cache that is shared across all user sessions on a particular server. It's like a communal cache that everyone can use.</li>
</ul>
<p>Caching can significantly improve performance by reducing the need to repeatedly fetch data from the database, which is generally a slower operation. However, as the text emphasizes, it's crucial to manage the cache properly, especially the global cache, to avoid consuming excessive memory and to ensure that the data in the cache remains up-to-date.</p>
<p>Finally, the concept of <strong>context classes</strong> is brought up. These classes are used to pass additional information or parameters to methods without modifying their original signature. This is particularly useful when working with extensions, as you cannot directly alter the parameters of existing methods in standard Dynamics 365 code. Context classes provide a workaround for this limitation. A context class can also be used to avoid "breaking" changes for other developers in the future. For example, if a public method signature was changed, other developers who use this method must also update their code to reflect the change in the number of method parameters.</p>
<p>In essence, the scenario walks through a typical development process in Dynamics 365 Finance and Operations, involving creating a form, linking it to data, applying patterns, adding UI elements, and optionally using classes and caching to enhance functionality and performance. It's a glimpse into the world of customizing and extending this powerful enterprise resource planning (ERP) system.</p>
<p>Let's continue our conversation by diving deeper into classes, methods, and data manipulation within the context of Dynamics 365 Finance and Operations, using X++ as our programming language.</p>
<p>The provided text focuses on a practical example: creating a <strong>runnable class</strong> to insert customer records into a table. Remember, a runnable class is a special type of class that can be executed directly, like a mini-program, because it has a "main" method.</p>
<p>In this scenario, the developer is creating a class called "FMInsertCustomers". The goal is to insert two new customer records into the "FMCustomer" table, which presumably already exists within the Fleet Management company's database.</p>
<p>The code starts by declaring a variable of type "FMCustomer". Think of this like creating a container that can hold information about a single customer. The line "FMCustomer FMCustomer;" is saying, "I'm creating a variable named "FMCustomer", and it's designed to store data that matches the structure of the "FMCustomer" table."</p>
<p>Next, within the "main" method, we have:</p>
<p>"""x++<br>
FMInsertCustomers FMInsertCustomers = new  FMInsertCustomers();<br>
FMInsertCustomers.run();<br>
"""</p>
<p>This code does two things:</p>
<ol>
<li>It creates a new instance of the "FMInsertCustomers" class itself. Even though it's a runnable class, this line demonstrates that you can create objects from your own classes.</li>
<li>It calls the "run()" method on that newly created object.</li>
</ol>
<p>The real action happens in the "insertRecords()" method, which is called from within "run()". Let's break it down:</p>
<p>"""x++<br>
public void run()<br>
{<br>
this.insertRecords();<br>
}<br>
private void insertRecords()<br>
{<br>
ttsBegin;</p>
<pre><code>        FMCustomer.FirstName = "John";
        FMCustomer.LastName = "Smith";
        FMCustomer.Email = "johnsmith@contoso.com";

        FMCustomer.insert();

        FMCustomer.clear();
        FMCustomer.FirstName = "Sally";
        FMCustomer.LastName = "Smith";
        FMCustomer.Email = "sallysmith@contoso.com";

        FMCustomer.insert();

     ttsCommit;
}
</code><button class="copy-code-button">Copy</button></pre>
<p>"""</p>
<ul>
<li><strong>"ttsBegin" and "ttsCommit":</strong> These are keywords related to database transactions. "ttsBegin" marks the start of a transaction, and "ttsCommit" marks the end. Transactions are important because they ensure that a series of database operations either all succeed or all fail together. This helps maintain data integrity. In this case, it means both customer records will be inserted, or neither will be.</li>
<li><strong>"FMCustomer.FirstName = "John";" (and similar lines):</strong> Here, the code is setting the values of specific fields within the "FMCustomer" variable. It's like filling out a form with the customer's details.</li>
<li><strong>"FMCustomer.insert();":</strong> This is the crucial line that actually inserts the data into the "FMCustomer" table. The "insert()" method is a built-in method for table objects that handles the database insertion process.</li>
<li><strong>"FMCustomer.clear();":</strong> This line clears the values from the "FMCustomer" variable, preparing it for the next record. It's like resetting the form so you can fill it out with new information.</li>
</ul>
<p>The code then repeats the process to insert a second customer record for "Sally Smith."</p>
<p>To run this code, the developer sets the "FMInsertCustomers" class as the <strong>startup object</strong>. Then, they use the "Start Without Debugging" option (or "Ctrl+F5") to execute the class. If all goes well, the two new customer records will be added to the database.</p>
<p>Now, let's talk about the different types of <strong>methods</strong> you can create in X++ classes, as explained in the text:</p>
<ul>
<li><strong>"new" method (Constructor):</strong> This method is used to create a new instance of a class. It's called automatically when you use the "new" keyword. The default constructor is simply "new()", but you can customize it to initialize the object's data in a specific way.</li>
<li><strong>"finalize" method (Destructor):</strong> This method is called when an object is being destroyed or garbage-collected. It's used to clean up any resources the object might have been using.</li>
<li><strong>"main" method:</strong> As we've discussed, this is the entry point for runnable classes. It's where execution begins.</li>
<li><strong>"instance" method:</strong> These methods are associated with specific instances (objects) of a class. You need to create an object before you can call its instance methods.</li>
<li><strong>"static" method:</strong> These methods belong to the class itself, not to any particular instance. You can call them directly using the class name, like "ClassName::methodName();". Static methods are often used for utility functions or operations that don't depend on the state of a specific object.</li>
</ul>
<p>The text also mentions <strong>accessor keywords</strong> ("public", "protected", "private"), which control the visibility and accessibility of methods:</p>
<ul>
<li><strong>"public":</strong> Public methods can be called from anywhere.</li>
<li><strong>"protected":</strong> Protected methods can only be called from within the same class or from subclasses (classes that inherit from this class).</li>
<li><strong>"private":</strong> Private methods can only be called from within the same class.</li>
</ul>
<p>These keywords are essential for encapsulation, a core principle of object-oriented programming that involves bundling data and methods together and controlling access to them.</p>
<p>Finally, the text touches upon <strong>data manipulation</strong> using X++:</p>
<ul>
<li><strong>"select":</strong> This statement is used to retrieve data from the database.</li>
<li><strong>"insert":</strong> This statement is used to add new records to a table.</li>
<li><strong>"update":</strong> This statement is used to modify existing records.</li>
<li><strong>"delete":</strong> This statement is used to remove records.</li>
<li><strong>"where":</strong> This clause is used to specify conditions for selecting, updating, or deleting records.</li>
<li><strong>"next":</strong> Used after a "select" statement to fetch subsequent records.</li>
<li><strong>"while select":</strong> This is a loop that iterates through multiple records that meet a specified condition.</li>
</ul>
<p>The provided code examples demonstrate how to use these statements to perform common database operations. For example, "select firstonly custTable;" fetches the first record from the "CustTable", and "info("AccountNum: " + custTable.AccountNum);" displays the value of the "AccountNum" field.</p>
<p>The examples of "insert", "update", and "delete" methods show how to add, modify, and remove records from a hypothetical "MyCustomerTable" table. Note the use of "ttsBegin" and "ttsCommit" to ensure that these operations are performed within transactions.</p>
<p>Let's discuss Business Document Management and SQL Server Reporting Services (SSRS) reports in the context of Dynamics 365 Finance and Operations.</p>
<p><strong>Business Document Management</strong> is a relatively newer feature that empowers business users to create and modify documents, such as invoices, directly within the web client without requiring extensive development effort. Think of it as a user-friendly way to customize the look and feel of your business documents.</p>
<p>Here's a breakdown of the key concepts:</p>
<ul>
<li><strong>Templates:</strong> Business documents are based on templates, which are typically created using Microsoft Word or Excel. These templates define the basic structure and layout of the document.</li>
<li><strong>Electronic Reporting (ER):</strong> Business Document Management relies on the ER framework to define the data that will be populated into the templates. ER allows you to create configurations that map data from Dynamics 365 to placeholders within the templates.</li>
<li><strong>Print Management:</strong> Once a template is created and linked to an ER configuration, it can be connected to the Print Management framework. This allows you to specify which document template should be used when printing a specific type of document, such as an invoice.</li>
<li><strong>SharePoint:</strong> Business Document Management uses SharePoint as temporary storage for the templates while they are being edited. This enables collaboration and version control.</li>
<li><strong>Business Document Type Tag:</strong> This is a special tag within ER configurations that identifies a particular configuration as being used for Business Document Management.</li>
</ul>
<p>The process typically involves importing a template from Lifecycle Services (LCS) or creating a new one, editing it within the Business Document Management workspace, and then linking it to Print Management. Users with appropriate permissions can access and modify these templates.</p>
<p><strong>SSRS Reports</strong>, on the other hand, are a more traditional and powerful way to generate reports within Dynamics 365. They are particularly well-suited for high-volume, transaction-focused reports that require precise formatting and complex logic.</p>
<p>Here are some key aspects of SSRS reports:</p>
<ul>
<li><strong>Report Designer:</strong> SSRS reports are designed using the Report Designer tool within Visual Studio. This provides a graphical interface for creating the layout, adding data elements, and defining parameters.</li>
<li><strong>Data Sources:</strong> SSRS reports can connect to various data sources, including the Dynamics 365 database, to retrieve the data they need.</li>
<li><strong>Parameters:</strong> SSRS reports can be parameterized, allowing users to filter the data displayed by specifying values at runtime.</li>
<li><strong>Drill-Down Navigation:</strong> SSRS reports can be designed to allow users to drill down into more detailed information by clicking on specific elements within the report.</li>
<li><strong>Hyperlinks:</strong> You can embed hyperlinks within SSRS reports that link to other pages within Dynamics 365.</li>
<li><strong>Batch Jobs:</strong> SSRS reports can be scheduled to run automatically as part of a batch job, which is useful for generating reports on a regular basis.</li>
<li><strong>Compliance:</strong> SSRS reports are often used to create compliance documents that meet specific regulatory requirements.</li>
</ul>
<p>When designing an SSRS report, it's important to carefully consider the data requirements, the desired layout, and the intended use case. SSRS reports are ideal for scenarios where you need a high degree of control over the report's appearance and functionality.</p>
<p>In summary, Business Document Management and SSRS reports offer different approaches to generating documents and reports within Dynamics 365 Finance and Operations. Business Document Management provides a user-friendly way to customize business documents, while SSRS reports offer greater flexibility and power for creating complex, data-intensive reports. The choice between them depends on the specific needs of your organization and the complexity of the reporting requirements.</p>
<p>Let's keep our conversation going and explore further into the realm of reporting within Dynamics 365 Finance and Operations, focusing on SSRS reports, query objects, Power BI, and Excel integration.</p>
<p>The text emphasizes the two primary design approaches for <strong>SSRS reports</strong>:</p>
<ol>
<li><strong>Autodesign:</strong> This is the simpler approach, suitable for basic reports where the layout is automatically determined by the system. It's like letting Dynamics 365 handle the formatting for you based on the data you provide.</li>
<li><strong>Precision Design:</strong> This approach offers more control and flexibility for complex reports. It provides a free-form design surface where you can manually arrange fields, tables, and other elements. Think of it as having a blank canvas where you can meticulously design the report's layout.</li>
</ol>
<p>The precision designer allows for fine-tuning, such as adjusting margins, spacing, adding expressions for calculations, setting fonts, incorporating logos, and using labels for automatic text translation.</p>
<p>The process of <strong>creating an SSRS report</strong> involves several steps within Visual Studio:</p>
<ol>
<li><strong>Create a New Report Object:</strong> This is the starting point where you define a new report within your project.</li>
<li><strong>Assign a Data Set:</strong> This is where you specify the data that will be used in the report. You define the data source and the data source type.</li>
</ol>
<p>The text highlights four <strong>data source types</strong>:</p>
<ul>
<li><strong>Query:</strong> This uses an existing query from the Application Object Tree (AOT). Queries defined in the AOT are pre-built data retrieval mechanisms. Using them can be efficient as they often involve optimized SQL statements.</li>
<li><strong>Business Logic:</strong> This is used when the data source is not directly from Dynamics 365. It involves writing custom code to retrieve the data, and the class name must match the report name.</li>
<li><strong>Report Data Provider (RDP):</strong> This is used when a simple query is insufficient and additional logic is needed. For example, when you need to apply dynamic filters, call business classes, or use parameters entered in X++ code. RDP classes are commonly used to create temporary tables that can be used as data sources for an SSRS report. The class populates the table with the required data.</li>
<li><strong>AX Enum Provider:</strong> This is used when a report parameter is an enumeration type, allowing you to filter the report view based on the selected enum value.</li>
</ul>
<p>After creating the data set, you choose between autodesign and precision design. Once the report is complete, you deploy it to make it available within Dynamics 365.</p>
<p><strong>Modifying SSRS reports</strong> is also possible using extensions. This allows you to customize existing reports without altering the standard solutions. You can add new columns by extending tables or report data provider classes, and you can redirect navigation to your custom reports by extending menu items and other classes.</p>
<p>Now, let's talk about <strong>query objects</strong> and the <strong>query builder</strong>. These are tools within Dynamics 365 that allow you to construct SQL statements programmatically. They are particularly useful for creating dynamic queries where the conditions or data sources might vary based on user input or other factors.</p>
<p>The text introduces two key classes:</p>
<ul>
<li><strong>"QueryBuildDataSource":</strong> This class is used to define the data sources for your query. You can specify the table, fields to include, sort order, and relationships with other data sources.</li>
<li><strong>"QueryBuildRange":</strong> This class is used to define filters or conditions for your query. You can specify criteria based on field values, such as equal to, not equal to, greater than, less than, and so on.</li>
</ul>
<p>The provided examples demonstrate how to use these classes to build queries programmatically. For instance, the code shows how to create a query that retrieves data from the "TrainingMaster" table, sorts it by "TrainingID", and filters it based on "TrainingType" or "NoofDays".</p>
<p>The example with the "TrainerTable" demonstrates how to build a query that involves related tables using joins. It shows how to filter online trainings delivered by technical trainers by joining the "TrainingMaster" and "TrainerTable" based on the "TrainerID" field.</p>
<p><strong>Advanced query syntax</strong> is also mentioned, allowing for more complex filtering options for string, number, and date fields. For example, you can use wildcards for string comparisons or specify date ranges for date fields.</p>
<p><strong>Power BI</strong> is presented as a powerful tool for data exploration and interactive reporting. Unlike SSRS, which is primarily for formatted, printable reports, Power BI excels at creating dashboards and visualizations that allow users to dynamically interact with the data.</p>
<p>To create a Power BI report with Dynamics 365 data, you typically use Power BI Desktop or Power BI Online and connect to the Dynamics 365 data using the OData feed. You then select the entities you want to use and design the report by dragging fields onto the canvas and choosing visualizations.</p>
<p><strong>Excel integration</strong> is another important aspect of reporting within Dynamics 365. The <strong>Excel Data Connector</strong> add-in allows you to view, update, and edit data directly within Excel.</p>
<p>The text distinguishes between two features:</p>
<ul>
<li><strong>Open in Excel:</strong> This uses data entities and allows you to not only view data but also make changes and publish them back to Dynamics 365.</li>
<li><strong>Export to Excel:</strong> This exports all the grid data to Excel, but it's a one-way export. You can manipulate the data within Excel, but you cannot publish changes back.</li>
</ul>
<p>Finally, the text touches upon <strong>optimizing data source queries</strong> to improve report performance. For SSRS reports, it emphasizes the importance of only retrieving the necessary data and avoiding unnecessary joins or fields. For Power BI, it suggests removing unused fields, avoiding fields with high cardinality (many unique values), and limiting the use of data slicers.</p>
<p><strong>Display fields</strong> are also discussed. They are used to group data together, provide meaningful layouts, calculate summaries, and enhance the appearance of reports. Labels should be used for text in SSRS reports to enable automatic translation for different languages.</p>
<p>The exercise at the end walks through creating a simple SSRS report, demonstrating the steps involved in creating a query, designing the report, deploying it, and creating a menu item to run it. The key takeaway from the exercise is to practice these steps in order to fully understand how to create an SSRS report.</p>
<p>In essence, this section provides a comprehensive overview of various reporting tools and techniques within Dynamics 365 Finance and Operations. It highlights the strengths and use cases of SSRS reports, query objects, Power BI, and Excel integration, equipping you with the knowledge to choose the right tool for your specific reporting needs.</p>
<p>Let's continue our discussion by focusing on workspaces, KPIs, and the various elements you can incorporate into a workspace within Dynamics 365 Finance and Operations.</p>
<p><strong>Workspaces</strong> are essentially personalized dashboards that provide users with a centralized view of the information and tasks relevant to their roles. They act as a single point of entry into the application, streamlining workflows and improving efficiency. Workspaces are customizable and can be tailored to individual user preferences. A workspace is made up of tiles, lists, and links that provide a quick overview of information and allow users to complete tasks directly from the workspace.</p>
<p>The screenshot provided illustrates a typical workspace, showcasing tiles, lists, and potentially other elements that provide quick access to data and actions.</p>
<p><strong>KPIs (Key Performance Indicators)</strong> are a crucial component of workspaces. They are quantifiable metrics that reflect the performance of a business or a specific process. KPIs help users monitor progress, identify trends, and make informed decisions.</p>
<p>The text highlights several key aspects of KPIs:</p>
<ul>
<li><strong>Clear Objectives:</strong> KPIs should be tied to specific, measurable objectives that are regularly reviewed.</li>
<li><strong>Aggregate Data:</strong> KPIs are often based on aggregate data, which is derived from aggregate measurements.</li>
<li><strong>Aggregate Measurements:</strong> These are models that contain a collection of measures (e.g., total sales, average order value) and their corresponding dimensions (e.g., time, product category, customer group). They are used to calculate aggregate numbers.</li>
<li><strong>Dimensions:</strong> These are attributes that allow you to slice and dice the data, providing different perspectives on the measures.</li>
<li><strong>Aggregate Data Entity:</strong> This is an entity that combines aggregate measurements and dimensions, allowing you to define KPIs using X++.</li>
<li><strong>Customization:</strong> Once a KPI is defined, users can customize it at runtime, adjusting parameters or thresholds to suit their needs.</li>
</ul>
<p>The relationship between aggregate measurements, dimensions, and KPIs can be visualized as a process where raw data is transformed into meaningful metrics through aggregation and then used to define KPIs that provide insights into performance.</p>
<p>The text then walks through a detailed example of creating a KPI in Visual Studio, including steps for creating:</p>
<ol>
<li><strong>Aggregate Measurements:</strong> Defining the measures (e.g., total mileage) and linking them to data sources (e.g., a view like "FMVehicleExtendedView"). You can set the default aggregation type to sum, count, average, min, or max.</li>
<li><strong>Aggregate Dimensions:</strong> Defining the dimensions (e.g., vehicle model) that will be used to analyze the measures.</li>
<li><strong>Model Dimension References:</strong> Establishing relationships between aggregate measurements and dimensions.</li>
<li><strong>Aggregate Data Entity:</strong> Combining the measurements and dimensions into an entity that can be used to define the KPI.</li>
<li><strong>KPI Object:</strong> Defining the KPI itself, specifying the measurement, goal type, and other properties.</li>
<li><strong>Tile:</strong> Creating a tile to display the KPI on a workspace.</li>
<li><strong>Workspace Extension:</strong> Adding the tile to a workspace.</li>
</ol>
<p><strong>Drill-through elements</strong> are another important feature of workspaces. They allow users to navigate from a summarized view to a more detailed view of the underlying data. For example, a tile showing the total number of sales could have a drill-through link to a detailed list of individual sales transactions.</p>
<p>The text explains how to create tiles and add them to workspaces, both through the user interface and the developer environment. Tiles can display record counts and provide drill-through access to more detailed information.</p>
<p><strong>Adding a tile</strong> to a workspace through the user interface involves personalizing the workspace and using the "Add to workspace" option from a specific page (e.g., "All customers"). You can configure the tile's name and whether to display a record count.</p>
<p><strong>Adding a tile</strong> through the developer environment requires creating a new tile object in Visual Studio, linking it to a menu item, and then extending an existing workspace to include the tile.</p>
<p><strong>Reusable report functions</strong> using RDL (Report Definition Language) are also discussed. RDL is an XML-based language used to define SSRS reports. You can embed custom functions within RDL files to control report item values, styles, and formatting. RDL can be used to define various report elements, such as charts, graphs, calculations, text, and images. You can create RDL files using .NET Framework classes, such as "XmlTextWriter".</p>
<p>In addition to the elements already mentioned, workspaces can include built-in KPIs, charts, and other components. Users can add these elements directly from the user interface, choosing from presentation types like Tile, List, or Link.</p>
<p><strong>Tile</strong> option allows displaying a record count. </p>
<p><strong>List</strong> option allows selecting the tab name, list style, and columns to display. </p>
<p><strong>Link</strong> option creates a hyperlink to another form.</p>
<p>Developers can create custom KPI tiles that display calculated measures based on aggregate data.</p>
<p><strong>Power BI reports</strong> can also be embedded into workspaces. This involves populating the local Entity Store database with aggregate measurements, setting up a batch job for periodic updates, connecting Power BI to the Entity Store, creating reports, and then publishing and pinning the reports to the workspace.</p>
<p>The exercise at the end demonstrates how to create a custom workspace and add various elements, including a tile, list, link, and Power BI report. This provides a practical example of how to personalize a workspace to meet specific user needs.</p>
<p>In essence, this section provides a comprehensive overview of workspaces, KPIs, and the various elements that can be incorporated into them. It highlights the importance of these features in providing users with a personalized and efficient way to access information and perform tasks within Dynamics 365 Finance and Operations.</p>
<p>Let's wrap up our conversation by exploring the final pieces of the puzzle: role-based security, permissions policies, the Extensible Data Security (XDS) framework, and Microsoft Entra ID/OAuth 2.0 authentication within Dynamics 365 Finance and Operations.</p>
<p><strong>Role-based security</strong> is the foundation of access control in Dynamics 365. Instead of granting access directly to individual users, access is granted to security roles, and users are then assigned to these roles. This approach simplifies security management and aligns security with the organizational structure.</p>
<p>Here's a recap of the key concepts:</p>
<ul>
<li><strong>Roles:</strong> Represent job functions or responsibilities within an organization (e.g., Accounts Payable Clerk, Sales Manager). Users are assigned to one or more roles based on their job duties.</li>
<li><strong>Duties:</strong> Represent parts of a business process (e.g., Maintain Ledgers, Maintain Bank Transactions). They are collections of privileges.</li>
<li><strong>Privileges:</strong> Represent specific tasks or actions that a user can perform (e.g., Generate Deposit Slips, Cancel Payments). They are collections of permissions.</li>
<li><strong>Permissions:</strong> Represent access to individual securable objects, such as menu items, tables, fields, and buttons. They are the most granular level of security control.</li>
</ul>
<p>The hierarchy flows from permissions (most granular) to privileges, then to duties, and finally to roles (least granular). This structure allows for a modular and reusable approach to security, as permissions, privileges, and duties can be combined in different ways to create various roles.</p>
<p>The text emphasizes that while security roles can be managed within the Finance and Operations application, it's recommended that system administrators manage roles, duties, and privileges within Visual Studio. Permissions can only be created or modified in the development environment.</p>
<p>The example of the accounting process cycle helps illustrate how these elements work together. The "Maintain Bank Transactions" duty might include the "Generate Deposit Slips" and "Cancel Payments" privileges. The "Cancel Payments" privilege, in turn, would contain permissions to access the specific menu items, fields, and tables required to cancel payments.</p>
<p><strong>Creating new roles, duties, and privileges</strong> is done within Visual Studio by adding new items to a project in the Solution Explorer. You can also create extensions of existing security elements to modify them without altering the standard definitions.</p>
<p><strong>Permissions policies</strong> are used to grant access to users for specific purposes. They are often based on queries that define the criteria for accessing data. For example, a permissions policy might restrict access to a particular field based on a user's role or team.</p>
<p>A new security policy is added to a project in Visual Studio, and its properties define the query, primary table, and context type. The context type determines when the policy is applicable (e.g., based on role name or a context string).</p>
<p>The <strong>Extensible Data Security (XDS) framework</strong> is used to assign data security policies to security roles. XDS is an evolution of record-level security (RLS) and provides a more flexible and powerful way to control access to data.</p>
<p>An XDS policy typically involves the following:</p>
<ol>
<li><strong>Query:</strong> Defines the filters or limits to be applied to the primary table(s).</li>
<li><strong>Security Policy:</strong> Links the query to the primary table and defines the context for the rule.</li>
<li><strong>Constrained Tables:</strong> Related tables that store data from the primary tables.</li>
</ol>
<p>XDS policies are enforced regardless of how data is accessed, whether through the Finance and Operations application, an SSRS report, or other services.</p>
<p><strong>Assigning users to roles</strong> is done within the System administration module in Finance and Operations. You can automatically assign users to roles based on queries, or you can manually assign them.</p>
<p>The lab exercise demonstrates how to create a new security role ("FMServiceManager") and add duties to it ("FMBasicDuties," "FMManageCars," "FMManageCustomers"). This provides a practical example of how to define a new role based on job responsibilities.</p>
<p><strong>Microsoft Entra ID and OAuth 2.0 authentication</strong> are crucial for securing access to Finance and Operations. Microsoft Entra ID (formerly Azure Active Directory) is the primary identity provider, and users must have an authenticated account to access the system.</p>
<p><strong>OAuth 2.0</strong> is a standard that allows applications to access resources on behalf of a user. In Finance and Operations, OAuth 2.0 is used for integrations, allowing external systems to access the application's API, services, and data.</p>
<p>The text outlines the steps for registering Finance and Operations with OAuth 2.0 in Microsoft Entra ID and configuring the application to use OAuth 2.0. This involves creating an app registration, defining redirect URIs, generating a client secret, configuring API permissions, and then setting up the Microsoft Entra ID applications within Finance and Operations.</p>
<p>In summary, this section provides a comprehensive overview of role-based security, permissions policies, the XDS framework, and Microsoft Entra ID/OAuth 2.0 authentication in Dynamics 365 Finance and Operations. It highlights the importance of these features in securing access to the application and its data, and it provides practical examples and steps for implementing them.</p>
<p>Let's continue our exploration of performance optimization in Dynamics 365 Finance and Operations, focusing on caching, temporary tables, set-based operations, query optimization, and data migration.</p>
<p><strong>Caching</strong> is a technique used to store frequently accessed data in a temporary location (the cache) to improve performance by reducing the number of database calls.</p>
<p>The text discusses two types of table caching:</p>
<ol>
<li><strong>Set-based caching:</strong> Caches all records of a table after the first selection. It's suitable for small tables but should be avoided for large tables, as the cache can move from memory to disk when it reaches 128 KB, which slows down performance. It is configured by setting the "CacheLookup" property of a table to "EntireTable".</li>
<li><strong>Single-record caching:</strong> Caches one record at a time. It's used when the "CacheLookup" property is set to "NotInTTS", "Found", or "FoundAndEmpty", and the "record buffer disableCache" method doesn't return "true". Single-record caches are used on both the client and the Application Object Server (AOS).</li>
</ol>
<p>The text also explains <strong>display method caching</strong>. Display methods are used to show calculated fields on forms. Caching a display method means that it's calculated only once and then retrieved from the cache, improving performance.</p>
<p>You can enable display method caching in two ways:</p>
<ol>
<li>Add the display method to the "init" method on the form using "cacheAddMethod".</li>
<li>Use the "[SysClientCacheDataMethodAttribute]" attribute on the display method.</li>
</ol>
<p><strong>Temporary tables</strong> are used to store and manipulate data efficiently without writing to the main database. The text describes two types:</p>
<ol>
<li><strong>InMemory tables:</strong> Stored in memory until they reach 128 KB, then written to a disk file. They are suitable for small amounts of data and are similar to containers but allow for indexes to speed up data retrieval. They are instantiated when the first record is inserted and exist as long as the object in which they are created exists.</li>
<li><strong>TempDB tables:</strong> Stored in the TempDB database of SQL Server. They are useful for larger amounts of data and are commonly used in reports to manipulate data. They are automatically created for regular tables when their configuration key is disabled.</li>
</ol>
<p>TempDB tables have several capabilities, including:</p>
<ul>
<li>Joining to regular tables</li>
<li>Using foreign keys</li>
<li>Being per company or global</li>
<li>Having indexes</li>
<li>Having methods (but not overriding them)</li>
<li>Being instantiated from the client or server tier</li>
<li>Being used as a query</li>
<li>Having no requirements for a configuration key</li>
</ul>
<p>They also have limitations, such as:</p>
<ul>
<li>Inability to manage date-effective data</li>
<li>No delete actions</li>
<li>No Record Level Security</li>
<li>Cannot be used in views</li>
</ul>
<p>The text provides examples of how to create and use both InMemory and TempDB tables. Key differences include:</p>
<ul>
<li>InMemory tables use "setTmpData" to link data between objects, while TempDB tables use "linkPhysicalTableInstance".</li>
<li>TempDB tables can be global or per company, support transaction scope (TTS), and support joins to regular tables.</li>
</ul>
<p><strong>Set-based operations</strong> are used to update, insert, or delete multiple records at once, while <strong>row-based operations</strong> process one record at a time. Using set-based queries can significantly improve performance by reducing the number of database calls.</p>
<p>Examples of set-based statements include:</p>
<ul>
<li>"update_recordset": Updates multiple records.</li>
<li>"delete_from": Deletes multiple records.</li>
<li>"insert_recordset": Inserts multiple records from one or more tables into another table.</li>
</ul>
<p><strong>Query optimization</strong> is crucial for performance. The text provides several best practices:</p>
<ul>
<li>Pull only the required data by specifying the necessary fields in the query.</li>
<li>Avoid nested queries; use joins instead.</li>
<li>Use the appropriate join type (inner, outer, exists, not exists) to minimize the amount of data retrieved.</li>
<li>Use indexes and index hints for large and rapidly growing tables.</li>
</ul>
<p><strong>Diagnostic validation rules</strong> can be maintained to identify performance optimization opportunities. The Optimization advisor workspace lists potential issues. You can configure the rules and frequency of the performance check in System administration.</p>
<p>Lifecycle Services provides tools for monitoring performance, including SQL Insights and server health metrics.</p>
<p><strong>Data migration</strong> performance can be improved by:</p>
<ul>
<li>Turning off change tracking.</li>
<li>Turning on set-based processing.</li>
<li>Creating a data migration batch group.</li>
<li>Enabling priority-based batch scheduling.</li>
<li>Setting up the maximum number of batch threads (up to 16).</li>
<li>Importing in batch mode.</li>
<li>Cleaning staging tables.</li>
<li>Updating statistics.</li>
<li>Configuring entity execution parameters.</li>
<li>Turning off unnecessary validations.</li>
<li>Separating files into smaller chunks.</li>
<li>Testing performance in an appropriate environment.</li>
<li>Taking notes during testing.</li>
</ul>
<p>In essence, this section provides a comprehensive overview of various performance optimization techniques in Dynamics 365 Finance and Operations, covering caching, temporary tables, set-based operations, query optimization, diagnostic tools, and data migration. It emphasizes the importance of understanding these concepts to build efficient and well-performing applications.</p>
<p>Let's delve into the final aspects of performance optimization and development practices in Dynamics 365 Finance and Operations, focusing on variable scope, concurrency, parallel processing, asynchronous processing, Windows PowerShell scripting, and runnable classes.</p>
<p><strong>Variable scope</strong> determines where a variable can be accessed within your code. Understanding variable scope is crucial for managing memory usage and writing clean, maintainable code.</p>
<ul>
<li><strong>Instance variables:</strong> Declared in the class declarations. They are accessible from any method within the class and from methods in derived classes (unless declared as "private").</li>
<li><strong>Local variables:</strong> Declared within a method. They are only accessible within that specific method.</li>
</ul>
<p>The text emphasizes the importance of using local variables whenever possible to reduce memory overhead. Declaring all variables as instance variables means that memory is allocated for them throughout the lifetime of the object, even if they are only needed within a specific method. Local variables, on the other hand, only consume memory while the method is executing.</p>
<p>The example illustrates the difference between an instance variable ("instanceVariable") and a local variable ("localVariable"). The instance variable is accessible throughout the "ExampleClass", while the local variable is only accessible within the "demonstrateVariableScope" method.</p>
<p><strong>Best practices for variable declaration</strong> include:</p>
<ul>
<li>Use local scope whenever possible.</li>
<li>Avoid unnecessary instance variables.</li>
<li>Organize your code for clarity.</li>
</ul>
<p><strong>Concurrency</strong> controls how multiple users or processes access and modify data simultaneously. Finance and Operations uses two concurrency models:</p>
<ol>
<li><strong>Pessimistic Concurrency Control (PCC):</strong> Locks records as soon as they are retrieved from the database. This prevents other users from modifying the records until the lock is released. It should be used when there's serialization logic that requires locks or when update conflicts are likely.</li>
<li><strong>Optimistic Concurrency Control (OCC):</strong> Locks records only when they are being updated. This allows multiple users to read the same records simultaneously, but it can lead to update conflicts if two users try to modify the same record at the same time. OCC is generally preferred as it improves throughput, uses fewer resources for locking, and keeps records available for longer periods. However, it can lead to performance issues if there are frequent update conflicts and retries.</li>
</ol>
<p>Most standard tables in Finance and Operations use OCC. You can set the concurrency model on the "OccEnabled" table property or override it in a "Select" statement using the "optimisticLock" or "pessimisticLock" keywords.</p>
<p><strong>Parallel processing</strong> involves dividing a task into smaller subtasks that can be executed concurrently, potentially reducing the overall processing time. The text discusses three approaches to batch processing using parallel processing:</p>
<ol>
<li><strong>Individual task modeling:</strong> Creates a separate batch task for each work item. It's suitable for small, interdependent workloads but can lead to performance issues with a large number of tasks due to overhead.</li>
<li><strong>Batch bundling:</strong> Creates a limited number of batch tasks, each processing a "bundle" of work items. It works well for uniform workloads with predictable processing times but can be inefficient for uneven workloads.</li>
<li><strong>Top picking:</strong> Creates a limited number of batch tasks that pick and process the first available work item from a staging table. It's suitable for uneven workloads with varying processing times but can be affected by overhead if there are many small work items.</li>
</ol>
<p>The choice of approach depends on workload characteristics, performance optimization needs, and implementation complexity.</p>
<p><strong>Asynchronous processing</strong> allows long-running processes to execute in the background without blocking the user interface. This is achieved using the "runAsync" methods from the "FormRun" and "Global" classes.</p>
<p>To use asynchronous processing, you need to define a static method that will be executed asynchronously. This method should:</p>
<ul>
<li>Be static.</li>
<li>Return a container as an output parameter.</li>
<li>Have a container for in-parameters.</li>
<li>Have a cancellation token for in-parameters.</li>
</ul>
<p>You can also define a callback method to handle the result of the asynchronous operation.</p>
<p>The "runAsync" function takes several parameters, including the class number, the static method to run, input parameters, a cancellation token, and information about the callback method.</p>
<p><strong>Windows PowerShell</strong> is a scripting language that can be used to automate tasks and interact with Finance and Operations. You can create PowerShell scripts to perform various administrative tasks, such as:</p>
<ul>
<li>Database synchronization</li>
<li>Restarting services</li>
<li>Resetting the data mart</li>
<li>Deploying and installing models</li>
</ul>
<p><strong>Best practices for PowerShell scripting</strong> include:</p>
<ul>
<li>Use error handling.</li>
<li>Log activities.</li>
<li>Test thoroughly.</li>
<li>Follow security best practices.</li>
</ul>
<p><strong>Runnable classes</strong> are a way to test code within Visual Studio. They have a "main" method that serves as the entry point for execution. The lab exercise demonstrates how to create a runnable class, add code to insert a record into the "FMCustomer" table, and then run the class to test the code.</p>
<p>Finally, the text touches upon the importance of avoiding intrusive customizations to reduce upgrade costs and ensure that you're always running the latest version of Finance and Operations. Binary updates are provided monthly, and intrusive customizations can interfere with these updates.</p>
<p>In essence, this section provides a comprehensive overview of various development practices and techniques in Dynamics 365 Finance and Operations, covering variable scope, concurrency, parallel processing, asynchronous processing, Windows PowerShell scripting, runnable classes, and the importance of avoiding intrusive customizations. It emphasizes the importance of writing efficient, maintainable, and optimized code to ensure optimal system performance and user experience.</p>
<p>Let's dive into the world of extensions, customization models, and extending the framework within Dynamics 365 Finance and Operations.</p>
<p><strong>Extensions</strong> are the primary mechanism for customizing Finance and Operations. They allow you to add functionality to existing elements and code without modifying the base application code (which is now restricted). This approach ensures a smoother upgrade process and reduces the risk of conflicts when applying updates or hotfixes.</p>
<p><strong>Customization models</strong> play a crucial role in organizing and managing extensions. While overlaying (modifying base code directly) was possible in previous versions, it's no longer supported in Finance and Operations. Extensions provide a cleaner and more maintainable way to customize the application.</p>
<p>The text highlights the drawbacks of overlaying:</p>
<ul>
<li>Code upgrades are required when applying updates or hotfixes.</li>
<li>Recompiling code takes more time.</li>
<li>Extensive and costly testing is needed.</li>
<li>Compile cycles are longer.</li>
<li>Package distributions are larger.</li>
</ul>
<p><strong>Extension models</strong>, on the other hand, offer several benefits:</p>
<ul>
<li>Simplified and improved performance of builds, test automation, deployments, and delivery.</li>
<li>Unaffected customizations during upgrades, as Microsoft handles installations, patches, and upgrades in the cloud.</li>
<li>No forced recompilation of the entire application during builds.</li>
</ul>
<p>The text explains how to update model parameters in Visual Studio to reference the necessary models for creating extensions. The most common models to reference are "ApplicationCommon", "ApplicationFoundation", "ApplicationPlatform", "ApplicationSuite", and "Directory".</p>
<p>It also describes how to create a new extension model using the Create model wizard, selecting the "Create a new package" option.</p>
<p><strong>Best practices for extensions</strong> include:</p>
<ul>
<li>Reference only necessary models.</li>
<li>Keep customizations modular and reusable.</li>
<li>Adhere to naming conventions.</li>
<li>Document customizations.</li>
</ul>
<p><strong>Extension points for frameworks</strong> allow you to extend various elements within the application, such as:</p>
<ul>
<li><strong>Labels:</strong> Modify label values, add new labels, or add new languages.</li>
<li><strong>Enumerations (Enums):</strong> Add new values to extensible enums.</li>
<li><strong>Extended Data Types (EDTs):</strong> Modify properties like label, string size, or help text.</li>
<li><strong>Tables:</strong> Add new fields, indexes, or relations, and modify properties. You can also implement event handlers for table methods.</li>
<li><strong>Data Entities:</strong> Add data sources or modify properties.</li>
<li><strong>Forms:</strong> Add data sources, controls, or modify properties. You can also implement event handlers for form methods.</li>
<li><strong>Menus:</strong> Add or hide menu items, submenus, or menu references.</li>
</ul>
<p>For customizations that cannot be achieved through extensions, you need to log an <strong>extensibility request</strong> with Microsoft through a specific project in Lifecycle Services. Microsoft engineers review these requests and add them to a backlog. Intrusive customizations that could hinder seamless upgrades are generally not supported.</p>
<p><strong>Guidelines to avoid intrusive customizations</strong> include:</p>
<ul>
<li>Do not change a type's definition.</li>
<li>Allow model authors to maintain control of encapsulated code and types.</li>
<li>Focus on adding new behaviors through extensions.</li>
<li>Design extension capabilities to be open for extensions and support multiple extensions side by side.</li>
</ul>
<p>The general steps for creating an extension involve:</p>
<ol>
<li>Expanding the AOT node in the Application Explorer.</li>
<li>Expanding the node for the element type you want to extend (e.g., Tables).</li>
<li>Right-clicking the specific element you want to extend.</li>
<li>Selecting "Create extension" (to add it to the current project) or "Create extension in new project."</li>
</ol>
<p><strong>Developing code to extend a framework</strong> often involves creating class extensions or adding event handlers.</p>
<ul>
<li><strong>Class extensions (Augmentation Classes):</strong> Used to extend the business logic related to a table or other elements. You decorate the class with the "ExtensionOf" attribute, use the "_Extension" suffix, and mark the class as "final".</li>
<li><strong>Event handlers:</strong> Allow you to write code that executes when a specific event occurs (e.g., a field is modified, a record is deleted). You can copy event handler methods from the Events node of an element in the designer window.</li>
<li><strong>Chain of Command (CoC):</strong> Enables wrapping logic around public and protected methods in base classes without using event handlers. You use the "next" keyword to create a chain of methods, ensuring that all methods in the chain, including the original implementation, are executed.</li>
</ul>
<p><strong>Pre-event and post-event handler classes</strong> provide an alternative to CoC for extending methods. You create a new event handler class and add pre-event and post-event methods, which are executed before and after the original method, respectively.</p>
<p>Attributes like "Hookable", "Wrappable", and "Replaceable" can be used to control the extensibility of methods:</p>
<ul>
<li><strong>"Hookable":</strong> Determines whether pre-events and post-events can be created.</li>
<li><strong>"Wrappable":</strong> Determines whether a method can be wrapped using CoC.</li>
<li><strong>"Replaceable":</strong> Determines whether a CoC extension must unconditionally call "next".</li>
</ul>
<p>The <strong>SysOperationSandbox framework</strong> is used to run time-consuming operations asynchronously in a separate session, preventing the web client from freezing and avoiding timeouts. It creates two client sessions: one for the business logic and one to communicate with the user interface. The framework handles the serialization of the class state between sessions.</p>
<p>The example provided illustrates how to use the "SysOperationSandbox" framework to add 100 new check numbers to a custom "BankCheque" table. It involves creating a class with the business logic and "pack"/"unpack" methods for serialization.</p>
<p>In essence, this section provides a comprehensive overview of extensions, customization models, and various techniques for extending the framework in Dynamics 365 Finance and Operations. It emphasizes the importance of using extensions over overlaying, following best practices, and understanding the different extension points and mechanisms available for customizing the application.</p>
<p>Let's continue our exploration of development in Dynamics 365 Finance and Operations, focusing on the implementation of the workflow framework.</p>
<p>Workflows are automated business processes that define how a document or task flows through the system, specifying who needs to complete a task, make a decision, or approve a document. You can configure workflows within the application or create them in the development environment using Visual Studio.</p>
<p>The text focuses on creating a workflow and its elements from the development environment. Key workflow elements include:</p>
<ul>
<li><strong>Workflow categories:</strong> Specifies the module to which the workflow belongs.</li>
<li><strong>Workflow approvals:</strong> Define the approval steps within a workflow.</li>
<li><strong>Workflow manual and automated tasks:</strong> Define the tasks that need to be performed within a workflow.</li>
<li><strong>Workflow types:</strong> Define the overall structure and behavior of a workflow.</li>
<li><strong>Flow-control elements:</strong> Control the flow of the workflow (e.g., conditional branches).</li>
<li><strong>Providers:</strong> Handle specific aspects of the workflow, such as notifications or assignments.</li>
</ul>
<p>The process of creating a new workflow type involves several steps, starting with creating the required elements:</p>
<ol>
<li><strong>Workflow category:</strong> Defines the module for the workflow.</li>
<li><strong>Query:</strong> Specifies the data that the workflow will operate on.</li>
<li><strong>Document menu item:</strong> Points to the form associated with the workflow.</li>
</ol>
<p>The text provides detailed instructions on how to create these elements using Visual Studio, including:</p>
<ul>
<li>Creating a new project for the workflow development.</li>
<li>Adding a workflow category item and setting its properties (module, label).</li>
<li>Creating a base enumerator to manage the workflow's status (e.g., Submit, Started, Canceled, Complete).</li>
<li>Creating a table to store workflow-related data, including the workflow status.</li>
<li>Creating a form to display the workflow data.</li>
<li>Adding a display menu item for the form.</li>
<li>Adding the display menu item to the "InventoryManagement" menu extension.</li>
<li>Creating a query to retrieve the workflow data.</li>
</ul>
<p>Once these elements are in place, you can create the <strong>workflow type</strong> using the Workflow Type wizard in Visual Studio. The wizard prompts you for the workflow category, query, and document menu item that you created earlier.</p>
<p>After creating the workflow type, you need to:</p>
<ul>
<li>Modify the labels of the generated action menu items (e.g., Cancel, Submit).</li>
<li>Implement the "SubmitManager" class to handle the submission of the workflow.</li>
<li>Add a method to the workflow table to update the workflow status.</li>
<li>Override the "canSubmitToWorkflow" method in the workflow table to control when the workflow can be submitted.</li>
<li>Implement the event handler class for the workflow type to update the status when the workflow is started, canceled, or completed.</li>
<li>Configure the workflow properties in the form (e.g., title data source, workflow data source, workflow enabled, workflow type).</li>
</ul>
<p>Next, you create a <strong>workflow approval</strong> element, which defines an approval step within the workflow. This involves:</p>
<ul>
<li>Adding a preview field group to the workflow table to display relevant information in the approval step.</li>
<li>Creating a new workflow approval item using the Workflow Approval wizard.</li>
<li>Implementing the event handler class for the workflow approval to update the status for each event (e.g., started, canceled, completed, denied, change requested, returned).</li>
<li>Setting labels for the workflow approval action menu items (e.g., Approve, Delegate, Reject, Request change, Re-submit).</li>
<li>Adding a workflow element reference to the workflow type to link the workflow approval to the workflow type.</li>
</ul>
<p>Finally, to test the created workflow, you need to activate it within the Finance and Operations application:</p>
<ul>
<li>Navigate to Inventory Management &gt; Setup &gt; Inventory management workflows.</li>
<li>Create a new workflow configuration based on your workflow type.</li>
<li>Configure the workflow in the workflow designer, adding the workflow approval element and connecting the elements.</li>
<li>Configure the basic settings and user assignments for the workflow approval step.</li>
<li>Save and activate the workflow configuration.</li>
</ul>
<p>The example provided in the text uses a custom table ("MyWorkflowTable") and form ("MyWorkflowTableForm") to demonstrate the workflow creation process. It involves creating a workflow type ("MyWorkflowType") and a workflow approval ("MyWorkflowApproval") to manage the approval process for records in the custom table. The workflow status is tracked using a custom enumeration ("MyWorkflowStatus").</p>
<p>In essence, this section provides a detailed guide on how to implement the workflow framework in Dynamics 365 Finance and Operations using Visual Studio. It covers the creation of various workflow elements, the configuration of the workflow type and approval, and the steps involved in testing the workflow within the application. The example demonstrates how to create a custom workflow to manage the approval process for a custom table, but the same principles can be applied to create workflows for standard tables and processes as well.</p>
<p>Let's continue our journey into extending elements in Dynamics 365 Finance and Operations, focusing on table and form extensions.</p>
<p><strong>Table extensions</strong> allow you to modify existing tables by adding fields, field groups, indexes, mappings, and relations. You can also modify properties of the table or its individual elements.</p>
<p><strong>Adding fields</strong> to a table extension can be done in two ways:</p>
<ol>
<li>Right-clicking the Fields node of the extended table and selecting "New."</li>
<li>Dragging a field from the AOT or Solution Explorer window to the Fields node.</li>
</ol>
<p>Once a field is added, you can drag it to an existing field group or create a new field group for it. Added fields and field groups appear in bold type in the designer, while existing elements are in italic type.</p>
<p><strong>Table indexes</strong> can be added to improve the performance of database searches. You can use existing or newly added fields to create an index. However, creating a unique index through an extension is not allowed as it's considered an intrusive customization.</p>
<p><strong>Relations</strong> define links between tables and help ensure referential integrity. Adding a relation to an extended table follows the same process as adding a relation to a new table.</p>
<p>You can adjust properties of the table or its fields, such as the label or help text. Some properties, like "Created By" or "Modified Date Time," can be set to "Yes" to enable tracking, but they cannot be changed to "No" if they are set to "Yes" on the base table. Properties that cannot be modified appear dimmed in the Properties window.</p>
<p><strong>Table business logic</strong> can be extended by implementing event handlers that are called from the base implementations of the table methods. This involves creating an augmentation class decorated with the "ExtensionOf" attribute and the "_Extension" suffix. You can then add methods to this class and call them from event handlers.</p>
<p>The example provided demonstrates how to create an augmentation class for the "InventTable" table and an event handler class that calls a method from the augmentation class when a new record is being inserted.</p>
<p><strong>Form extensions</strong> allow you to modify existing forms by adding controls, enabling/disabling existing controls, changing control visibility, modifying properties (e.g., Label, Caption, Help text), adding data sources, or adding form parts.</p>
<p>You can also implement event handlers for form methods to extend the form's behavior. However, not all properties or controls can be edited through an extension. For example, the "Name" property might be dimmed, indicating that it cannot be changed.</p>
<p>Some tasks, like changing a form pattern, might require creating a new form instead of using an extension.</p>
<p>The screenshot provided illustrates adding a tab, button group, and menu item button to the Action Pane of the "FMCustomer" form extension.</p>
<p><strong>Adding a data source</strong> to a form extension involves the following steps:</p>
<ol>
<li>Create an extension of the form.</li>
<li>Rename the extension by adding a company-specific suffix.</li>
<li>Drag a table from the AOT or Solution Explorer to the Data Sources node of the form.</li>
<li>Drag fields from the new data source to the Grid control in the design pane.</li>
</ol>
<p>The lab exercise demonstrates how to extend the "FMCustomer" form and add a tab, button group, and button to the Action Pane to link to the free text invoice page.</p>
<p>In essence, this section provides a comprehensive overview of table and form extensions in Dynamics 365 Finance and Operations. It explains how to add fields, indexes, relations, and modify properties for tables. It also covers extending form functionality by adding controls, modifying properties, and adding data sources. The lab exercise provides a practical example of extending a form to meet specific business requirements.</p>
<p>Let's delve into the realm of business events within Dynamics 365 Finance and Operations, focusing on their purpose, implementation, and management.</p>
<p><strong>Business events</strong> provide a mechanism for notifying external systems about significant occurrences within Finance and Operations. They are triggered when a business process is executed, and they can be used for integration and notification scenarios.</p>
<p><strong>Types of Business Actions:</strong></p>
<ul>
<li><strong>Workflow actions:</strong> Actions related to workflow processes, such as approving a purchase requisition.</li>
<li><strong>Non-workflow actions:</strong> Actions not directly related to workflows, such as confirming a purchase order.</li>
</ul>
<p>Both types of actions can generate business events.</p>
<p><strong>Class extensions</strong> are used to implement new business events.</p>
<p><strong>Accessing Existing Business Events:</strong></p>
<p>You can find a list of existing business events in the <strong>Business event catalog</strong> under System administration &gt; Setup &gt; Business events. The catalog allows you to filter and search for specific events.</p>
<p><strong>Business Event Framework:</strong></p>
<p>The framework determines whether a business event is published to a user. It's best practice to have the application send a business event regardless of whether it's enabled, unless there's a significant performance impact or additional logic is required.</p>
<p><strong>Activating Business Events:</strong></p>
<p>Business events are inactive by default. To activate them, you need to go to the Business events page in the System administration module, select the desired events, and click "Activate." You can specify whether the event applies to all legal entities or a specific one.</p>
<p><strong>Deactivating Business Events:</strong></p>
<p>You can deactivate active business events from the "Active business events" tab. Deactivated events are moved to the "Inactive business events" tab, retaining their error history.</p>
<p><strong>Business Event Catalog:</strong></p>
<p>The catalog lists all available business events and provides information about them, including:</p>
<ul>
<li><strong>Category:</strong> Identifies the source of the event (e.g., Workflow, SalesOrder).</li>
<li><strong>Business event ID:</strong> A unique identifier for the event.</li>
<li><strong>Name:</strong> A descriptive name for the event.</li>
<li><strong>Description:</strong> A detailed explanation of the event and its context.</li>
<li><strong>Data fields:</strong> The payload of the event (the data that will be sent).</li>
</ul>
<p>You can download the JSON schema for a business event's payload from the catalog.</p>
<p><strong>Business Events Parameters:</strong></p>
<p>The Business events parameters page provides settings for managing business events.</p>
<p><strong>General Tab:</strong></p>
<ul>
<li><strong>Retry count:</strong> The number of times the system will retry sending an event if an error occurs.</li>
<li><strong>Wait time between retries:</strong> The interval between retry attempts.</li>
<li><strong>Endpoints allowed per event:</strong> The maximum number of endpoints that can subscribe to an event in a legal entity.</li>
<li><strong>Use business events batch job:</strong> Enables using a batch job for processing events (used as a workaround for issues with dedicated processing).</li>
<li><strong>Key vault secret cache interval:</strong> The caching duration for key vault secrets used for business events.</li>
</ul>
<p><strong>Performance Tab:</strong></p>
<ul>
<li><strong>Processing threads:</strong> The number of threads used to process events.</li>
<li><strong>Bundle size:</strong> The number of events grouped together for processing by a thread.</li>
</ul>
<p><strong>Creating a New Business Event:</strong></p>
<p>Implementing a new business event involves creating two classes:</p>
<ol>
<li><strong>Business event class:</strong>
<ul>
<li>Extends the "BusinessEventsBase" class.</li>
<li>Includes the "BusinessEvents" attribute to provide information about the event (contract, name, description).</li>
<li>Contains methods for constructing the event ("newFrom"), maintaining internal state ("parm"), and building the contract ("buildContract").</li>
</ul>
</li>
<li><strong>Business events contract class:</strong>
<ul>
<li>Extends the "BusinessEventsContract" class.</li>
<li>Defines the payload of the event.</li>
<li>Allows populating the contract at runtime.</li>
</ul>
</li>
</ol>
<p><strong>Example: "SalesInvoicePostedBusinessEvent"</strong></p>
<p>The text provides an example of creating a "SalesInvoicePostedBusinessEvent" class and its associated contract class.</p>
<p><strong>Key Methods in the Business Event Class:</strong></p>
<ul>
<li><strong>"newFrom":</strong> A static method that creates a new instance of the business event class and initializes it with data from the relevant table buffer (e.g., "CustInvoiceJour").</li>
<li><strong>"parm":</strong> Private methods to set and get internal state variables of the class.</li>
<li><strong>"buildContract":</strong> Populates the business event contract with data. It's decorated with "Wrappable(true)" and "Replaceable(true)" attributes.</li>
</ul>
<p><strong>Business Events Contract Class:</strong></p>
<ul>
<li>Defines the structure of the data that will be sent as part of the business event.</li>
</ul>
<p>In essence, this section provides a comprehensive overview of business events in Dynamics 365 Finance and Operations, covering their purpose, the framework, the catalog, parameters, and the steps involved in creating a new business event. The example of creating a "SalesInvoicePostedBusinessEvent" helps illustrate the process of implementing a custom business event.</p>
<p>Let's continue our exploration of business events in Dynamics 365 Finance and Operations, focusing on extending the "BusinessEventContract" class, activating and consuming business events, handling errors, and applying role-based security.</p>
<p><strong>Extending the "BusinessEventContract" Class:</strong></p>
<p>The "BusinessEventContract" class defines the payload of a business event. You can extend it to add new data elements to the event.</p>
<p><strong>Steps to Extend:</strong></p>
<ol>
<li>Find the business event class name in the Business event catalog (System administration &gt; Setup &gt; Business events &gt; Business event catalog). The class name is in the "Business event ID" field.</li>
<li>Locate the class in Visual Studio using the Application Explorer.</li>
<li>Right-click the class name and select "Create code extension."</li>
<li>Add new private variables to hold the contract state.</li>
<li>Add a private "initialize" method to set the private variables based on data from the table buffer.</li>
<li>Add a static "newFrom" method decorated with "[ExtensionOf]" to create an instance of the extended contract and call the "initialize" method.</li>
<li>Add "parm" methods decorated with "DataMember" and "BusinessEventsDataMember" attributes to access the contract state. These methods should have a corresponding label for the "BusinessEventsDataMember" attribute.</li>
</ol>
<p><strong>Example: Extending "SalesInvoicePostedBusinessEventContract"</strong></p>
<p>The text provides an example of extending the "SalesInvoicePostedBusinessEventContract" class to add a "CustName" field.</p>
<p><strong>Key Elements in the Extension:</strong></p>
<ul>
<li><strong>"[ExtensionOf(classStr(SalesInvoicePostedBusinessEventContract))]":</strong> Decorates the extension class, linking it to the original contract class.</li>
<li><strong>"private CustName name;":</strong> A new private variable to store the customer's name.</li>
<li><strong>"[DataMember('Name'), BusinessEventsDataMember("@AccountsReceivable:OrganizationName")]":</strong> Decorates the "parmName" method, making it part of the data contract and providing a label for the business event.</li>
<li><strong>"newFromCustInvoiceJour":</strong> A static method that creates an instance of the extended contract and calls the "myInitialize" method to populate the new data member.</li>
<li><strong>"myInitialize":</strong> A private method that sets the "name" variable based on data from the "CustInvoiceJour" table buffer.</li>
</ul>
<p><strong>Activating Business Events:</strong></p>
<p>Business events are inactive by default. To activate them:</p>
<ol>
<li>Go to the Business event catalog (System administration &gt; Setup &gt; Business events).</li>
<li>Select the desired events.</li>
<li>Click "Activate."</li>
<li>Specify whether to activate the events in all legal entities or a specific one.</li>
</ol>
<p><strong>Endpoints:</strong></p>
<p>Activated business events need endpoints to receive the event data. Endpoints can be various services, such as Azure Service Bus, Event Grid, Event Hubs, Blob Storage, HTTPS, or Power Automate.</p>
<p><strong>Error Handling:</strong></p>
<p>If an error occurs while sending a business event, the system retries several times. Unsuccessful attempts are logged in the error log. You can access error logs from the "Active events," "Inactive events," or "Errors" tabs.</p>
<p><strong>Actions on Errors:</strong></p>
<ul>
<li><strong>Resend:</strong> Manually retry sending the event.</li>
<li><strong>Download payload:</strong> Download the event data for offline processing.</li>
</ul>
<p><strong>Subscribing to Business Events from a Service:</strong></p>
<p>Users can only see and subscribe to business events that their roles have access to. The organizational assignments in role-based security are honored, allowing users to subscribe to events only in the organizations they have access to.</p>
<p><strong>Backward Compatibility:</strong></p>
<ul>
<li>Role-based security for business events is disabled by default.</li>
<li>Even if enabled in Feature management, role-based security won't take effect unless explicitly enabled in the Business events catalog security menu.</li>
<li>After enabling, security will be enforced. Non-admin users will only see events their roles are assigned to.</li>
</ul>
<p><strong>Consuming Business Events:</strong></p>
<p>Different endpoint types can consume activated business events.</p>
<p><strong>Business Events and Power Automate:</strong></p>
<p>You can use the "When a Business Event occurs" trigger in Power Automate to subscribe to business events.</p>
<p><strong>Steps to Set Up a Power Automate Flow:</strong></p>
<ol>
<li>Create a new automated cloud flow.</li>
<li>Add the "When a Business Event occurs" trigger (search for "Dynamics").</li>
<li>Specify the instance, category, business event, and legal entity.</li>
<li>Add a "Parse JSON" action to parse the event message.</li>
<li>Download the schema from the Business event catalog and use it to generate the JSON schema in Power Automate.</li>
<li>Change the type of integer fields to "number" in the schema.</li>
<li>Add other actions to consume the event data (e.g., "Get a record," "Send an email").</li>
</ol>
<p><strong>Extending an Existing Business Event:</strong></p>
<ol>
<li>Create a code extension for the business event contract class.</li>
<li>Add new data members and methods to the extension class.</li>
<li>Save and build the project.</li>
<li>The updated schema will be available in the Business event catalog.</li>
<li>Update the Power Automate flow to use the new data members (if applicable).</li>
</ol>
<p><strong>Testing the Business Event Extension:</strong></p>
<ol>
<li>Perform an action that triggers the business event (e.g., post an invoice).</li>
<li>The Power Automate flow will run, and the email will be sent with the extended data.</li>
</ol>
<p><strong>Role-Based Security for Business Events:</strong></p>
<p>You can apply role-based security to control access to business events using privileges and duties:</p>
<ul>
<li><strong>"BusinessEventsCatalogView":</strong> View the business events catalog.</li>
<li><strong>"BusinessEventsCatalogMaintain":</strong> Activate business events.</li>
<li><strong>"Business events security privilege":</strong> Create and manage endpoints.</li>
<li><strong>"Subscribe to business events from service":</strong> Subscribe to business events from external applications.</li>
<li><strong>"BusinessEventsCatalogSecuritySetupView":</strong> View business events security setup.</li>
<li><strong>"Maintain business events catalog security":</strong> Manage business events security.</li>
</ul>
<p><strong>Business Events in Microsoft Power Automate:</strong></p>
<p>The "When a business event occurs" trigger allows you to subscribe to business events in Power Automate.</p>
<p><strong>Subscribing and Unsubscribing:</strong></p>
<p>When adding the trigger, you need to provide the instance, category, business event, and legal entity.</p>
<p>In essence, this section provides a comprehensive guide to extending business event contracts, activating and consuming business events, handling errors, applying role-based security, and using business events with Power Automate. It emphasizes the importance of extending existing business events to add new functionality and provides practical examples and steps for implementing these extensions. The example of extending the "SalesInvoicePostedBusinessEventContract" and updating the Power Automate flow demonstrates how to add a new data member and use it in an integration scenario.</p>
<p>Let's continue our exploration of data integration in Dynamics 365 Finance and Operations, focusing on data entities, custom services, and various integration scenarios.</p>
<p><strong>Data entities</strong> are a key concept in data integration. They provide a de-normalized view of data that is spread across multiple tables, simplifying development and enabling various integration scenarios.</p>
<p><strong>Key Characteristics of Data Entities:</strong></p>
<ul>
<li><strong>De-normalized view:</strong> Data from multiple tables is combined into a single, simplified view.</li>
<li><strong>Abstraction:</strong> They provide an abstraction layer over the underlying table structure.</li>
<li><strong>Business logic:</strong> They can encapsulate business logic related to the data.</li>
<li><strong>Public APIs:</strong> They expose public APIs for synchronous access (OData).</li>
<li><strong>Data management:</strong> They support asynchronous data import/export and recurring integrations.</li>
</ul>
<p><strong>Common Uses of Data Entities:</strong></p>
<ul>
<li>Data import/export</li>
<li>Integration with external systems</li>
<li>OData services</li>
<li>Custom services</li>
<li>Microsoft Power Apps</li>
<li>Microsoft Excel (Dynamics add-in)</li>
<li>Electronic Data Interchange (EDI)</li>
<li>Data validation</li>
</ul>
<p><strong>Developing and Enabling a Data Entity for Export:</strong></p>
<ol>
<li><strong>Define data sources:</strong> Select the primary and secondary tables for the entity.</li>
<li><strong>Enable public API and data management:</strong> This creates a staging table for import/export and enables OData access.</li>
<li><strong>Select fields:</strong> Choose the fields from the primary and secondary tables to include in the entity.</li>
<li><strong>Rename fields (optional):</strong> You can rename fields or use labels to generate names.</li>
<li><strong>Define the natural key:</strong> Select the field(s) that uniquely identify a record in the entity.</li>
</ol>
<p><strong>Creating a New Data Entity (Video Demonstration):</strong></p>
<p>The text refers to a video that demonstrates the process of creating a new data entity in Visual Studio.</p>
<p><strong>Adding a Data Entity Using the Data Entity Wizard:</strong></p>
<ol>
<li>Right-click the project in Solution Explorer and select Add &gt; New Item &gt; Data Model &gt; Data Entity.</li>
<li>Provide a name for the entity (e.g., "mySalesPoolEntity").</li>
<li>In the wizard, specify the primary data source (e.g., "SalesPool" table).</li>
<li>Enable "Public API" and provide names for the public entity and collection.</li>
<li>Select the fields to include in the entity.</li>
<li>Build the project with "Synchronize Database on Build" set to "true".</li>
</ol>
<p><strong>Testing the Data Entity:</strong></p>
<ul>
<li><strong>X++:</strong> You can use a runnable class to test creating, reading, updating, and deleting records through the entity.</li>
<li><strong>OData:</strong> Access the entity through a URL like "[your base URL]/data/MySalesPoolEntities".</li>
</ul>
<p><strong>Adding an Action to the Data Entity:</strong></p>
<ol>
<li>
<p>Open the data entity's code in Visual Studio.</p>
</li>
<li>
<p>Add a new method decorated with the "SysODataActionAttribute" attribute.</p>
<p>"""x++<br>
public class MySalesPoolEnitity extends common<br>
{<br>
[SysODataActionAttribute("Check", false)]<br>
public static str check(SalesPoolId _salesPoolId)<br>
{<br>
// Your logic here<br>
return SalesPool::exist(_salesPoolId) ? "Sales pool exist" : "Sales pool does not exist";<br>
}<br>
}<br>
"""</p>
</li>
<li>
<p>Build the project.</p>
</li>
<li>
<p>Restart IIS Express to update the OData endpoint.</p>
</li>
<li>
<p>Test the action using a URL like "[your base URL]/data/MySalesPoolEntities/Microsoft.Dynamics.DataEntities.Check".</p>
</li>
</ol>
<p><strong>Finding Metadata and Actions:</strong></p>
<p>You can find metadata and actions for data entities using the URL "[your base URL]/Metadata/PublicEntities/".</p>
<p><strong>Extending a Data Entity:</strong></p>
<p>The text doesn't explicitly cover extending existing data entities, but it's a common practice. You can create an extension of a data entity to add new fields, methods, or modify existing properties, similar to how you extend tables and forms.</p>
<p>In essence, this section provides a detailed overview of data entities in Dynamics 365 Finance and Operations, covering their purpose, characteristics, development process, testing, and usage in various integration scenarios. It also demonstrates how to add actions to data entities and access them through OData. The example of creating a "mySalesPoolEntity" illustrates the steps involved in building a custom data entity.</p>
<p>Let's continue our deep dive into data integration, focusing on extending data entities, implementing custom services, the Batch OData API, consuming external web services, and securing sensitive information with Azure Key Vault.</p>
<p><strong>Extending Data Entities:</strong></p>
<p>When you add fields to tables, you often need to update the corresponding data entities to include those fields. You can extend existing data entities to incorporate these changes.</p>
<p><strong>Extending a Data Entity Involves:</strong></p>
<ol>
<li><strong>Creating an extension of the data entity:</strong>
<ul>
<li>Find the data entity in the Application Explorer (under Data Model).</li>
<li>Right-click the entity and select "Create extension."</li>
</ul>
</li>
<li><strong>Adding the new field to the extension:</strong>
<ul>
<li>Go to the Data sources node of the entity extension.</li>
<li>Select the relevant table (e.g., CustTable).</li>
<li>Find the new field (e.g., PhoneMobile).</li>
<li>Copy the field.</li>
<li>Paste the field under the Fields node of the data entity extension.</li>
</ul>
</li>
<li><strong>Building and synchronizing the project:</strong> This ensures the changes are reflected in the database.</li>
</ol>
<p><strong>Extending a Staging Table:</strong></p>
<p>Data entities use staging tables for import/export operations. When you extend a data entity, you also need to extend the corresponding staging table.</p>
<p><strong>Steps to Extend a Staging Table:</strong></p>
<ol>
<li><strong>Create an extension of the staging table:</strong>
<ul>
<li>Find the staging table (e.g., CustCustomerV3Staging) in the Application Explorer.</li>
<li>Right-click the table and select "Create extension."</li>
</ul>
</li>
<li><strong>Add the new field to the staging table extension:</strong>
<ul>
<li>Drag the extended data type (EDT) of the field (e.g., PhoneMobile) to the Fields node of the staging table extension.</li>
</ul>
</li>
<li><strong>Build and synchronize the project:</strong> This updates the staging table in the database.</li>
</ol>
<p><strong>Important Considerations:</strong></p>
<ul>
<li>The data types of fields in the data entity extension and staging table extension should match.</li>
<li>Refreshing the entity list in Finance and Operations (Data management &gt; Framework parameters &gt; Entity settings &gt; Refresh entity list) ensures that all entities, including extensions, are available and have the latest metadata.</li>
</ul>
<p><strong>Implementing Custom Services:</strong></p>
<p>Custom services expose X++ functionality to external clients. They are useful when you need to expose complex business logic that goes beyond simple data access.</p>
<p><strong>Components of a Custom Service:</strong></p>
<ul>
<li><strong>Data contract request class:</strong> Defines the input parameters for the service.</li>
<li><strong>Service class:</strong> Contains the business logic.</li>
<li><strong>Data contract response class:</strong> Defines the output of the service.</li>
</ul>
<p><strong>Steps to Create a Custom Service:</strong></p>
<ol>
<li><strong>Create a data contract request class:</strong>
<ul>
<li>Add the "DataContractAttribute" to the class.</li>
<li>Add "DataMemberAttribute" to each input parameter.</li>
</ul>
</li>
<li><strong>Create a data contract response class:</strong>
<ul>
<li>Add the "DataContractAttribute" to the class.</li>
<li>Add "DataMemberAttribute" to each output parameter.</li>
</ul>
</li>
<li><strong>Create the service class:</strong>
<ul>
<li>Implement the business logic in a method that takes the request class as input and returns the response class.</li>
</ul>
</li>
<li><strong>Add the service class to a service:</strong>
<ul>
<li>Create a new service in the project.</li>
<li>Add the service class to the service's properties.</li>
<li>Add a new service operation and select the method from the service class.</li>
</ul>
</li>
<li><strong>Add the service to a service group:</strong>
<ul>
<li>Create a new service group in the project.</li>
<li>Drag the service to the service group.</li>
</ul>
</li>
<li><strong>Build the project.</strong></li>
<li><strong>Test the service using a tool like Postman or Insomnia.</strong></li>
</ol>
<p><strong>Example: Creating a Custom Service to Summarize Two Numbers:</strong></p>
<p>The text provides a detailed example of creating a custom service that takes two numbers as input and returns their sum.</p>
<p><strong>Batch OData API:</strong></p>
<p>The Batch OData API allows you to reschedule batch jobs using OData. It can be combined with business events to automate rescheduling in case of failures.</p>
<p><strong>Example: Rescheduling a Failed Batch Job Using Power Automate:</strong></p>
<ol>
<li>Create an automated flow in Power Automate.</li>
<li>Use the "When a Business Event occurs" trigger (category: Batch, event: Batch job failed).</li>
<li>Add a "Parse JSON" action to parse the event message.</li>
<li>Download the schema for the "Batch job failed" event from the Business event catalog and use it in the "Parse JSON" action.</li>
<li>Add a "Condition" action to filter for a specific batch job ID.</li>
<li>In the "True" branch, add an "Execute action" (Fin &amp; Ops) to call the "BatchJobs-SetBatchJobToWaiting" action, providing the batch job ID.</li>
</ol>
<p><strong>Exposing OData Endpoints from Data Entities:</strong></p>
<p>OData provides a RESTful way to access data entities. Data entities with the "IsPublic" property set to "True" are exposed as OData endpoints.</p>
<p><strong>Consuming External Web Services:</strong></p>
<p>You can consume external web services from within Finance and Operations using an HTTP client.</p>
<p><strong>Securing Sensitive Information with Azure Key Vault:</strong></p>
<ul>
<li>Store secrets (API keys, passwords, etc.) in Azure Key Vault instead of directly in Finance and Operations.</li>
<li>Configure Key Vault parameters in System administration &gt; Setup &gt; Key Vault parameters.</li>
<li>Retrieve secrets from Key Vault in X++ using the "KeyVaultCertificateHelper" class.</li>
</ul>
<p><strong>Creating an HTTP Client:</strong></p>
<p>The text provides an example of creating an HTTP client in X++ to send a request to an external web service, using an API key retrieved from Azure Key Vault.</p>
<p><strong>Key Elements in the HTTP Client Example:</strong></p>
<ul>
<li><strong>"getKeyVaultSecretValue":</strong> A method to retrieve a secret from Azure Key Vault.</li>
<li><strong>"getResponse":</strong> A method to send an HTTP request and get the response.</li>
<li><strong>"System.Net.HttpWebRequest":</strong> Used to create the HTTP request.</li>
<li><strong>"System.Net.HttpWebResponse":</strong> Used to get the HTTP response.</li>
<li><strong>"StreamWriter":</strong> Used to write the JSON payload to the request stream.</li>
</ul>
<p>In essence, this section provides a comprehensive overview of extending data entities, implementing custom services, using the Batch OData API, consuming external web services, and securing sensitive information with Azure Key Vault in Dynamics 365 Finance and Operations. It emphasizes the importance of extending existing elements instead of creating duplicates and provides detailed examples and steps for implementing these concepts. The examples of extending the "CustCustomerV3Entity" data entity, creating a custom service to summarize numbers, and using Power Automate to reschedule failed batch jobs illustrate practical applications of these techniques.</p>
<p>Let's continue our discussion of data integration in Dynamics 365 Finance and Operations, focusing on the Data Management Framework's Package REST API, custom services, and the distinction between cloud and on-premises deployments.</p>
<p><strong>Data Management Framework's Package REST API:</strong></p>
<p>This API enables integration with Finance and Operations using data packages. It supports both import and export operations and can be used in cloud and on-premises deployments.</p>
<p><strong>Key Features:</strong></p>
<ul>
<li><strong>RESTful API:</strong> Uses standard HTTP methods (e.g., POST) for interaction.</li>
<li><strong>Data Packages:</strong> Works with data packages, which are compressed files containing data and metadata.</li>
<li><strong>Cloud and On-Premises:</strong> Supports both deployment models.</li>
<li><strong>Authorization:</strong> Uses OAuth 2.0 for cloud deployments and AD FS for on-premises deployments.</li>
<li><strong>Asynchronous Operations:</strong> Primarily designed for asynchronous operations, suitable for large data volumes.</li>
</ul>
<p><strong>Custom Services:</strong></p>
<p>Custom services expose X++ functionality to external clients. When deployed, they are available on two endpoints:</p>
<ul>
<li><strong>SOAP:</strong> For SOAP-based integrations.</li>
<li><strong>JSON:</strong> For RESTful integrations using JSON.</li>
</ul>
<p><strong>Import and Export APIs:</strong></p>
<p><strong>Importing APIs:</strong></p>
<ul>
<li><strong>"ImportFromPackage":</strong> Initiates an import operation from a data package.</li>
<li><strong>Cloud Deployments:</strong> The data package is uploaded to Azure Blob storage associated with the Finance and Operations instance.</li>
<li><strong>On-Premises Deployments:</strong> The data package is uploaded to local storage.</li>
<li><strong>Parameters:</strong>
<ul>
<li>"packageUrl" (Cloud): A unique file name (often including a GUID) to track the blob in Azure Storage.</li>
<li>"definitionGroupId": The name of the data project for the import.</li>
<li>"executionId": An optional ID for the job. If not provided, a new one is generated.</li>
<li>"execute": A Boolean indicating whether to run the target step.</li>
<li>"overwrite": A Boolean indicating whether to overwrite existing data (set to "false" for composite entities).</li>
<li>"legalEntityId": The legal entity for the import.</li>
</ul>
</li>
</ul>
<p><strong>Exporting APIs:</strong></p>
<ul>
<li><strong>"ExportToPackage":</strong> Initiates an export operation to a data package.</li>
<li><strong>Cloud and On-Premises:</strong> The process is the same for both deployment models.</li>
<li><strong>Parameters:</strong>
<ul>
<li>"definitionGroupId": The data project ID.</li>
<li>"packageName": The name for the downloaded file.</li>
<li>"executionId": An optional execution ID for rerunning a job.</li>
<li>"reExecute": A Boolean indicating whether to rerun the target step.</li>
<li>"legalEntityId": The legal entity for the export.</li>
</ul>
</li>
</ul>
<p><strong>Monitoring API Status:</strong></p>
<ul>
<li><strong>"GetExecutionSummaryStatus":</strong> Checks the status of an import or export job.</li>
<li><strong>Applicable to:</strong> Cloud and on-premises deployments.</li>
<li><strong>Parameter:</strong>
<ul>
<li>"executionId": The ID of the job to check.</li>
</ul>
</li>
<li><strong>File Retention:</strong> Exported files remain in Azure Blob storage for seven days and are then automatically deleted.</li>
</ul>
<p><strong>Key Differences Between Cloud and On-Premises Deployments:</strong></p>
<table>
<thead>
<tr>
<th align="left">Feature</th>
<th align="left">Cloud Deployments</th>
<th align="left">On-Premises Deployments</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Authorization</td>
<td align="left">OAuth 2.0</td>
<td align="left">AD FS</td>
</tr>
<tr>
<td align="left">Data Package Storage</td>
<td align="left">Azure Blob storage</td>
<td align="left">Local storage</td>
</tr>
<tr>
<td align="left">Recurring Integrations API</td>
<td align="left">Supported</td>
<td align="left">Not supported</td>
</tr>
<tr>
<td align="left">Data Management Package REST API</td>
<td align="left">Supported (same API names as cloud)</td>
<td align="left">Supported (same API names, but uses local storage for packages)</td>
</tr>
<tr>
<td align="left">Data Management Framework</td>
<td align="left">Available out of the box (supports data import/export)</td>
<td align="left">Only supported API for on-premises deployments</td>
</tr>
</tbody>
</table>
<p><strong>Example Code Snippets:</strong></p>
<p><strong>Import (Cloud):</strong></p>
<p>"""<br>
POST /data/DataManagementDefinitionGroups/Microsoft.Dynamics.DataEntities.ImportFromPackage<br>
BODY<br>
{<br>
"packageUrl":"your_unique_file_name.zip",<br>
"definitionGroupId":"your_data_project_name",<br>
"executionId":"",<br>
"execute":true,<br>
"overwrite":true,<br>
"legalEntityId":"USMF"<br>
}<br>
"""</p>
<p><strong>Export (Cloud and On-Premises):</strong></p>
<p>"""<br>
POST /data/DataManagementDefinitionGroups/Microsoft.Dynamics.DataEntities.ExportToPackage<br>
BODY<br>
{<br>
"definitionGroupId":"your_data_project_name",<br>
"packageName":"exported_data.zip",<br>
"executionId":"",<br>
"reExecute":false,<br>
"legalEntityId":"USMF"<br>
}<br>
"""</p>
<p><strong>Monitoring Status:</strong></p>
<p>"""<br>
POST /data/DataManagementDefinitionGroups/Microsoft.Dynamics.DataEntities.GetExecutionSummaryStatus<br>
BODY<br>
{"executionId":"your_execution_id"}<br>
"""</p>
<p>In essence, this section provides a detailed explanation of the Data Management Framework's Package REST API, highlighting its features, usage in import and export operations, and the differences between cloud and on-premises deployments. It also emphasizes the importance of understanding these APIs for integrating with Finance and Operations using data packages.</p>
<p>Let's continue our discussion, focusing on monitoring API status, managing entity change tracking, consuming external web services with wrapper classes, integrating with Excel and Power Apps, data integration with Microsoft Power Platform, downloading EDI solutions, verifying data with the Data Validation Checklist workspace, and debugging with JSON Web Tokens (JWTs).</p>
<p><strong>Monitoring API Status:</strong></p>
<p>The "GetExecutionSummaryStatus" API provides several status outputs:</p>
<ul>
<li><strong>Unknown:</strong> The status cannot be determined.</li>
<li><strong>NotRun:</strong> The job has been created but hasn't started.</li>
<li><strong>Executing:</strong> The job is currently running.</li>
<li><strong>Succeeded:</strong> The job completed successfully.</li>
<li><strong>PartiallySucceeded:</strong> The job completed, but with some errors.</li>
<li><strong>Failed:</strong> The job failed to complete due to errors.</li>
<li><strong>Canceled:</strong> The job was manually stopped.</li>
</ul>
<p><strong>Managing Entity Change Tracking:</strong></p>
<p>Change tracking allows for incremental data export using the Data Management Framework.</p>
<p><strong>Enabling Change Tracking:</strong></p>
<ol>
<li>Go to the Data management workspace.</li>
<li>Select "Configure entity export to database."</li>
<li>Select the entity you want to publish to your database.</li>
<li>Select "Publish."</li>
<li>Select the entity you just published.</li>
<li>Select "Change tracking" in the Action Pane.</li>
</ol>
<p><strong>Change Tracking Options:</strong></p>
<ul>
<li><strong>Enable primary table:</strong> Tracks changes in the primary table of the entity.</li>
<li><strong>Enable entire entity:</strong> Tracks changes in all tables within the entity.</li>
<li><strong>Enable custom query:</strong> Allows you to define a custom set of fields to track.</li>
</ul>
<p><strong>Consuming External Web Services with Wrapper Classes:</strong></p>
<p>Wrapper classes encapsulate the logic for interacting with external web services.</p>
<p><strong>Creating Wrapper Classes with C#:</strong></p>
<p>The example demonstrates creating a C# class library that includes a service reference to a SOAP web service.</p>
<p>"""csharp<br>
using System.ServiceModel;</p>
<p>namespace MyWebServiceWrapper<br>
{<br>
public class WebServiceHelper<br>
{<br>
public static string CallWebServiceMethod()<br>
{<br>
var binding = new System.ServiceModel.BasicHttpBinding();<br>
var endpointAddress = new EndpointAddress("SOAP web service url");<br>
WebServiceSoapClient client = new WebServiceSoapClient(binding, endpointAddress);</p>
<pre><code>        return client.WebServiceMethod(); // Replace WebServiceMethod with the actual method name
    }
}
</code><button class="copy-code-button">Copy</button></pre>
<p>}<br>
"""</p>
<p><strong>Using the Wrapper Class in Finance and Operations:</strong></p>
<ol>
<li>
<p>Add the compiled assembly (DLL) of the wrapper class to the References node in the AOT.</p>
</li>
<li>
<p>Call the methods of the wrapper class from X++.</p>
<p>"""x++<br>
WebServiceHelper.CallWebServiceMethod();<br>
"""</p>
</li>
</ol>
<p><strong>Integrating with Microsoft Excel:</strong></p>
<p>The Dynamics 365 add-in for Excel allows you to view, update, and add data to data entities directly from Excel.</p>
<p><strong>Opening Data in Excel from Finance and Operations:</strong></p>
<ol>
<li>Open the desired page in Finance and Operations.</li>
<li>Select the Office icon and then "Open in Excel."</li>
<li>Enable editing and trust the add-in.</li>
<li>Sign in with your Finance and Operations credentials.</li>
</ol>
<p><strong>Opening Data in Excel from within Excel:</strong></p>
<ol>
<li>In Excel, go to Insert &gt; Store and add the "Microsoft Dynamics Office Add-in."</li>
<li>Trust the add-in.</li>
<li>Add the server information (URL of your Finance and Operations instance).</li>
<li>Select "Design" and then "Add table."</li>
<li>Choose the desired entity and add fields.</li>
<li>Select "Refresh" to pull data.</li>
</ol>
<p><strong>Using the Excel Add-in:</strong></p>
<ul>
<li><strong>Refresh:</strong> Updates the data in Excel from Finance and Operations.</li>
<li><strong>Publish:</strong> Publishes changes made in Excel back to Finance and Operations.</li>
<li><strong>New:</strong> Adds a new record (if all key and mandatory fields are available).</li>
<li><strong>Delete:</strong> Deletes the selected record.</li>
</ul>
<p><strong>Integrating with Power Apps:</strong></p>
<p>You can embed Power Apps within Finance and Operations to enhance functionality.</p>
<p><strong>Accessing Power Apps:</strong></p>
<ul>
<li><strong>Power Apps button:</strong> If available on a page, it launches an embedded Power App.</li>
<li><strong>New tab, FastTab, blade, or section:</strong> Power Apps can be displayed as part of the page layout.</li>
</ul>
<p><strong>Controlling Power Apps Embedding:</strong></p>
<ul>
<li><strong>"isPowerAppPersonalizationEnabled":</strong> Controls whether users can embed Power Apps on a page.</li>
<li><strong>"isPowerAppTabPersonalizationEnabled":</strong> Controls whether users can embed Power Apps as tabs, FastTabs, or sections.</li>
</ul>
<p><strong>Data Integration with Microsoft Power Platform:</strong></p>
<ul>
<li><strong>Model-driven Power Apps:</strong> OData entities are available as virtual entities in Dataverse, allowing you to build model-driven apps using Finance and Operations data.</li>
<li><strong>Power Pages:</strong> You can use Finance and Operations data to create Power Pages.</li>
</ul>
<p><strong>Downloading EDI Solutions:</strong></p>
<p>You can download EDI modules from AppSource (Products &gt; Dynamics 365 &gt; Finance) to integrate with supply chain partners.</p>
<p><strong>Verifying Data with the Data Validation Checklist Workspace:</strong></p>
<p>This workspace helps track data validation processes during implementations, upgrades, or migrations.</p>
<p><strong>Key Features:</strong></p>
<ul>
<li><strong>Project Selection:</strong> Filter data by a specific validation project.</li>
<li><strong>Summary Tiles:</strong> Provide an overview of task statuses (Not started, In progress, Completed, Remaining).</li>
<li><strong>Tasks and Status:</strong> Shows details of tasks, including status by legal entity, area, or task.</li>
<li><strong>Task Editing:</strong> Allows assigning and changing the status of tasks.</li>
<li><strong>Attachments:</strong> Supports adding files, notes, images, or URLs to tasks.</li>
</ul>
<p><strong>Setting up a Data Validation Project:</strong></p>
<ol>
<li>Go to the "Data validation project" page.</li>
<li>Create a new project.</li>
<li>Add tasks, legal entities, and task areas.</li>
</ol>
<p><strong>Debugging with JSON Web Tokens (JWTs):</strong></p>
<p>JWTs can be used to troubleshoot service authentication issues.</p>
<p><strong>Inspecting JWTs:</strong></p>
<ul>
<li><strong>Capture from HTTP request:</strong> Use a tool like Fiddler to capture the JWT from the "Authorization" header (remove the "Bearer" prefix).</li>
<li><strong>Use a deserializer tool:</strong> Paste the JWT into a tool like JWT.io to view its contents.</li>
</ul>
<p><strong>Verifying JWT Information:</strong></p>
<ul>
<li><strong>"aud":</strong> The intended audience (URI of the resource). Check for trailing slashes or incorrect capitalization.</li>
<li><strong>"appid":</strong> The client application ID.</li>
<li><strong>"upn":</strong> The user principal name.</li>
</ul>
<p>In essence, this section provides a comprehensive overview of various integration capabilities in Dynamics 365 Finance and Operations, including monitoring API status, managing entity change tracking, consuming external web services, integrating with Excel and Power Apps, using the Data Validation Checklist workspace, and debugging with JWTs. It emphasizes the importance of understanding these features for building robust and efficient integrations.</p>
<p>Let's continue our deep dive into data integration, focusing on monitoring API status and errors, data transformation, Dataverse integration, virtual entities, composite entities, and aggregate data entities.</p>
<p><strong>Monitoring API Status and Errors:</strong></p>
<p>When you use the "GetExecutionSummaryStatus" API to monitor data import/export jobs, you might receive various status messages:</p>
<ul>
<li><strong>Unknown:</strong> The status could not be determined.</li>
<li><strong>NotRun:</strong> The job has been created but not started.</li>
<li><strong>Executing:</strong> The job is currently running.</li>
<li><strong>Succeeded:</strong> The job completed successfully.</li>
<li><strong>PartiallySucceeded:</strong> The job completed with some errors.</li>
<li><strong>Failed:</strong> The job failed to complete.</li>
<li><strong>Canceled:</strong> The job was manually stopped.</li>
</ul>
<p><strong>Error Handling:</strong></p>
<ul>
<li><strong>Execution Log:</strong> View the execution log in the Data Management Framework to see details about errors.</li>
<li><strong>Staging Data:</strong> Examine the staging data to identify and correct issues.</li>
<li><strong>Reprocessing:</strong> You can reprocess failed files or fix mapping configurations to resolve errors.</li>
</ul>
<p><strong>Data Transformation:</strong></p>
<p>Data transformation is often necessary to prepare data for import or export.</p>
<p><strong>Transformation Using Configuration:</strong></p>
<p>You can perform basic transformations through configuration in the Data Management Framework.</p>
<ul>
<li><strong>Mapping Visualization:</strong> When you create an import or export project, the "View map" icon allows you to access the mapping visualization.</li>
<li><strong>Mapping Details:</strong> This tab shows the source-to-target field mapping and provides options for transformations.</li>
<li><strong>Ignore Blank Values:</strong> Ignores records where a specific field is blank.</li>
<li><strong>Text Qualifier:</strong> Handles text fields that might contain the column delimiter character (e.g., adding double quotes around text values).</li>
<li><strong>Use Enum Label:</strong> Uses the enum label instead of the underlying value in the data transfer file.</li>
<li><strong>Auto-generated:</strong> Automatically generates values for specific fields during import (e.g., party number).</li>
<li><strong>Auto default:</strong> Sets a default value for a field during import.</li>
<li><strong>Conversion:</strong> Maps specific values from the source to different values in the target (e.g., "false" to "No," "true" to "Yes").</li>
</ul>
<p><strong>Data Transformation Between Finance and Operations and the Staging Table:</strong></p>
<p>You can add more complex transformation logic using X++ code.</p>
<ul>
<li><strong>Modify Target Mapping:</strong> Access this option from the "Data entities" tile in the Data Management workspace.</li>
<li><strong>Mapping Details:</strong> This tab shows the mapping between the staging table and the target entity.</li>
<li><strong>Computed Columns:</strong>
<ul>
<li>Generate values based on SQL view computed columns.</li>
<li>Computed at the SQL Server level.</li>
<li>Generally more efficient than virtual fields.</li>
<li>Primarily used for read operations.</li>
</ul>
</li>
<li><strong>Virtual Fields:</strong>
<ul>
<li>Non-persisted fields controlled by custom X++ code.</li>
<li>Used for calculations or transformations that cannot be done with computed columns.</li>
<li>Computed row by row in X++.</li>
</ul>
</li>
</ul>
<p><strong>Microsoft Dataverse Integrations:</strong></p>
<p>Dataverse enables data synchronization between Finance and Operations and other Dynamics 365 applications (e.g., Sales, Customer Service).</p>
<p><strong>Key Features:</strong></p>
<ul>
<li><strong>Near Real-Time Synchronization:</strong> Data flows bidirectionally between Finance and Operations and Dataverse in near real-time.</li>
<li><strong>Out-of-the-Box Templates:</strong> Predefined templates simplify the integration process.</li>
<li><strong>Customizable Mappings:</strong> You can modify or create custom mappings between entities and fields.</li>
<li><strong>One Dynamics 365 Experience:</strong> Enables a unified view of data across applications.</li>
</ul>
<p><strong>Setting up Data Integration Projects:</strong></p>
<ol>
<li><strong>Create a connection:</strong> Provide credentials for the data sources (Finance and Operations and Dataverse).</li>
<li><strong>Create a connection set:</strong> Identify the environments for the connections.</li>
<li><strong>Create a data integration project:</strong> Use a template or define custom mappings.</li>
</ol>
<p><strong>Synchronous vs. Asynchronous Integration:</strong></p>
<ul>
<li><strong>Synchronous (Bidirectional):</strong> Near real-time, bidirectional data flow.</li>
<li><strong>Asynchronous (Unidirectional):</strong> Data flows in one direction (either from Finance and Operations to Dataverse or vice versa) and is not real-time.</li>
</ul>
<p><strong>Integrating Dataverse Using Virtual Entities:</strong></p>
<p>Virtual entities allow you to access Finance and Operations data directly from Dataverse without data duplication.</p>
<p><strong>Key Features:</strong></p>
<ul>
<li><strong>No Data Duplication:</strong> Data resides only in Finance and Operations.</li>
<li><strong>CRUD Operations:</strong> Supports create, read, update, and delete operations from Dataverse.</li>
<li><strong>Service-to-Service Calls:</strong> Communication between Dataverse and Finance and Operations happens behind the scenes.</li>
<li><strong>Power Platform Integration:</strong> Must be enabled for virtual entities to work.</li>
</ul>
<p><strong>Enabling Power Platform Integration:</strong></p>
<ul>
<li><strong>Tier-2+ Environments:</strong> Enable after deployment in the environment's "Power Platform Integration" FastTab.</li>
<li><strong>Tier-1 (Development) Environments:</strong> Enable during deployment in Lifecycle Services (Advanced settings &gt; Power Platform Integration).</li>
</ul>
<p><strong>Enabling Virtual Entities:</strong></p>
<ol>
<li>In the Power Platform admin center, select the environment linked to your Finance and Operations instance.</li>
<li>Open the environment URL.</li>
<li>Go to Settings (gear icon) &gt; Advanced Settings.</li>
<li>Use the Advanced find filter icon and search for "Available finance and operations entities."</li>
<li>Open the entity you want to enable.</li>
<li>Select the "Visible" checkbox and save.</li>
</ol>
<p><strong>Working with Composite Data Entities:</strong></p>
<p>Composite entities combine multiple related entities into a single unit (e.g., sales order header and lines).</p>
<p><strong>Key Features:</strong></p>
<ul>
<li><strong>Applicable to:</strong> Asynchronous integration scenarios (import/export).</li>
<li><strong>Not Supported:</strong> Synchronous OData or X++ programmatic interface.</li>
<li><strong>Format:</strong> XML file-based.</li>
</ul>
<p><strong>Importing/Exporting Composite Entities:</strong></p>
<p>Use the "Import" or "Export" tiles in the Data Management workspace.</p>
<p><strong>Working with Aggregate Data Entities:</strong></p>
<p>Aggregate data entities are based on views and are used for analytics purposes.</p>
<p><strong>Components:</strong></p>
<ul>
<li><strong>Aggregate measurements:</strong> Calculated values (e.g., sum, average).</li>
<li><strong>Aggregate dimensions:</strong> Attributes used to group or filter data.</li>
<li><strong>Model dimension references:</strong> Relationships between dimensions.</li>
</ul>
<p><strong>Accessing Aggregate Data Entities:</strong></p>
<p>If the "IsPublic" property is set to "true", you can access them via OData using the URL "https://[your dynamics name].dynamics.com/data/[aggregate data entity name]".</p>
<p><strong>Creating Aggregate Data Entities:</strong></p>
<p>Development happens in Visual Studio. The text provides steps for creating aggregate measurements, dimensions, and the entity itself.</p>
<p>In essence, this section provides a comprehensive overview of data transformation techniques, Dataverse integration, virtual entities, composite entities, and aggregate data entities in Dynamics 365 Finance and Operations. It emphasizes the importance of understanding these concepts for building robust and efficient data integration solutions. The examples and steps provided illustrate how to implement these features and use them in various integration scenarios.</p>
<p>Let's wrap up our exploration of data integration by examining how to monitor job history and errors, perform data transformations, integrate with Dataverse and Azure Data Lake, work with composite and aggregate data entities, and understand the implications of Power Platform convergence.</p>
<p><strong>Monitoring Job History and Errors:</strong></p>
<ul>
<li><strong>Job History:</strong> The Data Management Framework provides a history of data import/export jobs.</li>
<li><strong>Execution Status:</strong> You can view the execution status of each job (e.g., Succeeded, Failed, PartiallySucceeded).</li>
<li><strong>Execution Logs:</strong> Detailed logs provide information about errors and issues encountered during job execution.</li>
<li><strong>Staging Data:</strong> You can view and validate the data in the staging table before it's committed to the target tables.</li>
</ul>
<p><strong>Data Transformation:</strong></p>
<p>Data transformation is often necessary to map data from the source system to the target system's structure and format.</p>
<p><strong>Transformation Using Configuration:</strong></p>
<ul>
<li><strong>Mapping Visualization:</strong> The Data Management Framework provides a visual mapping interface.</li>
<li><strong>Mapping Details:</strong> This tab allows you to configure basic transformations, such as:
<ul>
<li>Ignoring blank values.</li>
<li>Handling text qualifiers.</li>
<li>Using enum labels.</li>
<li>Auto-generating values.</li>
<li>Setting default values.</li>
<li>Performing value conversions.</li>
</ul>
</li>
</ul>
<p><strong>Transformation Using Code:</strong></p>
<ul>
<li><strong>Modify Target Mapping:</strong> Access this option from the "Data entities" tile in the Data Management workspace.</li>
<li><strong>Computed Columns:</strong>
<ul>
<li>Use SQL view computed columns for calculations.</li>
<li>More efficient than virtual fields.</li>
<li>Primarily for read operations.</li>
</ul>
</li>
<li><strong>Virtual Fields:</strong>
<ul>
<li>Non-persisted fields controlled by X++ code.</li>
<li>Used for complex calculations or transformations.</li>
</ul>
</li>
</ul>
<p><strong>Dataverse Integrations:</strong></p>
<p>Dataverse enables data synchronization between Finance and Operations and other Dynamics 365 applications (e.g., Sales, Customer Service).</p>
<ul>
<li><strong>Near Real-Time, Bidirectional Synchronization:</strong> Data flows seamlessly between applications.</li>
<li><strong>Out-of-the-Box Templates:</strong> Predefined templates simplify the integration process.</li>
<li><strong>Customizable Mappings:</strong> You can modify or create custom mappings.</li>
<li><strong>One Dynamics 365 Experience:</strong> Provides a unified view of data.</li>
</ul>
<p><strong>Setting up Data Integration Projects:</strong></p>
<ol>
<li><strong>Create a connection:</strong> Provide credentials for Finance and Operations and Dataverse.</li>
<li><strong>Create a connection set:</strong> Identify the environments for the connections.</li>
<li><strong>Create a data integration project:</strong> Use a template or define custom mappings.</li>
</ol>
<p><strong>Virtual Entities:</strong></p>
<p>Virtual entities allow you to access Finance and Operations data directly from Dataverse without data duplication.</p>
<ul>
<li><strong>No Data Duplication:</strong> Data resides only in Finance and Operations.</li>
<li><strong>CRUD Operations:</strong> Supports create, read, update, and delete operations from Dataverse.</li>
<li><strong>Service-to-Service Calls:</strong> Communication happens behind the scenes.</li>
<li><strong>Power Platform Integration:</strong> Must be enabled to use virtual entities.</li>
</ul>
<p><strong>Enabling Virtual Entities:</strong></p>
<ol>
<li>Enable Power Platform Integration for your environment.</li>
<li>In the Power Platform admin center, select the environment linked to your Finance and Operations instance.</li>
<li>Open the environment URL.</li>
<li>Go to Settings &gt; Advanced Settings.</li>
<li>Use the Advanced find filter to search for "Available finance and operations entities."</li>
<li>Open the entity you want to enable.</li>
<li>Select the "Visible" checkbox and save.</li>
</ol>
<p><strong>Composite Data Entities:</strong></p>
<p>Composite entities combine multiple related entities into a single unit for import/export.</p>
<ul>
<li><strong>Applicable to:</strong> Asynchronous integration scenarios.</li>
<li><strong>Not Supported:</strong> Synchronous OData or X++ programmatic interface.</li>
<li><strong>Format:</strong> XML file-based.</li>
</ul>
<p><strong>Aggregate Data Entities:</strong></p>
<p>Aggregate data entities are based on views and are used for analytics.</p>
<ul>
<li><strong>Components:</strong>
<ul>
<li>Aggregate measurements (calculated values).</li>
<li>Aggregate dimensions (attributes for grouping/filtering).</li>
<li>Model dimension references (relationships between dimensions).</li>
</ul>
</li>
</ul>
<p><strong>Accessing Aggregate Data Entities:</strong></p>
<p>If the "IsPublic" property is set to "true", you can access them via OData using the URL "https://[your dynamics name].dynamics.com/data/[aggregate data entity name]".</p>
<p><strong>Creating Aggregate Data Entities:</strong></p>
<p>Development happens in Visual Studio. The text provides steps for creating aggregate measurements, dimensions, and the entity itself.</p>
<p><strong>Azure Data Lake and Entity Store:</strong></p>
<ul>
<li><strong>Azure Data Lake Storage:</strong> A cost-effective, scalable storage solution for large amounts of data.</li>
<li><strong>Entity Store in Azure Data Lake:</strong> Enables advanced analytics and AI by providing access to denormalized transactional data in near real-time.</li>
<li><strong>Common Data Model (CDM) Folders:</strong> Aggregate measurements are stored as CDM folders, enabling data mashups and reporting with tools like Power BI.</li>
</ul>
<p><strong>Making Entity Store Available as a Data Lake:</strong></p>
<ol>
<li>Enable automated Entity Store refresh (System administration &gt; Setup &gt; Entity Store).</li>
<li>Configure Data Lake integration (System administration &gt; System setup &gt; System parameters &gt; Data connections).
<ul>
<li>Enable Data Lake integration.</li>
<li>Optionally, enable trickle update.</li>
<li>Provide Application ID, Application Secret, DNS name, and Secret name for Azure Key Vault.</li>
</ul>
</li>
<li>Test the connection to Azure Key Vault and Azure Storage.</li>
</ol>
<p><strong>Connecting to Azure Data Lake Storage:</strong></p>
<ul>
<li><strong>Advantages over BYOD:</strong>
<ul>
<li>Data is already present (no export needed).</li>
<li>Reduced storage costs.</li>
<li>You can reverse existing downstream pipelines before export.</li>
</ul>
</li>
<li><strong>Data Format:</strong> CSV files in CDM format.</li>
<li><strong>Availability:</strong> Not available in Tier-1 (developer) environments (unless using the FastTrack prototype).</li>
</ul>
<p><strong>FastTrack Prototype for Tier-1 Environments:</strong></p>
<p>GitHub provides a solution for connecting a cloud-hosted development environment to Azure Data Lake Storage in CDM format.</p>
<p><strong>Steps to Use the Prototype:</strong></p>
<ol>
<li>Clone the repository from GitHub.</li>
<li>Build the repository.</li>
<li>Deploy it as an Azure Function.</li>
<li>Enable the built MSI files.</li>
<li>Set up Azure storage account, Synapse Analytics, and Azure Data Factory.</li>
<li>Deploy the Azure Data Factory template from the cloned repository.</li>
<li>Connect Azure Data Factory to the Finance and Operations environment.</li>
<li>Run the pipelines in Azure Data Factory.</li>
</ol>
<p><strong>Change Data in Azure Data Lake:</strong></p>
<ul>
<li><strong>"Enable near real-time data changes":</strong> Inserts, updates, and deletes data in the data lake in near real-time.</li>
<li><strong>Change Feed Folder:</strong> Contains a history of data changes, useful for incremental updates of downstream systems.</li>
</ul>
<p><strong>Best Practices When Using Change Feeds:</strong></p>
<ul>
<li>Use for near real-time updates of data warehouses or data marts.</li>
<li>Change records are grouped into files (around 4-8 MB).</li>
<li>Change records are appended; files are never updated.</li>
<li>Use the LSN field or file date/time stamp as a watermark for processing changes.</li>
<li>Consider triggering a full refresh of downstream pipelines when tables are reactivated.</li>
</ul>
<p><strong>Power Platform Convergence:</strong></p>
<ul>
<li><strong>Simplified Integration:</strong> Streamlines the integration of Finance and Operations with Power Platform.</li>
<li><strong>One-Click Dual-Write Setup:</strong> Simplifies the configuration of dual-write.</li>
<li><strong>Unlocked Add-ins:</strong> Enables the use of add-ins.</li>
<li><strong>Virtual Entities by Default:</strong> Makes virtual entities readily available.</li>
<li><strong>Converged Business Events:</strong> Unifies business events in Finance and Operations and Dataverse.</li>
<li><strong>CUD Events:</strong> Enables create, update, and delete events for OData-enabled data entities.</li>
<li><strong>C# Plug-ins:</strong> Allows the execution of C# plug-ins in Dataverse for Finance and Operations scenarios.</li>
<li><strong>Plugin Registration Tool:</strong> Enables registering C# plug-ins for business and CUD events.</li>
<li><strong>Visual Studio Integration:</strong> Provides a unified development experience for Finance and Operations and Dataverse customizations.</li>
</ul>
<p>In essence, this section provides a comprehensive overview of various advanced data integration concepts and features in Dynamics 365 Finance and Operations, including error handling, data transformation, Dataverse integration, virtual entities, composite and aggregate data entities, Azure Data Lake integration, change feeds, and Power Platform convergence. It emphasizes the importance of understanding these features for building robust and efficient data integration solutions.</p>
<p>Let's continue our discussion, focusing on the final stages of data integration, including monitoring API status and errors, data transformation, integration with Dataverse and Azure Data Lake, composite and aggregate data entities, and the implications of Power Platform convergence. We will then delve into data migration strategies, scenarios, and tools.</p>
<p><strong>Monitoring API Status and Errors (Continued):</strong></p>
<ul>
<li><strong>Error Handling:</strong>
<ul>
<li><strong>Job History:</strong> The Data Management Framework provides a history of data import/export jobs, including their execution status (e.g., Succeeded, Failed, PartiallySucceeded).</li>
<li><strong>Execution Log:</strong> Provides detailed logs about errors and issues encountered during job execution.</li>
<li><strong>Staging Data:</strong> Allows you to view and validate the data in the staging table before it's committed to the target tables. You can also correct errors and reprocess data from the staging table.</li>
</ul>
</li>
</ul>
<p><strong>Data Transformation:</strong></p>
<p>Data transformation is crucial for mapping data from the source system to the target system's structure and format.</p>
<p><strong>Transformation Using Configuration:</strong></p>
<ul>
<li><strong>Mapping Visualization:</strong> The Data Management Framework offers a visual interface for mapping source fields to target fields.</li>
<li><strong>Mapping Details:</strong> This tab allows you to configure basic transformations, such as:
<ul>
<li><strong>Ignore Blank Values:</strong> Excludes records with blank values in specific fields.</li>
<li><strong>Text Qualifier:</strong> Handles text fields that might contain the column delimiter character (e.g., adding double quotes around text values).</li>
<li><strong>Use Enum Label:</strong> Uses the enum label instead of the numeric value in data transfers.</li>
<li><strong>Auto-generated:</strong> Automatically generates values for specific fields during import (e.g., generating a unique ID).</li>
<li><strong>Auto default:</strong> Sets a default value for a field during import if the source value is missing.</li>
<li><strong>Conversion:</strong> Maps specific values from the source to different values in the target (e.g., mapping "true" to "Yes" and "false" to "No").</li>
</ul>
</li>
</ul>
<p><strong>Transformation Using Code:</strong></p>
<ul>
<li><strong>Modify Target Mapping:</strong> This option, accessible from the "Data entities" tile in the Data Management workspace, allows you to add more complex transformation logic using X++.</li>
<li><strong>Computed Columns:</strong>
<ul>
<li>Defined using SQL view computed columns.</li>
<li>Calculations are performed at the SQL Server level.</li>
<li>Generally more efficient than virtual fields.</li>
<li>Primarily used for read operations.</li>
</ul>
</li>
<li><strong>Virtual Fields:</strong>
<ul>
<li>Non-persisted fields controlled by custom X++ code.</li>
<li>Useful for complex calculations or transformations that cannot be achieved with computed columns.</li>
<li>Calculations are performed row by row in X++.</li>
</ul>
</li>
</ul>
<p><strong>Microsoft Dataverse Integrations:</strong></p>
<p>Dataverse enables seamless data synchronization between Finance and Operations and other Dynamics 365 applications (e.g., Sales, Customer Service).</p>
<ul>
<li><strong>Near Real-Time, Bidirectional Synchronization:</strong> Data flows between applications in near real-time.</li>
<li><strong>Out-of-the-Box Templates:</strong> Predefined templates simplify the integration process.</li>
<li><strong>Customizable Mappings:</strong> You can modify or create custom mappings between entities and fields.</li>
<li><strong>One Dynamics 365 Experience:</strong> Provides a unified view of data across applications.</li>
</ul>
<p><strong>Setting up Data Integration Projects:</strong></p>
<ol>
<li><strong>Create a connection:</strong> Provide credentials for Finance and Operations and Dataverse.</li>
<li><strong>Create a connection set:</strong> Identify the environments for the connections.</li>
<li><strong>Create a data integration project:</strong> Use a template or define custom mappings.</li>
</ol>
<p><strong>Virtual Entities:</strong></p>
<p>Virtual entities allow you to access Finance and Operations data directly from Dataverse without data duplication.</p>
<ul>
<li><strong>No Data Duplication:</strong> Data resides only in Finance and Operations.</li>
<li><strong>CRUD Operations:</strong> Supports create, read, update, and delete operations from Dataverse.</li>
<li><strong>Service-to-Service Calls:</strong> Communication between Dataverse and Finance and Operations happens behind the scenes.</li>
<li><strong>Power Platform Integration:</strong> Must be enabled to use virtual entities.</li>
</ul>
<p><strong>Enabling Virtual Entities:</strong></p>
<ol>
<li>Enable Power Platform Integration for your environment.</li>
<li>In the Power Platform admin center, select the environment linked to your Finance and Operations instance.</li>
<li>Open the environment URL.</li>
<li>Go to Settings &gt; Advanced Settings.</li>
<li>Use the Advanced find filter to search for "Available finance and operations entities."</li>
<li>Open the entity you want to enable.</li>
<li>Select the "Visible" checkbox and save.</li>
</ol>
<p><strong>Composite Data Entities:</strong></p>
<p>Composite entities combine multiple related entities into a single unit for import/export (e.g., sales order header and lines).</p>
<ul>
<li><strong>Applicable to:</strong> Asynchronous integration scenarios (import/export).</li>
<li><strong>Not Supported:</strong> Synchronous OData or X++ programmatic interface.</li>
<li><strong>Format:</strong> XML file-based.</li>
</ul>
<p><strong>Aggregate Data Entities:</strong></p>
<p>Aggregate data entities are based on views and are used for analytics.</p>
<ul>
<li><strong>Components:</strong>
<ul>
<li>Aggregate measurements (calculated values).</li>
<li>Aggregate dimensions (attributes for grouping/filtering).</li>
<li>Model dimension references (relationships between dimensions).</li>
</ul>
</li>
</ul>
<p><strong>Accessing Aggregate Data Entities:</strong></p>
<p>If the "IsPublic" property is set to "true", you can access them via OData using the URL "https://[your dynamics name].dynamics.com/data/[aggregate data entity name]".</p>
<p><strong>Creating Aggregate Data Entities:</strong></p>
<p>Development happens in Visual Studio. The text provides steps for creating aggregate measurements, dimensions, and the entity itself.</p>
<p><strong>Azure Data Lake and Entity Store:</strong></p>
<ul>
<li><strong>Azure Data Lake Storage:</strong> A cost-effective, scalable storage solution for large amounts of data.</li>
<li><strong>Entity Store in Azure Data Lake:</strong> Enables advanced analytics and AI by providing access to denormalized transactional data in near real-time.</li>
<li><strong>Common Data Model (CDM) Folders:</strong> Aggregate measurements are stored as CDM folders, enabling data mashups and reporting with tools like Power BI.</li>
</ul>
<p><strong>Making Entity Store Available as a Data Lake:</strong></p>
<ol>
<li>Enable automated Entity Store refresh (System administration &gt; Setup &gt; Entity Store).</li>
<li>Configure Data Lake integration (System administration &gt; System setup &gt; System parameters &gt; Data connections).
<ul>
<li>Enable Data Lake integration.</li>
<li>Optionally, enable trickle update.</li>
<li>Provide Application ID, Application Secret, DNS name, and Secret name for Azure Key Vault.</li>
</ul>
</li>
<li>Test the connection to Azure Key Vault and Azure Storage.</li>
</ol>
<p><strong>Connecting to Azure Data Lake Storage:</strong></p>
<ul>
<li><strong>Advantages over BYOD:</strong>
<ul>
<li>Data is already present (no export needed).</li>
<li>Reduced storage costs.</li>
<li>You can reverse existing downstream pipelines before export.</li>
</ul>
</li>
<li><strong>Data Format:</strong> CSV files in CDM format.</li>
<li><strong>Availability:</strong> Not available in Tier-1 (developer) environments (unless using the FastTrack prototype).</li>
</ul>
<p><strong>FastTrack Prototype for Tier-1 Environments:</strong></p>
<p>GitHub provides a solution for connecting a cloud-hosted development environment to Azure Data Lake Storage in CDM format.</p>
<p><strong>Steps to Use the Prototype:</strong></p>
<ol>
<li>Clone the repository from GitHub.</li>
<li>Build the repository.</li>
<li>Deploy it as an Azure Function.</li>
<li>Enable the built MSI files.</li>
<li>Set up Azure storage account, Synapse Analytics, and Azure Data Factory.</li>
<li>Deploy the Azure Data Factory template from the cloned repository.</li>
<li>Connect Azure Data Factory to the Finance and Operations environment.</li>
<li>Run the pipelines in Azure Data Factory.</li>
</ol>
<p><strong>Change Data in Azure Data Lake:</strong></p>
<ul>
<li><strong>"Enable near real-time data changes":</strong> Inserts, updates, and deletes data in the data lake in near real-time.</li>
<li><strong>Change Feed Folder:</strong> Contains a history of data changes, useful for incremental updates of downstream systems.</li>
</ul>
<p><strong>Best Practices When Using Change Feeds:</strong></p>
<ul>
<li>Use for near real-time updates of data warehouses or data marts.</li>
<li>Change records are grouped into files (around 4-8 MB).</li>
<li>Change records are appended; files are never updated.</li>
<li>Use the LSN field or file date/time stamp as a watermark for processing changes.</li>
<li>Consider triggering a full refresh of downstream pipelines when tables are reactivated.</li>
</ul>
<p><strong>Power Platform Convergence:</strong></p>
<ul>
<li><strong>Simplified Integration:</strong> Streamlines the integration of Finance and Operations with Power Platform.</li>
<li><strong>One-Click Dual-Write Setup:</strong> Simplifies the configuration of dual-write.</li>
<li><strong>Unlocked Add-ins:</strong> Enables the use of add-ins.</li>
<li><strong>Virtual Entities by Default:</strong> Makes virtual entities readily available.</li>
<li><strong>Converged Business Events:</strong> Unifies business events in Finance and Operations and Dataverse.</li>
<li><strong>CUD Events:</strong> Enables create, update, and delete events for OData-enabled data entities.</li>
<li><strong>C# Plug-ins:</strong> Allows the execution of C# plug-ins in Dataverse for Finance and Operations scenarios.</li>
<li><strong>Plugin Registration Tool:</strong> Enables registering C# plug-ins for business and CUD events.</li>
<li><strong>Visual Studio Integration:</strong> Provides a unified development experience for Finance and Operations and Dataverse customizations.</li>
</ul>
<p><strong>Data Migration Strategies, Scenarios, and Tools:</strong></p>
<p><strong>Choosing a Data Integration Strategy:</strong></p>
<p>The choice of integration strategy depends on factors like:</p>
<ul>
<li><strong>Data volume:</strong> Small, medium, or large.</li>
<li><strong>Frequency:</strong> Real-time, near real-time, or scheduled.</li>
<li><strong>Direction:</strong> Unidirectional or bidirectional.</li>
<li><strong>Data format:</strong> Structured or unstructured.</li>
<li><strong>Transformation requirements:</strong> Simple or complex.</li>
</ul>
<p><strong>Common Migration Scenarios:</strong></p>
<ul>
<li><strong>Master Data Synchronization:</strong> Keeping master data (e.g., customers, vendors, products) in sync between systems.</li>
<li><strong>Transactional Data Exchange:</strong> Transferring transactional data (e.g., sales orders, invoices) between systems.</li>
<li><strong>Real-time Updates:</strong> Requiring immediate updates in the target system when data changes in the source system.</li>
<li><strong>Scheduled Batch Processes:</strong> Transferring data at specific intervals (e.g., daily, hourly).</li>
</ul>
<p><strong>Data Migration Tools:</strong></p>
<ul>
<li><strong>Data Management Workspace:</strong> Used for setting up and managing data import/export projects.</li>
<li><strong>Office Integration:</strong> Using the Excel add-in to view, update, and add data.</li>
<li><strong>Excel Workbook Designer:</strong> Used to design Excel templates for data import/export.</li>
<li><strong>Data Integrator for Admins:</strong> A point-to-point integration service for integrating data into Dataverse.</li>
<li><strong>BYOD (Bring Your Own Database):</strong> Exporting data entities to your own Azure SQL database.</li>
</ul>
<p><strong>Identifying Relevant (Legacy) Systems:</strong></p>
<ul>
<li>Understand the customer's current systems and their data formats.</li>
<li>Plan for data extraction from legacy systems.</li>
</ul>
<p><strong>Identifying and Importing Static Data:</strong></p>
<ul>
<li>Identify common static data (e.g., country codes, postal codes) that can be imported before the main data migration.</li>
<li>This helps reduce complexity and cost.</li>
</ul>
<p><strong>Creating and Reviewing Test Plans:</strong></p>
<ul>
<li>Develop a comprehensive test plan for data migration.</li>
<li>Include unit testing, system testing, and regression testing.</li>
<li>Verify data accuracy and completeness.</li>
<li>Test data transformations and mappings.</li>
</ul>
<p><strong>Identifying and Extracting Source Data:</strong></p>
<ul>
<li>Identify the relevant data sources and entities for the migration.</li>
<li>Extract the data in a suitable format.</li>
<li>Perform data cleansing and transformation as needed.</li>
</ul>
<p><strong>Generating Field Mapping:</strong></p>
<ul>
<li>Map source fields to target fields in the data entities.</li>
<li>Use the Data Management Framework's mapping features or create custom mappings.</li>
</ul>
<p><strong>Performing a Test Migration and Validating Output:</strong></p>
<ul>
<li>Conduct a test migration in a non-production environment.</li>
<li>Validate the migrated data to ensure accuracy and completeness.</li>
<li>Use reports and inquiries in Finance and Operations to compare the migrated data with the source data.</li>
</ul>
<p><strong>Supporting the Transition:</strong></p>
<ul>
<li>Ensure a smooth transition from the legacy system to the migrated system.</li>
<li>Provide training and support to users.</li>
<li>Monitor the system after go-live to address any issues.</li>
</ul>
<p>In essence, this section provides a comprehensive overview of the final stages of data integration, including monitoring, transformation, integration with Dataverse and Azure Data Lake, composite and aggregate data entities, and Power Platform convergence. It also covers important aspects of data migration, such as choosing a strategy, identifying scenarios and tools, working with BYOD, identifying legacy systems and static data, creating test plans, extracting source data, generating field mappings, performing test migrations, and supporting the transition. The information provided is essential for planning and executing successful data integration and migration projects in Dynamics 365 Finance and Operations.</p>
<p>Let's explore how to integrate Finance and Operations apps with other systems and services, focusing on Azure Logic Apps, Microsoft Power Platform (Power Automate, Power BI, Power Apps), Microsoft 365, and the broader concept of enterprise application integration.</p>
<p><strong>Azure Logic Apps Integration:</strong></p>
<ul>
<li><strong>Purpose:</strong> Azure Logic Apps is a cloud service that helps automate workflows and integrate various applications, data, systems, and services.</li>
<li><strong>Usage Scenarios:</strong>
<ul>
<li>Process and route orders across on-premises and cloud systems.</li>
<li>Send email notifications based on events in different systems.</li>
<li>Move files between FTP/SFTP servers and Azure Storage.</li>
<li>Monitor social media for specific content and trigger actions.</li>
</ul>
</li>
<li><strong>Connectors:</strong> Logic Apps offers a wide range of connectors (200+) to integrate with various services, including Azure services, Microsoft 365, Dynamics 365, Salesforce, SAP, and more.</li>
<li><strong>Triggers and Actions:</strong> Connectors provide triggers (events that start a workflow) and actions (tasks performed within a workflow).</li>
<li><strong>Development:</strong> You can build and edit logic apps visually in the Azure portal.</li>
</ul>
<p><strong>Microsoft Power Platform Integration:</strong></p>
<p>Finance and Operations apps integrate with the Power Platform, which includes:</p>
<ul>
<li><strong>Power Automate:</strong> Automates workflows and processes.</li>
<li><strong>Power BI:</strong> Provides business intelligence and data visualization capabilities.</li>
<li><strong>Power Apps:</strong> Enables building custom business applications.</li>
</ul>
<p><strong>Connecting to Power Automate:</strong></p>
<ol>
<li>Sign in to Power Automate.</li>
<li>Go to Connections and select "New connection."</li>
<li>Choose the desired connection (e.g., Finance and Operations) and click the plus sign (+).</li>
<li>Select "Create."</li>
<li>Enter your credentials to configure the connection.</li>
</ol>
<p><strong>Connecting to Power BI Desktop:</strong></p>
<ol>
<li>In Power BI Desktop, select "Get Data" from the Home ribbon.</li>
<li>Choose "Other" and then select "OData Feed."</li>
<li>Click "Connect."</li>
<li>Enter the OData endpoint URL for your Finance and Operations instance.</li>
<li>Select the data you need and click "Load."</li>
</ol>
<p><strong>Connecting to Power Apps:</strong></p>
<ol>
<li>When creating a new Power App, select the "Start from data" option.</li>
<li>Click the right arrow to show more connection options.</li>
<li>Click "New Connection".</li>
<li>Select "Dynamics 365 for Fin &amp; Ops."</li>
<li>Click "Create."</li>
<li>Choose the Finance and Operations instance and the data entity you want to connect to.</li>
</ol>
<p><strong>Microsoft 365 Integration:</strong></p>
<ul>
<li><strong>Excel Integration:</strong> The Data Connector add-in for Excel allows you to interact with Finance and Operations data entities directly from Excel.</li>
<li><strong>Connecting to Excel:</strong>
<ol>
<li>In the Data Connector add-in panel, select "Add server information."</li>
<li>Enter the base URL for your Finance and Operations instance.</li>
<li>Sign in to your user account.</li>
<li>From any form in Finance and Operations, select the Microsoft Office icon in the Action Pane.</li>
</ol>
</li>
</ul>
<p><strong>Enterprise Application Integration (EAI):</strong></p>
<p>EAI involves connecting various applications within an organization to enable data sharing and process automation. Finance and Operations apps can be integrated with other enterprise applications using the methods discussed earlier (Logic Apps, Power Platform, custom services, etc.).</p>
<p><strong>Key Considerations for Integration:</strong></p>
<ul>
<li><strong>Data Entities:</strong> Data entities are the primary mechanism for exposing data for integration.</li>
<li><strong>OData:</strong> OData is a standard protocol for accessing data entities via RESTful APIs.</li>
<li><strong>Custom Services:</strong> Expose custom business logic through SOAP or JSON endpoints.</li>
<li><strong>Batch Data API:</strong> Used for asynchronous, high-volume data import/export.</li>
<li><strong>Security:</strong> Use appropriate authentication and authorization mechanisms (e.g., OAuth 2.0, Azure AD).</li>
<li><strong>Performance:</strong> Consider performance implications when designing integrations, especially for real-time or high-volume scenarios.</li>
<li><strong>Error Handling:</strong> Implement robust error handling and logging to ensure data integrity and system stability.</li>
</ul>
<p>In essence, this section provides an overview of how to integrate Finance and Operations apps with Azure Logic Apps, Microsoft Power Platform (Power Automate, Power BI, Power Apps), and Microsoft 365 (Excel). It highlights the various integration options available and emphasizes the importance of choosing the right approach based on specific business requirements.</p>
<p>Let's delve into the integration of Finance and Operations apps with Excel, Logic Apps, external web services, and Power Platform (Power Automate, Power Apps). We'll also explore the Common Data Model and its role in data integration.</p>
<p><strong>Integrating with Microsoft Excel:</strong></p>
<ul>
<li><strong>Excel Data Connector Add-in:</strong> Enables interacting with Finance and Operations data entities directly from Excel.</li>
<li><strong>Opening Data from Finance and Operations:</strong>
<ol>
<li>Open the desired page in Finance and Operations.</li>
<li>Select the Office icon in the Action Pane.</li>
<li>Choose the "Open in Excel" option for the desired table.</li>
<li>Enable editing in the downloaded Excel workbook.</li>
<li>Sign in with your Finance and Operations credentials.</li>
</ol>
</li>
<li><strong>Opening Data from within Excel:</strong>
<ol>
<li>In Excel, go to Insert &gt; Store and add the "Microsoft Dynamics Office Add-in."</li>
<li>Trust the add-in.</li>
<li>Select "Add server information" and enter the URL of your Finance and Operations instance.</li>
<li>Select "Design" and then "Add table."</li>
<li>Choose the desired entity and add fields.</li>
<li>Select "Refresh" to pull data.</li>
</ol>
</li>
<li><strong>Data Interaction Options:</strong>
<ul>
<li><strong>New:</strong> Add a new record.</li>
<li><strong>Refresh:</strong> Repopulate the table with the latest data.</li>
<li><strong>Publish:</strong> Save changes back to Finance and Operations.</li>
<li><strong>Delete:</strong> Delete a record.</li>
</ul>
</li>
</ul>
<p><strong>Automating Processes with Logic Apps:</strong></p>
<ul>
<li><strong>Enterprise Integration Pack (EIP):</strong> Provides features for building logic apps that handle B2B, EDI, and XML processing.</li>
<li><strong>Message Transformation:</strong> Logic Apps can convert messages between different formats (e.g., EDIFACT, AS2, X12).</li>
<li><strong>Security:</strong> Supports encryption and digital signatures.</li>
<li><strong>File System Connector:</strong> Access files on a local or network file system.
<ul>
<li>Triggers: Detect when files are added or modified.</li>
<li>Recurrence Trigger: Aggregate changes periodically.</li>
</ul>
</li>
<li><strong>Azure Blob Storage Connector:</strong> Access files in Azure Blob Storage.
<ul>
<li>"Get blob content": Retrieve file content.</li>
</ul>
</li>
<li><strong>Finance and Operations Connectors:</strong> Perform actions in Finance and Operations, such as:
<ul>
<li>Create record</li>
<li>Delete record</li>
<li>Get a record</li>
<li>Get list of entities</li>
<li>List items present in table</li>
<li>Update a record</li>
</ul>
</li>
</ul>
<p><strong>Consuming External Web Services:</strong></p>
<ul>
<li><strong>Wrapper Classes:</strong> Create wrapper classes (e.g., in C#) to encapsulate the logic for interacting with external web services.</li>
<li><strong>Service References:</strong> Add service references in Visual Studio to generate proxy classes for SOAP or REST services.</li>
<li><strong>HTTPClient (REST) or WCF (SOAP):</strong> Use .NET tools to make HTTP requests and handle responses.</li>
<li><strong>Authentication:</strong> Use appropriate authentication mechanisms (e.g., API keys, OAuth 2.0).</li>
<li><strong>Error Handling:</strong> Implement robust error handling to manage exceptions.</li>
<li><strong>Security:</strong> Store sensitive information (e.g., API keys) securely in Azure Key Vault.</li>
</ul>
<p><strong>Integrating with Microsoft Power Platform:</strong></p>
<ul>
<li><strong>Power Automate:</strong> Automate workflows and processes.</li>
<li><strong>Power BI:</strong> Perform data analysis and visualization.</li>
<li><strong>Power Apps:</strong> Build custom business applications.</li>
</ul>
<p><strong>Connecting to Power Automate:</strong></p>
<ol>
<li>Sign in to Power Automate.</li>
<li>Go to Connections and select "New connection."</li>
<li>Choose the desired connection (e.g., Finance and Operations) and click the plus sign (+).</li>
<li>Select "Create."</li>
<li>Enter your credentials to configure the connection.</li>
</ol>
<p><strong>Connecting to Power BI Desktop:</strong></p>
<ol>
<li>In Power BI Desktop, select "Get Data" from the Home ribbon.</li>
<li>Choose "Other" and then select "OData Feed."</li>
<li>Click "Connect."</li>
<li>Enter the OData endpoint URL for your Finance and Operations instance.</li>
<li>Select the data you need and click "Load."</li>
</ol>
<p><strong>Connecting to Power Apps:</strong></p>
<ol>
<li>When creating a new Power App, select the "Start from data" option.</li>
<li>Click the right arrow to show more connection options.</li>
<li>Click "New Connection".</li>
<li>Select "Dynamics 365 for Fin &amp; Ops."</li>
<li>Click "Create."</li>
<li>Choose the Finance and Operations instance and the data entity you want to connect to.</li>
<li>You need to be an admin user for the environment and have permissions to access the data entity.</li>
</ol>
<p><strong>Embedding Power Apps and Third-Party Apps in Finance and Operations:</strong></p>
<ul>
<li><strong>Embedding Power Apps:</strong>
<ol>
<li>Select the Power Apps icon in the top-right corner of a page.</li>
<li>Select "Add an app."</li>
<li>Enter the app name and ID.</li>
<li>Specify the input context and application size.</li>
<li>Optionally, specify legal entity access.</li>
<li>Select "Insert."</li>
</ol>
</li>
<li><strong>Embedding Third-Party Apps:</strong>
<ul>
<li>Requires development.</li>
<li>Use the website host control in Visual Studio to add third-party apps to Finance and Operations pages.</li>
</ul>
</li>
<li><strong>Controlling Embedding:</strong>
<ul>
<li>"isPowerAppPersonalizationEnabled": Controls whether users can embed Power Apps on a page.</li>
<li>"isPowerAppTabPersonalizationEnabled": Controls whether users can embed Power Apps as tabs, FastTabs, or sections.</li>
</ul>
</li>
</ul>
<p><strong>Common Data Model and Microsoft Dataverse:</strong></p>
<ul>
<li><strong>Common Data Model (CDM):</strong> A standardized data model that simplifies data integration across applications.</li>
<li><strong>Dataverse:</strong> A data platform that uses the CDM and provides a unified view of data.</li>
<li><strong>Integration:</strong> Finance and Operations data can be integrated with Dataverse, enabling data sharing and synchronization with other Dynamics 365 applications and Power Platform.</li>
</ul>
<p><strong>Example Scenario:</strong></p>
<p>A company using Dynamics 365 for Finance and Dynamics 365 Sales can use the CDM to standardize data structures (e.g., Account entity) across both applications, simplifying data integration and enabling a unified view of customer information.</p>
<p><strong>Finance and Operations Data in CDM:</strong></p>
<ul>
<li>Finance and Operations data is available in the CDM.</li>
<li>You can build downstream applications and extensions using the CDM.</li>
<li>Analytical tools like Power BI can combine Finance and Operations data with data from other applications using the CDM.</li>
</ul>
<p>In essence, this section provides a comprehensive overview of how to integrate Finance and Operations apps with various services and platforms, including Excel, Logic Apps, external web services, Power Platform (Power Automate, Power BI, Power Apps), and the Common Data Model. It emphasizes the importance of understanding the different integration options and choosing the right approach based on specific business needs. The examples and steps provided illustrate how to implement these integrations and leverage their capabilities to enhance business processes and data analysis.</p>
<p>Let's continue our exploration of data integration and migration, focusing on the Common Data Model, Dataverse, dual-write, selecting an integration strategy, common migration scenarios and tools, BYOD, testing data migration, and identifying legacy systems and static data.</p>
<p><strong>Common Data Model (CDM):</strong></p>
<ul>
<li><strong>Purpose:</strong> Provides a standardized, modular, and extensible collection of data schemas (entities, attributes, relationships) published by Microsoft.</li>
<li><strong>Benefits:</strong>
<ul>
<li>Simplifies data integration across applications and systems.</li>
<li>Enables a shared understanding of data.</li>
<li>Reduces the need for custom data mapping.</li>
</ul>
</li>
<li><strong>Finance and Operations Apps:</strong> All data in Finance and Operations, including tables and entities, is described in the CDM.</li>
<li><strong>Usage:</strong> You can use the CDM to integrate data from Finance and Operations with other applications and services that support the CDM.</li>
</ul>
<p><strong>Dataverse and Dual-Write:</strong></p>
<ul>
<li><strong>Dataverse:</strong> A cloud-based data platform that provides a unified view of data across Dynamics 365 applications and other data sources.</li>
<li><strong>Dual-Write:</strong> A feature that enables near real-time, bidirectional data synchronization between Finance and Operations and model-driven apps in Dynamics 365 (e.g., Sales, Customer Service).</li>
<li><strong>Example:</strong> When a new customer record is created in Dynamics 365 Sales, dual-write can automatically create a corresponding customer record in Finance and Operations, and vice-versa.</li>
<li><strong>Benefits:</strong>
<ul>
<li>Seamless data integration between Finance and Operations and other Dynamics 365 applications.</li>
<li>Eliminates the need for manual data entry or custom integrations.</li>
<li>Ensures data consistency across applications.</li>
</ul>
</li>
<li><strong>Extensibility:</strong> The dual-write framework can be extended to support custom entities and mappings.</li>
</ul>
<p><strong>Enabling Dual-Write:</strong></p>
<ol>
<li>Link your Finance and Operations environment to a Dataverse environment using the Dual-write wizard in the Data Management workspace.</li>
<li>Apply the dual-write application orchestration solution.</li>
<li>Enable the desired entity maps for synchronization.</li>
</ol>
<p><strong>Selecting a Data Integration Strategy:</strong></p>
<p>The choice of integration strategy depends on factors such as:</p>
<ul>
<li><strong>Data volume:</strong> How much data needs to be integrated?</li>
<li><strong>Frequency:</strong> How often does the data need to be updated (real-time, near real-time, scheduled)?</li>
<li><strong>Direction:</strong> Is the data flow unidirectional or bidirectional?</li>
<li><strong>Format:</strong> What is the format of the source and target data (e.g., CSV, XML, JSON)?</li>
<li><strong>Transformation requirements:</strong> Does the data need to be transformed or mapped before integration?</li>
<li><strong>Security:</strong> What are the security requirements for the integration?</li>
</ul>
<p><strong>Available Integration Patterns:</strong></p>
<ul>
<li><strong>OData:</strong> A standard protocol for creating and consuming RESTful APIs. Suitable for synchronous, real-time integration scenarios.</li>
<li><strong>Batch Data API:</strong> Used for asynchronous, high-volume data import and export. Supports the Data Management Package REST API and Recurring Integrations.</li>
<li><strong>Custom Service:</strong> Exposes X++ business logic through SOAP or JSON endpoints. Suitable for complex integration scenarios that require custom logic.</li>
<li><strong>Consume External Web Services:</strong> Allows Finance and Operations to consume external web services using wrapper classes and .NET tools like "HttpClient" or "WCF".</li>
<li><strong>Excel Integration:</strong> Enables users to view, update, and add data to data entities directly from Excel using the Data Connector add-in.</li>
</ul>
<p><strong>Common Migration Scenarios and Tools:</strong></p>
<ul>
<li><strong>Scenario 1: Real-time Inventory Lookup:</strong> An energy company needs to look up inventory availability in real-time from a third-party scheduling application.
<ul>
<li><strong>Recommended Pattern:</strong> Custom service to calculate on-hand inventory and expose it through a SOAP or REST endpoint.</li>
</ul>
</li>
<li><strong>Scenario 2: Periodic Sales Order Import:</strong> A company receives a large volume of sales orders from an on-premises system and needs to import them into Finance and Operations periodically.
<ul>
<li><strong>Recommended Pattern:</strong> Batch data APIs for asynchronous, high-volume data import.</li>
</ul>
</li>
</ul>
<p><strong>Data Migration Tools:</strong></p>
<ul>
<li><strong>Data Management Workspace:</strong> Used for setting up and managing data import/export projects.</li>
<li><strong>Office Integration:</strong> Using the Excel add-in for data manipulation.</li>
<li><strong>Excel Workbook Designer:</strong> Used to design Excel templates for data import/export.</li>
<li><strong>Data Integrator for Admins:</strong> A point-to-point integration service for integrating data into Dataverse.</li>
</ul>
<p><strong>Bring Your Own Database (BYOD):</strong></p>
<ul>
<li><strong>Purpose:</strong> Allows you to export data entities from Finance and Operations to your own Azure SQL database.</li>
<li><strong>Use Cases:</strong>
<ul>
<li>Exporting data to a data warehouse.</li>
<li>Using analytical tools that require T-SQL access.</li>
<li>Performing batch integration with other systems.</li>
</ul>
</li>
<li><strong>Features:</strong>
<ul>
<li>Full or incremental data export.</li>
<li>Scheduled exports using the batch framework.</li>
<li>T-SQL access to the exported data.</li>
<li>Ability to extend the database with additional tables.</li>
</ul>
</li>
<li><strong>Configuration:</strong>
<ol>
<li>Create an Azure SQL database.</li>
<li>Create a SQL user account for the database.</li>
<li>In the Data Management workspace, go to "Configure entity export to database."</li>
<li>Configure a new database connection using the connection string.</li>
<li>Publish the desired entities to the database.</li>
<li>Use the "Export" function in the Data Management workspace to move data.</li>
</ol>
</li>
<li><strong>Considerations:</strong>
<ul>
<li>Premium databases support clustered columnstore indexes (CCIs) for improved query performance.</li>
<li>Timeouts for truncation and bulk insert operations might need adjustment for high volumes.</li>
<li>Ensure no active locks on the database during synchronization.</li>
<li>Composite entities cannot be exported to BYOD.</li>
<li>Entities without unique keys cannot be exported using incremental push.</li>
</ul>
</li>
<li><strong>Downstream Reporting:</strong> Avoid having reporting systems read directly from staging tables during data synchronization. Use SQL triggers to determine when the sync is complete and then update downstream systems.</li>
</ul>
<p><strong>Testing Data Migration and Validating Output:</strong></p>
<ul>
<li><strong>Identify Relevant (Legacy) Systems:</strong> Understand the customer's current systems and their data formats.</li>
<li><strong>Identify and Import Static Data:</strong> Import common static data (e.g., country codes, postal codes) before the main data migration.</li>
<li><strong>Create and Review Test Plans:</strong> Develop a comprehensive test plan that includes unit, system, and regression testing.</li>
<li><strong>Identify and Extract Source Data:</strong> Identify the relevant data sources and entities for the migration. Extract the data in a suitable format.</li>
<li><strong>Generate Field Mapping:</strong> Map source fields to target fields in the data entities.</li>
<li><strong>Perform a Test Migration:</strong> Conduct a test migration in a non-production environment.</li>
<li><strong>Validate Output:</strong> Verify the migrated data for accuracy and completeness. Use reports and inquiries in Finance and Operations to compare the migrated data with the source data.</li>
<li><strong>Support the Transition:</strong> Ensure a smooth transition from the legacy system to the migrated system. Provide training and support to users. Monitor the system after go-live.</li>
</ul>
<p>In essence, this section provides a comprehensive overview of data integration and migration strategies, scenarios, and tools in Dynamics 365 Finance and Operations. It covers various integration patterns, including OData, batch data APIs, custom services, consuming external web services, and Excel integration. It also discusses the use of the Common Data Model, Dataverse, and dual-write for seamless data integration. The section emphasizes the importance of choosing the right integration approach based on specific business needs and provides guidance on configuring and using the BYOD feature. Finally, it outlines the key steps involved in planning and executing a successful data migration, including testing, validation, and supporting the transition to the new system.</p>
<p>Let's continue our discussion, focusing on data migration, the Data Management Platform, data entities, and the different categories of data entities.</p>
<p><strong>Data Migration:</strong></p>
<ul>
<li><strong>Planning:</strong>
<ul>
<li>Ensure proper system permissions for users involved in the migration.</li>
<li>Back up the database and source files before starting the migration.</li>
</ul>
</li>
<li><strong>Process:</strong>
<ol>
<li>Extract data from the source system.</li>
<li>Migrate data to the target system (Finance and Operations).</li>
<li>Clean and transform the data into the appropriate format.</li>
<li>Load the data into the Data Management Framework.</li>
<li>Map fields between the source and target systems.</li>
<li>Import the data into Finance and Operations.</li>
</ol>
</li>
<li><strong>Monitoring:</strong> Continuously monitor the migration process to identify and resolve issues.</li>
<li><strong>Validation:</strong>
<ul>
<li>Verify data accuracy and completeness after migration.</li>
<li>Perform unit, system, and regression testing.</li>
<li>Compare migrated data with the legacy system using reports and inquiries.</li>
<li>Involve key stakeholders (e.g., CFO) to validate the migrated data.</li>
</ul>
</li>
<li><strong>Example:</strong> When migrating financial data for a specific period, run a trial balance report in Finance and Operations and compare it with the corresponding report from the legacy system.</li>
</ul>
<p><strong>Identifying and Extracting Source Data:</strong></p>
<ul>
<li><strong>Analysis:</strong> Analyze the data in the source system and determine the relevant data for migration.</li>
<li><strong>Extraction:</strong> Extract the relevant data from the source system.</li>
<li><strong>Further Processing:</strong> You might need to add metadata or perform other data integration tasks.</li>
</ul>
<p><strong>Identifying Relevant Data Entities and Elements:</strong></p>
<ul>
<li><strong>Focus on Relevance:</strong> Only migrate data that is relevant to the specific business scenario.</li>
<li><strong>Plan the Migration:</strong> Develop a plan that outlines the data to be migrated and the order of migration.</li>
<li><strong>Example:</strong> When migrating purchase orders, consider related entities like vendor groups, vendors, tax codes, and purchase order headers and lines.</li>
<li><strong>Data Cleansing:</strong>
<ul>
<li>Verify data availability, accessibility, completeness, and format.</li>
<li>Define rules for manual and automated cleansing.</li>
<li>Manual cleansing is typically done before migration.</li>
<li>Automated cleansing can be done before or during the initial phase of migration.</li>
</ul>
</li>
</ul>
<p><strong>Generating Field Mapping:</strong></p>
<ul>
<li><strong>Identify Source Data Format:</strong> View available formats in the Data Management workspace under "Configure data source."</li>
<li><strong>Field Mapping:</strong> When you add an entity to a data project, Finance and Operations automatically creates a field mapping between the source data and the target table.</li>
<li><strong>Mapping Details:</strong> The "Mapping details" tab shows metadata about the table fields (e.g., type, required field) and provides options for:
<ul>
<li>Ignoring blank values.</li>
<li>Using a text qualifier.</li>
<li>Using the label for enumerator fields.</li>
</ul>
</li>
</ul>
<p><strong>Supporting the Transition:</strong></p>
<ul>
<li><strong>Data Validation:</strong> Validate the migrated data to ensure accuracy and completeness.</li>
<li><strong>System Comparison:</strong> Understand how each system (legacy and Finance and Operations) works and how the data is structured.</li>
<li><strong>Focus on Data Assurance:</strong> Ensure that the data is properly migrated and validated.</li>
</ul>
<p><strong>Data Management Platform:</strong></p>
<ul>
<li><strong>Purpose:</strong> Facilitates data import, export, and integration.</li>
<li><strong>Features:</strong>
<ul>
<li>Import/export data using data entities.</li>
<li>Perform data quality services and validation.</li>
<li>Map data from input to target.</li>
<li>Pre- and post-processing of data.</li>
<li>Schedule data imports/exports using batches.</li>
</ul>
</li>
<li><strong>Concepts:</strong>
<ul>
<li><strong>Data Entities:</strong> Conceptual abstractions of underlying tables, representing common data concepts (e.g., customers, vendors).</li>
<li><strong>Data Project:</strong> Contains configured data entities, mapping, and processing options.</li>
<li><strong>Data Job:</strong> An instance of a data project that performs the actual import/export operation.</li>
<li><strong>Job History:</strong> Tracks the history of data jobs, including execution details and errors.</li>
<li><strong>Data Package:</strong> A compressed file containing a data project manifest and data files. Used for ALM and data migration.</li>
</ul>
</li>
</ul>
<p><strong>Data Entities:</strong></p>
<ul>
<li><strong>Purpose:</strong> Provide a simplified, de-normalized view of data for easier development and integration.</li>
<li><strong>Capabilities:</strong>
<ul>
<li>Encapsulate business logic.</li>
<li>Enable import/export and integration scenarios.</li>
<li>Expose OData services for synchronous integration.</li>
<li>Support asynchronous integration through the Data Management Framework.</li>
</ul>
</li>
<li><strong>Example:</strong> A customer entity might combine data from multiple tables (e.g., "DirParty", "CustTable", "LogisticsPostalAddress", "LogisticsElectronicAddress") into a single view.</li>
</ul>
<p><strong>Categories of Data Entities:</strong></p>
<ul>
<li><strong>Parameters:</strong> Functional or behavioral settings for a module or process.</li>
<li><strong>Reference:</strong> Static or slowly changing data (e.g., units of measure, dimensions).</li>
<li><strong>Master:</strong> Core business data (e.g., customers, vendors, products).</li>
<li><strong>Document:</strong> Worksheets or documents (e.g., sales orders, purchase orders).</li>
<li><strong>Transaction:</strong> Operational transactions (e.g., sales order lines, purchase order lines).</li>
</ul>
<p>In essence, this section provides a detailed overview of data migration and the Data Management Platform in Dynamics 365 Finance and Operations. It covers the key steps involved in planning and executing a data migration, including identifying relevant systems and data, creating test plans, extracting and transforming data, generating field mappings, performing test migrations, and validating the output. It also explains the core concepts of the Data Management Framework, such as data entities, data projects, data jobs, job history, and data packages. Finally, it categorizes data entities based on their function and the type of data they represent. This information is essential for successfully migrating data to Finance and Operations and ensuring data integrity and consistency.</p>
<p>Let's continue our discussion, focusing on the remaining aspects of data integration, including data migration best practices, the Data Management Framework, data entities, and specific features like change tracking, composite entities, and aggregate data entities. We'll also cover BYOD, testing data migration, supporting the transition, and using templates in data management.</p>
<p><strong>Data Migration Best Practices:</strong></p>
<ul>
<li><strong>Planning:</strong>
<ul>
<li>Develop a detailed plan for the data migration process.</li>
<li>Ensure appropriate system permissions for users involved in the migration.</li>
<li>Back up the database and source files before starting.</li>
</ul>
</li>
<li><strong>Process:</strong>
<ol>
<li>Extract data from the source system.</li>
<li>Migrate data to Finance and Operations.</li>
<li>Clean and transform the data into the correct format.</li>
<li>Load the cleaned data into the Data Management Framework.</li>
<li>Map fields between source and target systems.</li>
<li>Import the data into Finance and Operations.</li>
</ol>
</li>
<li><strong>Monitoring:</strong> Continuously monitor the migration process to identify and resolve issues.</li>
<li><strong>Validation:</strong>
<ul>
<li>Verify data accuracy and completeness after migration.</li>
<li>Perform unit, system, and regression testing.</li>
<li>Compare migrated data with the legacy system using reports and inquiries.</li>
<li>Involve key stakeholders (e.g., CFO) to validate migrated data.</li>
</ul>
</li>
<li><strong>Example:</strong> When migrating financial data, run a trial balance report in Finance and Operations and compare it with the legacy system's report.</li>
</ul>
<p><strong>Identifying and Extracting Source Data:</strong></p>
<ul>
<li><strong>Analysis:</strong> Analyze data in the source system to identify relevant information.</li>
<li><strong>Extraction:</strong> Extract the relevant data from the source.</li>
<li><strong>Further Processing:</strong> You might need to add metadata or perform other data integration tasks.</li>
</ul>
<p><strong>Identifying Relevant Data Entities and Elements:</strong></p>
<ul>
<li><strong>Focus on Relevance:</strong> Only migrate data that is relevant to the specific business scenario.</li>
<li><strong>Planning:</strong> Develop a plan that outlines the data to be migrated and the order of migration.</li>
<li><strong>Example:</strong> When migrating purchase orders, consider related entities like vendor groups, vendors, tax codes, and purchase order headers and lines.</li>
<li><strong>Data Cleansing:</strong>
<ul>
<li>Verify data availability, accessibility, completeness, and format.</li>
<li>Define rules for manual and automated cleansing.</li>
<li>Manual cleansing is typically done before migration.</li>
<li>Automated cleansing can be done before or during the initial phase of migration.</li>
</ul>
</li>
</ul>
<p><strong>Generating Field Mapping:</strong></p>
<ul>
<li><strong>Identify Source Data Format:</strong> View available formats in the Data Management workspace under "Configure data source."</li>
<li><strong>Field Mapping:</strong> When you add an entity to a data project, Finance and Operations automatically creates a field mapping between the source data and the target table.</li>
<li><strong>Mapping Details:</strong> The "Mapping details" tab shows metadata about the table fields (e.g., type, required field) and provides options for:
<ul>
<li>Ignoring blank values.</li>
<li>Using a text qualifier.</li>
<li>Using the label for enumerator fields.</li>
</ul>
</li>
<li><strong>Map Staging to Target:</strong> The "Map staging to target" page allows you to map fields between the staging table and the target table.</li>
</ul>
<p><strong>Supporting the Transition:</strong></p>
<ul>
<li><strong>Data Validation:</strong> Validate migrated data to ensure accuracy and completeness.</li>
<li><strong>System Comparison:</strong> Understand how each system (legacy and Finance and Operations) works and how data is structured.</li>
<li><strong>Focus on Data Assurance:</strong> Ensure that data is properly migrated and validated.</li>
</ul>
<p><strong>Data Management Framework:</strong></p>
<ul>
<li><strong>Concepts:</strong>
<ul>
<li><strong>Data Entities:</strong> Abstractions of underlying tables, representing common data concepts.</li>
<li><strong>Data Project:</strong> Contains configured data entities, mapping, and processing options.</li>
<li><strong>Data Job:</strong> An instance of a data project that performs the actual import/export.</li>
<li><strong>Job History:</strong> Tracks the history of data jobs, including execution details and errors.</li>
<li><strong>Data Package:</strong> A compressed file containing a data project manifest and data files. Used for ALM and data migration.</li>
</ul>
</li>
<li><strong>Features:</strong>
<ul>
<li>Import/export data.</li>
<li>Perform data quality services and validation.</li>
<li>Map data from input to target.</li>
<li>Pre- and post-processing of data.</li>
<li>Schedule data imports/exports using batches.</li>
</ul>
</li>
<li><strong>Scenarios:</strong>
<ul>
<li><strong>Data Migration:</strong> Migrate data from legacy or external systems.</li>
<li><strong>Set Up and Copy Configurations:</strong> Copy configurations between companies or environments.</li>
<li><strong>Integration:</strong> Real-time, service-based integration or asynchronous integration.</li>
</ul>
</li>
</ul>
<p><strong>Data Entities:</strong></p>
<ul>
<li><strong>Categories:</strong>
<ul>
<li><strong>Parameters:</strong> Settings tables with only one record (e.g., AP, GL, client performance options, workflows).</li>
<li><strong>Reference:</strong> Simple reference data required for business processes (e.g., units, dimensions, tax codes).</li>
<li><strong>Master:</strong> Core business data (e.g., customers, vendors, projects).</li>
<li><strong>Document:</strong> Worksheet data that is later converted into transactions (e.g., sales orders, purchase orders, open balances, journals).</li>
<li><strong>Transaction:</strong> Operational transaction data (e.g., posted invoices, balances).</li>
</ul>
</li>
</ul>
<p><strong>Configuration Keys and Data Entities:</strong></p>
<ul>
<li><strong>Configuration Keys:</strong> Control the availability of data entities and their components (tables, fields).</li>
<li><strong>Impact:</strong>
<ul>
<li>If a data entity's configuration key is disabled, the entity is not available.</li>
<li>If a data entity's configuration key is enabled, but the primary table's key is disabled, the entity is not available.</li>
<li>If a data entity's and table's configuration keys are enabled, but some fields are disabled, those fields are not available in the entity.</li>
</ul>
</li>
<li><strong>Entity List Refresh:</strong> Rebuilds the configuration key metadata. It's important to refresh the entity list after changing configuration keys.</li>
</ul>
<p><strong>Parallel Processing:</strong></p>
<ul>
<li>Enables faster data import by using multiple threads.</li>
<li>Configure in the Data Management workspace (Framework parameters &gt; Entity settings).</li>
<li>Specify the "Import threshold record count" and "Import task count."</li>
</ul>
<p><strong>Job History Clean-up:</strong></p>
<ul>
<li>Schedules periodic cleanup of the execution history.</li>
<li>Replaces the previous staging table cleanup functionality.</li>
<li>Uses system batch jobs for automatic cleanup.</li>
<li>Archived data is stored in blob storage in the Data Management Framework package format.</li>
</ul>
<p><strong>Sequencing Data Entities:</strong></p>
<ul>
<li><strong>Within a Data Package:</strong>
<ul>
<li>Entities are loaded in the order they are added to the project by default.</li>
<li>You can change the sequence using the "Entity sequence" button in the data project.</li>
<li>Execution units and sequence levels determine the order of entity processing.</li>
<li>"Auto sequence" can automatically determine the sequence based on dependencies.</li>
</ul>
</li>
<li><strong>Data Package Imports:</strong>
<ul>
<li>Set the correct order for importing data packages due to dependencies within and across modules.</li>
<li>Suggested numbering format for data packages:
<ul>
<li>First segment: Module</li>
<li>Second segment: Data type (setup, master, transaction)</li>
<li>Third segment: Sequence number</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>Mapping:</strong></p>
<ul>
<li><strong>Automatic Mapping:</strong> Finance and Operations automatically maps fields between the source and target tables when you add an entity to a data project.</li>
<li><strong>Overriding Mapping:</strong> You can manually override the automatic mapping.</li>
<li><strong>Viewing Mapping:</strong> Select the "View map" option for an entity in the data project.</li>
<li><strong>Mapping Visualization:</strong> Provides a visual representation of the mapping.</li>
<li><strong>Mapping Details:</strong> Shows the detailed field mapping and allows for basic transformations.</li>
<li><strong>Regenerating a Map:</strong> If you extend an entity or the automatic mapping is incorrect, you can regenerate the mapping using the "Generate source mapping" option.</li>
</ul>
<p><strong>Generating Data:</strong></p>
<ul>
<li><strong>Auto-generated Fields:</strong> You can configure fields to be automatically generated during import (e.g., party number).</li>
<li><strong>Example:</strong> When importing customers and addresses, if the party number is not in the import file, you can enable auto-generation to create new party numbers.</li>
</ul>
<p><strong>Standard and Enhanced Views:</strong></p>
<ul>
<li><strong>Standard View:</strong> Streamlined interface with basic functionality.</li>
<li><strong>Enhanced View:</strong> Provides more detailed information and controls.</li>
<li><strong>Entity Sequence:</strong> The "Entity sequence" button is only available in the Standard view.</li>
</ul>
<p><strong>Using Templates in Data Management:</strong></p>
<ul>
<li><strong>Default Templates:</strong> Provided with each new release of Finance and Operations.</li>
<li><strong>Loading Templates:</strong> In the Data Management workspace, select the "Templates" tile and then "Load default templates" (available in Enhanced view).</li>
<li><strong>Customization:</strong> You can modify the default templates to suit your business requirements.</li>
<li><strong>Merging Templates:</strong> You can create larger templates that cover multiple modules or merge smaller templates into a data project.</li>
<li><strong>Overriding Templates:</strong> If you add a template that contains an entity already in a data project, the entity in the template replaces the existing entity.</li>
</ul>
<p><strong>Change Tracking for Entities:</strong></p>
<ul>
<li><strong>Purpose:</strong> Enables incremental export of data by tracking changes to records.</li>
<li><strong>Options:</strong>
<ul>
<li><strong>Enable primary table:</strong> Tracks changes in the primary table.</li>
<li><strong>Enable entire entity:</strong> Tracks changes in all tables within the entity.</li>
<li><strong>Enable custom query:</strong> Tracks changes based on a custom query.</li>
</ul>
</li>
<li><strong>BYOD:</strong> Change tracking can also track deletions for BYOD scenarios if the entity supports it.</li>
</ul>
<p><strong>Export, Import, and Copy Data into a Legal Entity:</strong></p>
<ul>
<li><strong>Data Import/Export Jobs:</strong> Use the Data Management workspace to create and manage import/export jobs.</li>
<li><strong>Copy into Legal Entity:</strong> Use this option to copy data between legal entities within the same Finance and Operations instance.</li>
</ul>
<p>In essence, this section provides a comprehensive overview of data migration best practices, the Data Management Framework, data entities, and various features related to data integration and management in Dynamics 365 Finance and Operations. It covers topics such as sequencing data entities, mapping fields, using templates, enabling change tracking, and performing import/export operations. This information is essential for effectively managing data and ensuring data integrity during migration and integration processes.</p>
<p>Let's conclude our discussion by exploring the final aspects of data integration and management in Dynamics 365 Finance and Operations, including data import/export jobs, data transformation, database movement operations, the data sharing framework, and other data management tools. We'll also touch upon testing and validation using Task Recorder and Business Process Modeler (BPM).</p>
<p><strong>Data Import and Export Jobs:</strong></p>
<ul>
<li><strong>Data Management Workspace:</strong> Used to create and manage data import and export jobs.</li>
<li><strong>Staging Tables:</strong> The process creates staging tables for each entity to verify, clean, or convert data before moving it to the target tables.</li>
<li><strong>Steps to Create an Import or Export Job:</strong>
<ol>
<li>Define the project category.</li>
<li>Identify the entities to import or export.</li>
<li>Set the data format for the job.</li>
<li>Sequence the entities logically.</li>
<li>Determine whether to use staging tables.</li>
<li>Validate that the source and target data are mapped correctly.</li>
<li>Verify security for the job.</li>
<li>Run the import or export job.</li>
<li>Validate that the job ran as expected.</li>
<li>Clean up the staging tables.</li>
</ol>
</li>
</ul>
<p><strong>Creating an Import or Export Job:</strong></p>
<ul>
<li>Can be run one time or multiple times.</li>
<li>Project categories help manage related jobs.</li>
<li>Select the data format (e.g., CSV, Excel, Package).</li>
<li>Sequence entities to address functional dependencies.</li>
<li>Use the "Entity sequence" button in Standard view to define the order.</li>
<li>"Auto sequence" can automatically determine the sequence.</li>
<li>Example: Sales tax codes and details should be loaded before sales tax groups.</li>
<li>Example: Entities without dependencies can be in their own execution unit and load immediately.</li>
</ul>
<p><strong>Security for Import/Export Jobs:</strong></p>
<ul>
<li><strong>Applicable Roles:</strong> Restrict job access to specific security roles.</li>
<li><strong>Applicable Users:</strong> Restrict job access to specific users within a role.</li>
<li><strong>Applicable Legal Entities:</strong> Control access to jobs based on legal entities.</li>
</ul>
<p><strong>Cleaning Up Staging Tables:</strong></p>
<ul>
<li>Use the "Staging clean up" feature in the Data Management workspace.</li>
<li>Options for deleting records:
<ul>
<li><strong>Entity:</strong> Deletes all records from the entity's staging table.</li>
<li><strong>Job ID:</strong> Deletes records for all entities in the selected job.</li>
<li><strong>Data projects:</strong> Deletes records for all entities and jobs in the selected data project.</li>
</ul>
</li>
</ul>
<p><strong>Data Transformation:</strong></p>
<p>Data transformation can be performed between the source and the staging table, or between the staging table and the target table.</p>
<ul>
<li><strong>Transformation between Source and Staging:</strong>
<ul>
<li>Accessed by selecting the "View map" icon when creating an import/export project.</li>
<li><strong>Mapping Visualization:</strong> Shows the mapping between source and staging fields.</li>
<li><strong>Mapping Details:</strong> Provides options for basic transformations:
<ul>
<li>Ignore blank values.</li>
<li>Text qualifier.</li>
<li>Use enum label.</li>
<li>Auto-generated.</li>
<li>Auto default.</li>
<li>Conversion (value mapping).</li>
</ul>
</li>
</ul>
</li>
<li><strong>Transformation between Staging and Target:</strong>
<ul>
<li>Accessed by selecting "Modify target mapping" for a data entity in the Data Management workspace.</li>
<li><strong>Mapping Details:</strong> Shows the mapping between staging and target fields.</li>
<li><strong>Computed Columns:</strong>
<ul>
<li>Use SQL view computed columns for calculations.</li>
<li>More efficient than virtual fields.</li>
<li>Primarily for read operations.</li>
</ul>
</li>
<li><strong>Virtual Fields:</strong>
<ul>
<li>Non-persisted fields controlled by custom X++ code.</li>
<li>Used for complex calculations or transformations.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>Database Movement Operations:</strong></p>
<ul>
<li><strong>Purpose:</strong> A suite of self-service actions for Data Application Lifecycle Management (DataALM).</li>
<li><strong>Features:</strong>
<ul>
<li>Run database operations between Tier-2+ environments in Lifecycle Services.</li>
<li>Create requests for database operations for production environments.</li>
<li>Refresh database (copy data from one environment to another).</li>
<li>Export database (export a database to the Asset Library).</li>
<li>Import database (import a database from a developer environment or a previously exported database).</li>
<li>Point-in-time restore (PITR) for sandbox and production environments.</li>
</ul>
</li>
<li><strong>Database Movement Operations between Tier-1 and Tier-2+:</strong>
<ul>
<li>Bacpac procedure (SQL Server to Azure SQL or vice-versa).</li>
<li>SQL Server backup/restore between one-box environments.</li>
</ul>
</li>
</ul>
<p><strong>Data Sharing Framework:</strong></p>
<ul>
<li><strong>Purpose:</strong> Replicates (shares) reference and group data among companies within the same deployment.</li>
<li><strong>Features:</strong>
<ul>
<li>Data integrity is verified before replication.</li>
<li>Immediate replication of changes across companies.</li>
<li>Fields not selected for sharing are maintained separately in each company.</li>
<li>Existing records can be copied when enabling a policy.</li>
</ul>
</li>
<li><strong>Limitations:</strong>
<ul>
<li>Cannot be used for transactional data.</li>
<li>Cannot be used with dual-write.</li>
<li>Not supported for complex scenarios (e.g., subtype/supertype tables, tables with date effectivity rules).</li>
<li>Not supported for tables without a unique index.</li>
</ul>
</li>
</ul>
<p><strong>When to Use Data Sharing:</strong></p>
<ul>
<li>Sharing simple reference and group data in a single deployment.</li>
<li>Sharing among companies with similar configurations.</li>
<li>Sharing scenarios explicitly evaluated by Microsoft.</li>
</ul>
<p><strong>When Not to Use Data Sharing:</strong></p>
<ul>
<li>Franchising solutions with thousands of records shared across many companies.</li>
<li>Sharing transactional records for reporting or management purposes.</li>
<li>Sharing across deployments.</li>
</ul>
<p><strong>Other Data Management Tools:</strong></p>
<ul>
<li><strong>Bring Your Own Database (BYOD):</strong> Export entities to your own Azure SQL database.</li>
<li><strong>Entity Store Refresh:</strong> For embedded Power BI reports.</li>
<li><strong>Process Data Package:</strong> Consolidates multiple data packages into one bundle.</li>
<li><strong>Data Validation Checklist Workspace:</strong> Tracks data validation processes.</li>
<li><strong>Office Add-in:</strong> Export and import data via Excel add-in using data entities.</li>
</ul>
<p><strong>Testing and Validation:</strong></p>
<ul>
<li><strong>Task Recorder:</strong> A tool for recording test cases.</li>
<li><strong>Business Process Modeler (BPM):</strong> Used to organize test cases by business processes and create user acceptance test libraries.</li>
<li><strong>User Acceptance Testing (UAT):</strong> End users test the system to ensure it meets their requirements.</li>
</ul>
<p><strong>Environment for Testing and Evaluation:</strong></p>
<p>Before going live, you need a Finance and Operations environment with all configurations and customizations for testing and evaluation.</p>
<p><strong>Methodology for Creating and Testing:</strong></p>
<ul>
<li>Organize and plan the creation and testing process.</li>
<li>Use short, efficient, and role-focused test case scenarios.</li>
</ul>
<p><strong>Validating the Solution:</strong></p>
<ul>
<li>Create test cases to validate customizations and identify configuration issues.</li>
<li>Use test cases for user acceptance testing.</li>
</ul>
<p>In essence, this section provides a comprehensive overview of data management and integration in Dynamics 365 Finance and Operations. It covers various aspects, including data import/export jobs, data transformation techniques, database movement operations, the data sharing framework, and other data management tools. It also emphasizes the importance of testing and validation before going live with a new implementation or customization. The information provided is crucial for effectively managing data and ensuring data integrity in Finance and Operations.</p>
<p>Let's continue our exploration of Dynamics 365 Finance and Operations, focusing on testing, validation, and automation. We'll cover topics such as creating test libraries using Business Process Modeler (BPM), recording test cases with Task Recorder, synchronizing with Azure DevOps, running user acceptance tests, utilizing the Regression Suite Automation Tool (RSAT), and automating data tasks.</p>
<p><strong>Testing and Validation:</strong></p>
<ul>
<li><strong>Environment:</strong> Before going live, you need a Finance and Operations environment with all configurations and customizations for testing and evaluation.</li>
<li><strong>Methodology:</strong> Use a structured methodology to organize and plan testing.</li>
<li><strong>Test Cases:</strong>
<ul>
<li>Create test cases to validate customizations and identify configuration issues.</li>
<li>Use test cases for user acceptance testing (UAT).</li>
</ul>
</li>
<li><strong>Tools:</strong>
<ul>
<li><strong>Task Recorder:</strong> Records user actions to create test cases.</li>
<li><strong>Business Process Modeler (BPM):</strong> Organizes test cases by business processes and creates UAT libraries.</li>
<li><strong>Azure DevOps:</strong> Manages test plans, test suites, test execution, and results.</li>
<li><strong>Regression Suite Automation Tool (RSAT):</strong> Automates the running of test cases based on task recordings.</li>
</ul>
</li>
</ul>
<p><strong>Creating User Acceptance Test Libraries with BPM:</strong></p>
<ul>
<li><strong>BPM:</strong> A Lifecycle Services tool that describes a hierarchy of business processes and user tasks.</li>
<li><strong>BPM Libraries:</strong>
<ul>
<li>Can be global (distributed by Microsoft) or corporate (published by your organization).</li>
<li>Must be part of your Lifecycle Services project before you can edit them.</li>
</ul>
</li>
<li><strong>Creating a BPM Library:</strong>
<ol>
<li>Sign in to Lifecycle Services.</li>
<li>Select the "Business process modeler" tile.</li>
<li>Create a new library.</li>
</ol>
<ul>
<li>You can also import a library from Excel.</li>
</ul>
</li>
<li><strong>Adding a Process:</strong>
<ol>
<li>Select an existing process in the library.</li>
<li>Select "Add process" and choose "As child" or "As sibling."</li>
<li>Edit the process properties (name, description, industries, countries/regions, keywords, links).</li>
</ol>
</li>
<li><strong>Copying a Library:</strong>
<ol>
<li>Open the "Business process libraries" page in Lifecycle Services.</li>
<li>Select the library to copy and click the ellipsis button (...).</li>
<li>Select "Copy."</li>
<li>Enter a name for the new library.</li>
</ol>
</li>
</ul>
<p><strong>Recording Test Cases and Saving to BPM:</strong></p>
<ul>
<li><strong>Task Recorder:</strong> Used to record user actions and create test cases.</li>
<li><strong>Starting Point:</strong> Task recordings should start on the main dashboard of Finance and Operations.</li>
<li><strong>End-to-End Processes:</strong> Divide large processes into smaller, user-specific tasks for easier maintenance and security role-based testing.</li>
<li><strong>Creating a Task Recording:</strong>
<ol>
<li>Go to Settings &gt; Task recorder.</li>
<li>Select "Create recording."</li>
<li>Enter a name and description.</li>
<li>Select "Start."</li>
<li>Perform the steps of the task.</li>
<li>Select "Stop" when finished.</li>
</ol>
</li>
<li><strong>Saving to BPM:</strong>
<ol>
<li>Select "Save to Lifecycle Services."</li>
<li>Choose the library and process to save the recording to.</li>
</ol>
</li>
<li><strong>Saving to PC:</strong>
<ol>
<li>Select "Save to this PC" to save the recording as an AXTR file.</li>
</ol>
</li>
<li><strong>Uploading an AXTR File to BPM:</strong>
<ol>
<li>In Lifecycle Services, in your project, go to the "Business process libraries" page.</li>
<li>Select the library to upload to.</li>
<li>Select "Upload" in the right pane.</li>
<li>Browse for the AXTR file and select "Upload."</li>
</ol>
</li>
<li><strong>Downloading a Task Recording:</strong>
<ol>
<li>In Lifecycle Services, in your project, go to the "Business process libraries" page.</li>
<li>Select the library containing the recording.</li>
<li>Select the process with the recording.</li>
<li>Select "Download" in the Overview pane.</li>
</ol>
</li>
<li><strong>Saving an Existing Task Recording to BPM:</strong>
<ol>
<li>Go to Settings &gt; Task recorder.</li>
<li>Select "Edit Recording."</li>
<li>Attach the file by saving directly to Lifecycle Services or downloading the AXTR and then uploading to BPM.</li>
</ol>
</li>
</ul>
<p><strong>Guidelines for Recording Test Cases:</strong></p>
<ul>
<li>Write a limited number of test cases that cover complete end-to-end processes when combined.</li>
<li>Focus on customized business processes.</li>
<li>Each test case should cover one or two business tasks performed by one person.</li>
<li>Have at least one validation in each test case, focusing on critical fields.</li>
<li>Avoid printing reports in test cases; select them on screen instead.</li>
<li>80+% of test cases should be transactions or source documents; limit master data to 20%.</li>
</ul>
<p><strong>Synchronizing and Configuring Your Test Plan in Azure DevOps:</strong></p>
<ul>
<li><strong>Acceptance Test Library:</strong> Your starting point, containing test cases organized by business processes.</li>
<li><strong>Test Plans and Test Suites:</strong>
<ul>
<li><strong>Test Plan:</strong> Contains one or more test suites.</li>
<li><strong>Test Suite:</strong> A subset of the test library.</li>
<li>A test case can belong to multiple test suites.</li>
</ul>
</li>
<li><strong>Synchronization with Azure DevOps:</strong>
<ol>
<li>Configure the connection between Lifecycle Services and your Azure DevOps project.</li>
<li>In the BPM library, select the ellipsis button (...) and then "Azure DevOps sync."</li>
<li>Select "Sync test cases."</li>
</ol>
<ul>
<li>Task recordings become test cases in Azure DevOps.</li>
<li>The task recording XML file is attached to the test case.</li>
</ul>
</li>
</ul>
<p><strong>Running User Acceptance Tests:</strong></p>
<ul>
<li><strong>Creating Test Plans and Suites in Azure DevOps:</strong>
<ol>
<li>Sign in to Azure DevOps and select the project and test plan.</li>
<li>Go to Test &gt; Test Plans.</li>
<li>Create a new static suite.</li>
<li>Select "Add existing" and query the tag "Lifecycle Services:Test Cases."</li>
<li>Select "Run" &gt; "Add test cases."</li>
</ol>
</li>
<li><strong>Running Manual Test Cases:</strong>
<ul>
<li>Use Azure DevOps to run test cases manually.</li>
<li>Play task recordings and mark test cases as passed or failed.</li>
<li>Use Test Runner for managing manual test case runs (see "Run manual tests" in Azure DevOps documentation).</li>
</ul>
</li>
<li><strong>Running Automated Test Cases:</strong>
<ul>
<li>Use the Regression Suite Automation Tool (RSAT) for automated testing.</li>
<li>RSAT is integrated with Azure DevOps for test execution, reporting, and investigation.</li>
<li>Test parameters are stored in Excel files.</li>
</ul>
</li>
<li><strong>Investigating Test Runs:</strong>
<ul>
<li>In Azure DevOps, go to Test &gt; Runs to investigate test run results.</li>
<li>View test case details and attached XML files.</li>
<li>View the latest results in your test suite.</li>
</ul>
</li>
</ul>
<p><strong>RSAT Tool:</strong></p>
<ul>
<li>RSAT can be used without an Azure DevOps license on a trial basis to test out the tool before committing to the Azure DevOps subscription.</li>
<li>RSAT supports parallel execution to run multiple test cases concurrently.</li>
</ul>
<p><strong>Data Task Automation:</strong></p>
<ul>
<li><strong>Purpose:</strong> Automates data-related tasks, such as creating and configuring data projects, and running import/export operations.</li>
<li><strong>Use Cases:</strong>
<ul>
<li>Automating the setup of demo data or golden configuration data.</li>
<li>Automated testing of data entities.</li>
</ul>
</li>
<li><strong>Approach:</strong>
<ol>
<li>Identify data tasks that can benefit from automation.</li>
<li>Define tasks in an XML manifest.</li>
<li>Store data packages in the Shared asset library of Lifecycle Services.</li>
<li>Run data tasks and review outcomes using the Data Task Automation Manager.</li>
</ol>
</li>
</ul>
<p><strong>Lab Exercises:</strong></p>
<p>The text includes instructions for two lab exercises:</p>
<ol>
<li><strong>Explore the Data Management Workspace:</strong> Familiarize yourself with the Data Management workspace, create a new template, and explore various features.</li>
<li><strong>Export Data Using the Data Management Workspace:</strong> Create a data project to export customer data and run the export job.</li>
</ol>
<p>In essence, this section provides a comprehensive overview of testing and validation in Dynamics 365 Finance and Operations, with a focus on using Task Recorder, BPM, and Azure DevOps. It covers creating test libraries, recording test cases, synchronizing with Azure DevOps, running manual and automated tests, and using RSAT. It also introduces data task automation for automating data-related tasks. The lab exercises provide practical experience with the Data Management workspace and exporting data.</p>
<p>Let's conclude by reviewing the key aspects of preparing for go-live, completing the Lifecycle Services methodology, performing User Acceptance Testing (UAT), understanding the FastTrack go-live assessment, requesting the production environment, and exploring the go-live process. We'll also touch upon data migration, testing business functionality, and working with BPM, Task Recorder, and Azure DevOps.</p>
<p><strong>Preparing for Go-Live:</strong></p>
<ul>
<li><strong>Update Go-Live Date in Lifecycle Services:</strong> Keep milestone dates up-to-date throughout the project implementation.</li>
<li><strong>Complete and Send Pre-Go-Live Checklist:</strong> Complete all UATs and obtain sign-off from key stakeholders. The checklist is available on the FastTrack Resources page.</li>
<li><strong>Project Assessment (FastTrack Essentials):</strong> A FastTrack Architect reviews the checklist and provides feedback.</li>
<li><strong>Project Workshop (FastTrack):</strong> A FastTrack Architect coordinates activities for the assessment.</li>
<li><strong>Release for Production Deployment:</strong> Submit the production deployment request after the assessment is successfully completed.</li>
<li><strong>Request Production Deployment:</strong> The customer requests production deployment through Lifecycle Services.
<ul>
<li>Automatic sizing is based on the subscription estimate.</li>
<li>Manual sizing is based on an exception.</li>
<li>The Dynamics Service Engineering (DSE) team deploys the environment (up to 48 hours).</li>
</ul>
</li>
<li><strong>Submit Deployable Package Installation Request:</strong> Applying packages causes system downtime. Plan accordingly.</li>
<li><strong>Submit Request to Copy Database from Sandbox (if applicable):</strong> Follow the instructions in "Refresh database."</li>
<li><strong>Production Ready:</strong> You and the customer take control of the production environment.</li>
<li><strong>Go-Live:</strong> The customer goes live with Finance and Operations in the production environment.</li>
</ul>
<p><strong>Completing the Lifecycle Services Methodology:</strong></p>
<ul>
<li>Complete all phases and steps in the Lifecycle Services methodology before requesting the production environment.</li>
<li>Complete steps as you progress through the implementation, rather than waiting until the last minute.</li>
<li>Be thorough and ensure all requirements are met.</li>
</ul>
<p><strong>Performing User Acceptance Testing (UAT):</strong></p>
<ul>
<li>Test all implemented business processes and customizations in a sandbox environment.</li>
<li>Considerations:**
<ul>
<li>Test cases should cover the entire scope of requirements.</li>
<li>Test using migrated data (master data and opening balances).</li>
<li>Test using the correct security roles.</li>
<li>Ensure compliance with company-specific and industry-specific regulations.</li>
<li>Document all features and obtain customer approval/sign-off.</li>
</ul>
</li>
<li><strong>Note:</strong> Testing on local VMs or privately hosted VMs is not considered complete UAT.</li>
</ul>
<p><strong>FastTrack Go-Live Assessment:</strong></p>
<ul>
<li><strong>Purpose:</strong> A review conducted by the Microsoft FastTrack team to assess project readiness for go-live.</li>
<li><strong>Requirement:</strong> All Finance and Operations customers must complete a go-live assessment before their production environment can be deployed.</li>
<li><strong>Process:</strong>
<ol>
<li>About eight weeks before go-live, the FastTrack team will ask you to fill in a go-live checklist.</li>
<li>A Microsoft Solution Architect reviews the project and provides an assessment (potential risks, best practices, recommendations).</li>
<li>The Solution Architect may highlight risk factors and request a mitigation plan.</li>
<li>Upon successful completion, the Solution Architect indicates readiness for requesting the production environment.</li>
</ol>
</li>
<li><strong>Note:</strong> If you request the production environment before the assessment is completed, deployment will be queued until the assessment is successful.</li>
</ul>
<p><strong>Requesting the Production Environment:</strong></p>
<ul>
<li><strong>Timing:</strong> Request the production environment after completing the analysis, design and develop, and test phases in Lifecycle Services and after the go-live assessment is successful.</li>
<li><strong>Admin User:</strong> Use a service account (generic user account) as the Admin user for deployed environments.</li>
<li><strong>Data Center:</strong> Deploy the production environment to the same data center as your sandbox environments.</li>
<li><strong>Deployment:</strong> Microsoft deploys the production environment within 48 hours of the request (subject to the usage profile and any additional information needed).</li>
<li><strong>Status:</strong> The status in Lifecycle Services will change from "Queued" to "Deploying."</li>
<li><strong>Service Request:</strong> A service request is automatically created for the DSE team.</li>
<li><strong>Clearing Sign-off:</strong> You can clear the sign-off on a deployment request while it's in the "Queued" state to make changes to the configuration.</li>
</ul>
<p><strong>Data Migration:</strong></p>
<ul>
<li><strong>Planning:</strong>
<ul>
<li>Develop a detailed plan.</li>
<li>Ensure proper system permissions.</li>
<li>Back up the database and source files.</li>
</ul>
</li>
<li><strong>Process:</strong>
<ol>
<li>Extract data from the source system.</li>
<li>Migrate data to Finance and Operations.</li>
<li>Clean and transform the data.</li>
<li>Load data into the Data Management Framework.</li>
<li>Map fields.</li>
<li>Import data.</li>
</ol>
</li>
<li><strong>Monitoring:</strong> Continuously monitor the migration process.</li>
<li><strong>Validation:</strong>
<ul>
<li>Verify data accuracy and completeness.</li>
<li>Perform testing (unit, system, regression).</li>
<li>Compare migrated data with the legacy system using reports and inquiries.</li>
<li>Involve key stakeholders in validation.</li>
</ul>
</li>
</ul>
<p><strong>Identifying Relevant (Legacy) Systems:</strong></p>
<ul>
<li>Understand the customer's current systems and data formats.</li>
<li>Plan for data extraction from legacy systems.</li>
</ul>
<p><strong>Identifying and Importing Static Data:</strong></p>
<ul>
<li>Identify common static data (e.g., country codes, postal codes).</li>
<li>Import static data before the main data migration to reduce complexity and cost.</li>
</ul>
<p><strong>Generating Field Mapping:</strong></p>
<ul>
<li>Identify the source data format.</li>
<li>Use the "Configure data source" tile in the Data Management workspace.</li>
<li>Finance and Operations automatically creates field mappings when you add an entity to a data project.</li>
<li>Use the "Mapping details" tab to:
<ul>
<li>Ignore blank values.</li>
<li>Select a text identifier.</li>
<li>Use the label for enumerator fields.</li>
</ul>
</li>
</ul>
<p><strong>Supporting the Transition:</strong></p>
<ul>
<li>Validate migrated data.</li>
<li>Understand how each system (legacy and Finance and Operations) works and how data is structured.</li>
<li>Focus on data assurance.</li>
</ul>
<p><strong>Testing Business Functionality:</strong></p>
<ul>
<li><strong>BPM:</strong>
<ul>
<li>Used to describe a hierarchy of business processes and user tasks.</li>
<li>Create and manage BPM libraries in Lifecycle Services.</li>
<li>Can be synchronized with Azure DevOps to create test cases.</li>
</ul>
</li>
<li><strong>Task Recorder:</strong>
<ul>
<li>Used to record test cases.</li>
<li>Save recordings to BPM or to your PC (AXTR files).</li>
<li>Upload AXTR files to BPM.</li>
</ul>
</li>
<li><strong>Azure DevOps:</strong>
<ul>
<li>Synchronize BPM libraries with Azure DevOps projects.</li>
<li>Create test plans and test suites.</li>
<li>Add test cases to test suites using queries.</li>
<li>Run manual or automated tests.</li>
<li>Investigate test runs and results.</li>
</ul>
</li>
<li><strong>RSAT:</strong>
<ul>
<li>Automates the running of test cases based on task recordings.</li>
<li>Integrated with Azure DevOps.</li>
<li>Test parameters are stored in Excel files.</li>
<li>Can be run on a trial basis without an Azure DevOps license.</li>
<li>Supports parallel execution of test cases.</li>
</ul>
</li>
<li><strong>Data Task Automation:</strong>
<ul>
<li>Automates data-related tasks (e.g., creating data projects, running import/export operations).</li>
<li>Tasks are defined in an XML manifest.</li>
<li>Data packages are stored in the Shared asset library of Lifecycle Services.</li>
<li>Run tasks and review outcomes using the Data Task Automation Manager.</li>
</ul>
</li>
</ul>
<p><strong>Lab Exercises:</strong></p>
<p>The text includes instructions for lab exercises that involve:</p>
<ul>
<li>Creating a BPM library and adding a process.</li>
<li>Recording a test case using Task Recorder.</li>
<li>Saving the task recording to BPM.</li>
</ul>
<p>In essence, this section provides a comprehensive overview of the go-live process, including pre-go-live activities, requesting the production environment, and completing the Lifecycle Services methodology. It also covers UAT, the FastTrack go-live assessment, data migration best practices, and using tools like Task Recorder, BPM, and Azure DevOps for testing and validation. The information is essential for ensuring a smooth and successful go-live for Finance and Operations implementations.</p>
<p>Let's continue our discussion, focusing on the remaining aspects of data integration and management, including importing/exporting data, data transformation, database movement operations, data sharing, and other data management tools. We'll also cover financial reporting, report types and inquiries, and integrating with Microsoft Office, Logic Apps, and Azure Machine Learning.</p>
<p><strong>Data Import and Export Jobs:</strong></p>
<ul>
<li><strong>Data Management Workspace:</strong> Used to create and manage data import and export jobs.</li>
<li><strong>Staging Tables:</strong> Used to verify, clean, or convert data before moving it to target tables.</li>
<li><strong>Steps to Create an Import or Export Job:</strong>
<ol>
<li>Define the project category.</li>
<li>Identify the entities to import or export.</li>
<li>Set the data format for the job.</li>
<li>Sequence the entities logically.</li>
<li>Determine whether to use staging tables.</li>
<li>Validate that the source and target data are mapped correctly.</li>
<li>Verify security for the job.</li>
<li>Run the import or export job.</li>
<li>Validate that the job ran as expected.</li>
<li>Clean up the staging tables.</li>
</ol>
</li>
</ul>
<p><strong>Creating an Import or Export Job:</strong></p>
<ul>
<li>Can be run one time or multiple times.</li>
<li>Project categories help manage related jobs.</li>
<li>Select the data format (e.g., CSV, Excel, Package).</li>
<li>Sequence entities to address functional dependencies.</li>
<li>Use the "Entity sequence" button in Standard view to define the order.</li>
<li>"Auto sequence" can automatically determine the sequence.</li>
<li>Example: Sales tax codes and details should be loaded before sales tax groups.</li>
<li>Example: Entities without dependencies can be in their own execution unit and load immediately.</li>
</ul>
<p><strong>Security for Import/Export Jobs:</strong></p>
<ul>
<li><strong>Applicable Roles:</strong> Restrict job access to specific security roles.</li>
<li><strong>Applicable Users:</strong> Restrict job access to specific users within a role.</li>
<li><strong>Applicable Legal Entities:</strong> Control access to jobs based on legal entities.</li>
</ul>
<p><strong>Cleaning Up Staging Tables:</strong></p>
<ul>
<li>Use the "Staging clean up" feature in the Data Management workspace.</li>
<li>Options for deleting records:
<ul>
<li><strong>Entity:</strong> Deletes all records from the entity's staging table.</li>
<li><strong>Job ID:</strong> Deletes records for all entities in the selected job.</li>
<li><strong>Data projects:</strong> Deletes records for all entities and jobs in the selected data project.</li>
</ul>
</li>
</ul>
<p><strong>Data Transformation:</strong></p>
<p>Data transformation can be performed between the source and the staging table, or between the staging table and the target table.</p>
<ul>
<li><strong>Transformation between Source and Staging:</strong>
<ul>
<li>Accessed by selecting the "View map" icon when creating an import/export project.</li>
<li><strong>Mapping Visualization:</strong> Shows the mapping between source and staging fields.</li>
<li><strong>Mapping Details:</strong> Provides options for basic transformations:
<ul>
<li>Ignore blank values.</li>
<li>Text qualifier.</li>
<li>Use enum label.</li>
<li>Auto-generated.</li>
<li>Auto default.</li>
<li>Conversion (value mapping).</li>
</ul>
</li>
</ul>
</li>
<li><strong>Transformation between Staging and Target:</strong>
<ul>
<li>Accessed by selecting "Modify target mapping" for a data entity in the Data Management workspace.</li>
<li><strong>Mapping Details:</strong> Shows the mapping between staging and target fields.</li>
<li><strong>Computed Columns:</strong>
<ul>
<li>Use SQL view computed columns for calculations.</li>
<li>More efficient than virtual fields.</li>
<li>Primarily for read operations.</li>
</ul>
</li>
<li><strong>Virtual Fields:</strong>
<ul>
<li>Non-persisted fields controlled by custom X++ code.</li>
<li>Used for complex calculations or transformations.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>Database Movement Operations:</strong></p>
<ul>
<li><strong>Purpose:</strong> A suite of self-service actions for Data Application Lifecycle Management (DataALM).</li>
<li><strong>Features:</strong>
<ul>
<li>Run database operations between Tier-2+ environments in Lifecycle Services.</li>
<li>Create requests for database operations for production environments.</li>
<li>Refresh database (copy data from one environment to another).</li>
<li>Export database (export a database to the Asset Library).</li>
<li>Import database (import a database from a developer environment or a previously exported database).</li>
<li>Point-in-time restore (PITR) for sandbox and production environments.</li>
</ul>
</li>
<li><strong>Database Movement Operations between Tier-1 and Tier-2+:</strong>
<ul>
<li>Bacpac procedure (SQL Server to Azure SQL or vice-versa).</li>
<li>SQL Server backup/restore between one-box environments.</li>
</ul>
</li>
</ul>
<p><strong>Data Sharing Framework:</strong></p>
<ul>
<li><strong>Purpose:</strong> Replicates (shares) reference and group data among companies within the same deployment.</li>
<li><strong>Features:</strong>
<ul>
<li>Data integrity is verified before replication.</li>
<li>Immediate replication of changes across companies.</li>
<li>Fields not selected for sharing are maintained separately in each company.</li>
<li>Existing records can be copied when enabling a policy.</li>
</ul>
</li>
<li><strong>Limitations:</strong>
<ul>
<li>Cannot be used for transactional data.</li>
<li>Cannot be used with dual-write.</li>
<li>Not supported for complex scenarios (e.g., subtype/supertype tables, tables with date effectivity rules).</li>
<li>Not supported for tables without a unique index.</li>
</ul>
</li>
</ul>
<p><strong>When to Use Data Sharing:</strong></p>
<ul>
<li>Sharing simple reference and group data in a single deployment.</li>
<li>Sharing among companies with similar configurations.</li>
<li>Sharing scenarios explicitly evaluated by Microsoft.</li>
</ul>
<p><strong>When Not to Use Data Sharing:</strong></p>
<ul>
<li>Franchising solutions with thousands of records shared across many companies.</li>
<li>Sharing transactional records for reporting or management purposes.</li>
<li>Sharing across deployments.</li>
</ul>
<p><strong>Other Data Management Tools:</strong></p>
<ul>
<li><strong>Bring Your Own Database (BYOD):</strong> Export entities to your own Azure SQL database.</li>
<li><strong>Entity Store Refresh:</strong> For embedded Power BI reports.</li>
<li><strong>Process Data Package:</strong> Consolidates multiple data packages into one bundle.</li>
<li><strong>Data Validation Checklist Workspace:</strong> Tracks data validation processes.</li>
<li><strong>Office Add-in:</strong> Export and import data via Excel add-in using data entities.</li>
</ul>
<p><strong>Standard vs. Enhanced Views:</strong></p>
<ul>
<li><strong>Standard View:</strong> Streamlined interface with basic functionality.</li>
<li><strong>Enhanced View:</strong> Provides more detailed information and controls.</li>
<li><strong>Entity Sequence:</strong> The "Entity sequence" button is only available in the Standard view.</li>
</ul>
<p><strong>Using Templates in Data Management:</strong></p>
<ul>
<li><strong>Default Templates:</strong> Provided with each new release of Finance and Operations.</li>
<li><strong>Loading Templates:</strong> In the Data Management workspace, select the "Templates" tile and then "Load default templates" (available in Enhanced view).</li>
<li><strong>Customization:</strong> You can modify the default templates to suit your business requirements.</li>
<li><strong>Merging Templates:</strong> You can create larger templates that cover multiple modules or merge smaller templates into a data project.</li>
<li><strong>Overriding Templates:</strong> If you add a template that contains an entity already in a data project, the entity in the template replaces the existing entity.</li>
</ul>
<p><strong>Change Tracking for Entities:</strong></p>
<ul>
<li><strong>Purpose:</strong> Enables incremental export of data by tracking changes to records.</li>
<li><strong>Options:</strong>
<ul>
<li><strong>Enable primary table:</strong> Tracks changes in the primary table.</li>
<li><strong>Enable entire entity:</strong> Tracks changes in all tables within the entity.</li>
<li><strong>Enable custom query:</strong> Tracks changes based on a custom query.</li>
</ul>
</li>
<li><strong>BYOD:</strong> Change tracking can also track deletions for BYOD scenarios if the entity supports it.</li>
</ul>
<p><strong>Export, Import, and Copy Data into a Legal Entity:</strong></p>
<ul>
<li><strong>Data Import/Export Jobs:</strong> Use the Data Management workspace to create and manage import/export jobs.</li>
<li><strong>Copy into Legal Entity:</strong> Use this option to copy data between legal entities within the same Finance and Operations instance.</li>
</ul>
<p><strong>Financial Reporting:</strong></p>
<ul>
<li><strong>Purpose:</strong> Create, maintain, and view financial statements (e.g., balance sheets, income statements, cash flow statements).</li>
<li><strong>Features:</strong>
<ul>
<li>Dimension support for filtering and analysis.</li>
<li>Custom reporting structures.</li>
<li>Mapping of main accounts, dimensions, or a combination of both.</li>
<li>Report Designer for creating and modifying reports.</li>
<li>Report scheduling and distribution.</li>
<li>Export to PDF or Excel.</li>
<li>Interactive features (e.g., changing report date, currency, filtering).</li>
</ul>
</li>
<li><strong>Financial Reporting Database:</strong> Deployed to its own SQL Azure database.</li>
<li><strong>Accessing Financial Reporting:</strong> Found in various modules (e.g., General Ledger, Budgeting, Consolidations).</li>
<li><strong>Financial Reporting Setup:</strong> Configure dimensions and attributes for reporting.</li>
<li><strong>Components:</strong>
<ul>
<li><strong>Report Designer:</strong> Create and manage report building blocks.</li>
<li><strong>Report Schedules:</strong> Schedule report generation.</li>
</ul>
</li>
<li><strong>Default Reports:</strong> Includes balance sheet and trial balance reports.</li>
<li><strong>Report Retention Policies:</strong> Set expiration dates for reports (90 days or never).</li>
<li><strong>Row Definition:</strong> Defines the contents of each row on a report.</li>
<li><strong>Column Definition:</strong> Defines the contents of each column on a report.</li>
<li><strong>Reporting Tree Definition:</strong> Defines the structure and hierarchy of the organization for reporting.</li>
</ul>
<p><strong>Inquiries and Reports:</strong></p>
<ul>
<li><strong>Availability:</strong> Each module typically includes an "Inquiries and Reports" menu with predefined tools for accessing and analyzing data.</li>
<li><strong>Functionality:</strong> View detailed transactional records, summary data, and system-generated analytics.</li>
</ul>
<p><strong>Generating and Consuming Inquiries and Reports:</strong></p>
<ul>
<li><strong>Exporting Reports:</strong>
<ul>
<li>Export as Word documents using RDL-based designs.</li>
<li>Export to PDF, CSV, and other Office formats.</li>
</ul>
</li>
<li><strong>Previewing Documents:</strong>
<ul>
<li>Export options while previewing.</li>
<li>Send documents to network printers.</li>
</ul>
</li>
</ul>
<p><strong>Integrating with Microsoft Office:</strong></p>
<ul>
<li><strong>Excel Integration:</strong>
<ul>
<li>Export data to Excel for analysis using pivot tables, charts, etc.</li>
<li>Use the Excel Data Connector add-in to view, update, and edit data.</li>
<li>"Open in Excel" uses data entities and allows publishing changes back to Finance and Operations.</li>
<li>"Export to Excel" exports all grid data to Excel.</li>
</ul>
</li>
</ul>
<p><strong>Automating Processes with Logic Apps:</strong></p>
<ul>
<li><strong>Purpose:</strong> Automate tasks, business processes, and workflows.</li>
<li><strong>Features:</strong>
<ul>
<li>Connectors for various services (e.g., Azure services, Microsoft 365, Dynamics 365, SAP).</li>
<li>Triggers and actions for creating workflows.</li>
<li>Support for encryption and digital signatures.</li>
</ul>
</li>
<li><strong>File System Connector:</strong> Access files on a local or network file system.</li>
<li><strong>Azure Blob Storage Connector:</strong> Access files in Azure Blob Storage.</li>
<li><strong>Finance and Operations Connectors:</strong> Perform actions in Finance and Operations (e.g., create, delete, get records).</li>
</ul>
<p><strong>Consuming External Web Services:</strong></p>
<ul>
<li><strong>Wrapper Classes:</strong> Create wrapper classes (e.g., in C#) to encapsulate the logic for interacting with external web services.</li>
<li><strong>Service References:</strong> Add service references in Visual Studio to generate proxy classes for SOAP or REST services.</li>
<li><strong>HTTPClient (REST) or WCF (SOAP):</strong> Use .NET tools to make HTTP requests and handle responses.</li>
<li><strong>Authentication:</strong> Use appropriate authentication mechanisms (e.g., API keys, OAuth 2.0).</li>
<li><strong>Error Handling:</strong> Implement robust error handling to manage exceptions.</li>
<li><strong>Security:</strong> Store sensitive information (e.g., API keys) securely in Azure Key Vault.</li>
</ul>
<p><strong>Integrating with Power Platform:</strong></p>
<ul>
<li><strong>Power Automate:</strong> Automate workflows and processes.</li>
<li><strong>Power BI:</strong> Perform data analysis and visualization.</li>
<li><strong>Power Apps:</strong> Build custom business applications.</li>
</ul>
<p><strong>Connecting to Power Automate:</strong></p>
<ol>
<li>Sign in to Power Automate.</li>
<li>Go to Connections and select "New connection."</li>
<li>Choose the desired connection (e.g., Finance and Operations) and click the plus sign (+).</li>
<li>Select "Create."</li>
<li>Enter your credentials to configure the connection.</li>
</ol>
<p><strong>Connecting to Power BI Desktop:</strong></p>
<ol>
<li>In Power BI Desktop, select "Get Data" from the Home ribbon.</li>
<li>Choose "Other" and then select "OData Feed."</li>
<li>Click "Connect."</li>
<li>Enter the OData endpoint URL for your Finance and Operations instance.</li>
<li>Select the data you need and click "Load."</li>
</ol>
<p><strong>Connecting to Power Apps:</strong></p>
<ol>
<li>When creating a new Power App, select the "Start from data" option.</li>
<li>Click the right arrow to show more connection options.</li>
<li>Click "New Connection".</li>
<li>Select "Dynamics 365 for Fin &amp; Ops."</li>
<li>Click "Create."</li>
<li>Choose the Finance and Operations instance and the data entity you want to connect to. You will need admin access to the environment and the proper permissions.</li>
</ol>
<p><strong>Embedding Power Apps and Third-Party Apps in Finance and Operations:</strong></p>
<ul>
<li><strong>Embedding Power Apps:</strong>
<ol>
<li>Select the Power Apps icon in the top-right corner of a page.</li>
<li>Select "Add an app."</li>
<li>Enter the app name and ID.</li>
<li>Specify the input context and application size.</li>
<li>Optionally, specify legal entity access.</li>
<li>Select "Insert."</li>
</ol>
</li>
<li><strong>Embedding Third-Party Apps:</strong>
<ul>
<li>Requires development.</li>
<li>Use the website host control in Visual Studio to add third-party apps to Finance and Operations pages.</li>
</ul>
</li>
<li><strong>Controlling Embedding:</strong>
<ul>
<li>"isPowerAppPersonalizationEnabled": Controls whether users can embed Power Apps on a page.</li>
<li>"isPowerAppTabPersonalizationEnabled": Controls whether users can embed Power Apps as tabs, FastTabs, or sections.</li>
</ul>
</li>
</ul>
<p><strong>Common Data Model and Dataverse:</strong></p>
<ul>
<li><strong>Common Data Model (CDM):</strong> A standardized data model that simplifies data integration across applications.</li>
<li><strong>Dataverse:</strong> A data platform that uses the CDM and provides a unified view of data.</li>
<li><strong>Integration:</strong> Finance and Operations data can be integrated with Dataverse, enabling data sharing and synchronization with other Dynamics 365 applications and Power Platform.</li>
<li><strong>Dual-Write:</strong> Enables near real-time, bidirectional data synchronization between Finance and Operations and model-driven apps in Dynamics 365.</li>
</ul>
<p><strong>Example Scenario:</strong></p>
<p>A company using Dynamics 365 for Finance and Dynamics 365 Sales can use the CDM to standardize data structures (e.g., Account entity) across both applications, simplifying data integration and enabling a unified view of customer information.</p>
<p><strong>Enabling Dual-Write:</strong></p>
<ol>
<li>Link your Finance and Operations environment to a Dataverse environment using the Dual-write wizard in the Data Management workspace.</li>
<li>Apply the dual-write application orchestration solution.</li>
<li>Enable the desired entity maps for synchronization.</li>
</ol>
<p><strong>Lab - Explore the Data Management Workspace</strong><br>
This lab guides you through exploring the Data Management workspace, including:</p>
<ul>
<li>Viewing execution history messages.</li>
<li>Navigating to Templates and adding a new template.</li>
<li>Validating a template.</li>
<li>Viewing available data entities.</li>
<li>Exploring entity structure, target fields, and different views (model and category).</li>
<li>Working with the Data task automation tile.</li>
<li>Configuring data sources.</li>
</ul>
<p><strong>Lab - Export Data Using the Data Management Workspace</strong><br>
This lab demonstrates how to export data using the Data Management workspace, including:</p>
<ul>
<li>Creating an export project.</li>
<li>Specifying export settings (group name, description, data project operation type, project category, generate data package).</li>
<li>Adding entities to the project.</li>
<li>Defining entity-specific settings (entity name, target data format, skip staging, default refresh type, field selection).</li>
<li>Running the export job.</li>
<li>Downloading the exported data package.</li>
</ul>
<p>In conclusion, this comprehensive overview covers various aspects of data integration and management in Dynamics 365 Finance and Operations, including data migration best practices, the Data Management Framework, data entities, change tracking, composite and aggregate data entities, BYOD, Azure Data Lake integration, testing, and integration with other services like Excel, Logic Apps, and Power Platform. The detailed explanations, examples, and lab instructions provide a solid foundation for understanding and implementing effective data integration and management strategies in Finance and Operations.</p>
<p>Let's wrap up our exploration of Dynamics 365 Finance and Operations by delving into reporting, inquiries, financial reporting, integration with Microsoft Power Platform and Office, and Electronic Reporting (ER).</p>
<p><strong>Reports and Inquiries:</strong></p>
<ul>
<li><strong>Definition:</strong> A report is a structured presentation of data that helps users make informed decisions.</li>
<li><strong>Types of Reports and Inquiries:</strong>
<ul>
<li><strong>Embedded Analytics and Native Controls:</strong> Built-in visualizations (charts, grids) used in operational reports and workspaces.</li>
<li><strong>SQL Server Reporting Services (SSRS):</strong> Primarily for business documents and high-volume printing (e.g., invoices, packing slips).</li>
<li><strong>Financial Report Designer:</strong> Creates and customizes financial statements (e.g., balance sheets, income statements).</li>
<li><strong>Microsoft Power Platform:</strong> Dashboards, workspaces, Power BI, Power Apps, and Power Automate integrations.</li>
<li><strong>Electronic Reporting:</strong> Configures formats for incoming and outgoing electronic documents.</li>
<li><strong>Microsoft Office Integrations:</strong> Export data to Excel for further analysis and reporting.</li>
</ul>
</li>
</ul>
<p><strong>Financial Reporting:</strong></p>
<ul>
<li><strong>Purpose:</strong> Create, maintain, and view financial statements.</li>
<li><strong>Features:</strong>
<ul>
<li>Dimension support for filtering and analysis.</li>
<li>Custom reporting structures.</li>
<li>Mapping of main accounts, dimensions, or a combination of both.</li>
</ul>
</li>
<li><strong>Report Designer:</strong>
<ul>
<li>Save and reuse dimension combinations.</li>
<li>Control formatting and display of dimension descriptions.</li>
<li>Identify omitted accounts or dimensions.</li>
<li>Format headers for rolling forecasts.</li>
<li>Generate reports automatically on a schedule.</li>
<li>Export to PDF or Excel.</li>
<li>Change report date and currency.</li>
<li>View in summary or detailed view.</li>
<li>Add dimension and attribute filters.</li>
</ul>
</li>
<li><strong>Financial Reporting Database:</strong> Deployed to a separate SQL Azure database.</li>
<li><strong>Accessing Financial Reporting:</strong> Found in General Ledger, Budgeting, and Consolidations modules.</li>
<li><strong>Financial Reporting Setup:</strong> Configure dimensions and attributes for reporting.</li>
<li><strong>Components:</strong>
<ul>
<li><strong>Report Designer:</strong> Create and manage report building blocks.</li>
<li><strong>Report Schedules:</strong> Schedule report generation.</li>
</ul>
</li>
<li><strong>Default Reports:</strong> Includes balance sheet and trial balance reports.</li>
<li><strong>Report Retention Policies:</strong> Set expiration dates for reports (90 days or never).</li>
<li><strong>Row Definition:</strong> Defines the contents of each row on a report.</li>
<li><strong>Column Definition:</strong> Defines the contents of each column on a report.</li>
<li><strong>Reporting Tree Definition:</strong> Defines the structure and hierarchy of the organization for reporting.</li>
</ul>
<p><strong>Inquiries and Reports:</strong></p>
<ul>
<li><strong>Availability:</strong> Each module typically includes an "Inquiries and Reports" menu with predefined tools for accessing and analyzing data.</li>
<li><strong>Functionality:</strong> View detailed transactional records, summary data, and system-generated analytics.</li>
</ul>
<p><strong>Generating and Consuming Inquiries and Reports:</strong></p>
<ul>
<li><strong>Exporting Reports:</strong>
<ul>
<li>Export as Word documents using RDL-based designs.</li>
<li>Export to PDF, CSV, and other Office formats.</li>
</ul>
</li>
<li><strong>Previewing Documents:</strong>
<ul>
<li>Export options while previewing.</li>
<li>Send documents to network printers.</li>
</ul>
</li>
</ul>
<p><strong>Integrating with Microsoft Office:</strong></p>
<ul>
<li><strong>Excel Integration:</strong>
<ul>
<li>Export data to Excel for analysis using pivot tables, charts, etc.</li>
<li>Use the Excel Data Connector add-in to view, update, and edit data.</li>
<li>"Open in Excel" uses data entities and allows publishing changes back to Finance and Operations.</li>
<li>"Export to Excel" exports all grid data to Excel.</li>
</ul>
</li>
</ul>
<p><strong>Configuring for Microsoft Power Platform:</strong></p>
<ul>
<li><strong>Power Apps:</strong>
<ul>
<li>Embed Power Apps in Finance and Operations using the "Personalize" toolbar.</li>
<li>Build apps with a point-and-click approach.</li>
<li>Connect to various data sources, including Dataverse and Finance and Operations.</li>
</ul>
</li>
<li><strong>Power Automate:</strong>
<ul>
<li>Automate workflows and processes.</li>
<li>Connect to hundreds of apps and services.</li>
<li>Use the "When a Business Event occurs" trigger for Finance and Operations.</li>
</ul>
</li>
<li><strong>Power BI:</strong>
<ul>
<li>Unify data from multiple sources.</li>
<li>Create interactive dashboards and reports.</li>
<li>Several pre-built Power BI solutions are available for Finance and Operations (e.g., Financial analysis, Cost accounting analysis, Sales and profitability performance).</li>
</ul>
</li>
</ul>
<p><strong>Lab - Work with Reports:</strong></p>
<p>This lab demonstrates how to:</p>
<ul>
<li>Display the order of dimensions for financial reporting.</li>
<li>Generate a financial report.</li>
<li>Explore data in the report (drill into details, filter by dimensions, change currency).</li>
</ul>
<p><strong>Electronic Reporting (ER):</strong></p>
<ul>
<li><strong>Purpose:</strong> Configure formats for incoming and outgoing electronic documents to comply with legal requirements.</li>
<li><strong>Target Audience:</strong> Advanced business users (configuration, not code).</li>
<li><strong>Features:</strong>
<ul>
<li>Single tool for electronic reporting.</li>
<li>Version-independent formats.</li>
<li>Custom formats based on other formats.</li>
<li>Automatic upgrades of custom formats.</li>
<li>Support for localization.</li>
<li>Distribution of formats through Lifecycle Services.</li>
</ul>
</li>
<li><strong>Components:</strong>
<ul>
<li>Data models and model-mapping components.</li>
<li>Format components for outgoing and incoming documents.</li>
<li>Component versioning, date effectivity, and access.</li>
</ul>
</li>
<li><strong>Configuration:</strong>
<ul>
<li>Provider.</li>
<li>Repository.</li>
</ul>
</li>
</ul>
<p><strong>Setting up ER:</strong></p>
<ol>
<li>Set up document types in Document Management.</li>
<li>Open the Electronic Reporting workspace.</li>
<li>Configure ER parameters.</li>
</ol>
<p><strong>Using Logic Apps for EAI, B2B, EDI, and Business Processes:</strong></p>
<ul>
<li><strong>Enterprise Integration Pack (EIP):</strong> Build logic apps to convert messages between different formats (e.g., EDIFACT, AS2, X12).</li>
<li><strong>Security:</strong> Supports encryption and digital signatures.</li>
<li><strong>File System Connector:</strong> Access files on a local or network file system.
<ul>
<li>Triggers: Detect when files are added or modified.</li>
<li>Recurrence Trigger: Aggregate changes periodically.</li>
</ul>
</li>
<li><strong>Azure Blob Storage Connector:</strong> Access files in Azure Blob Storage.
<ul>
<li>"Get blob content": Retrieve file content.</li>
</ul>
</li>
<li><strong>Finance and Operations Connectors:</strong> Perform actions in Finance and Operations (e.g., create, delete, get records).</li>
</ul>
<p><strong>Consuming Azure Machine Learning Services:</strong></p>
<ul>
<li><strong>Seamless Integration:</strong> Finance and Operations integrates with Azure Machine Learning (e.g., Demand Forecasting in Dynamics 365 Supply Chain Management).</li>
<li><strong>Enabled by Default:</strong> The feature is enabled by default but can be manually disabled.</li>
</ul>
<p><strong>Enabling Connectivity with External Services:</strong></p>
<ul>
<li><strong>Register Services with Microsoft Entra ID:</strong>
<ol>
<li>Go to the Microsoft Entra ID tab of the Azure portal.</li>
<li>Note the tenant ID.</li>
<li>Select "App registrations."</li>
<li>Select "New registration."</li>
<li>Enter a name and select a supported account type.</li>
<li>Enter the redirect URI (if applicable).</li>
<li>Select "Register."</li>
</ol>
</li>
<li><strong>Register the Service in Finance and Operations:</strong>
<ol>
<li>Go to System administration &gt; Setup &gt; Microsoft Entra ID applications.</li>
<li>Select "New."</li>
<li>Enter the application ID, name, and an appropriate service account user ID.</li>
<li>Save the record.</li>
</ol>
</li>
</ul>
<p><strong>Optimizing Data Entities:</strong></p>
<ul>
<li><strong>Set-based Processing:</strong> Improves performance for entities that are not composite and don't require row-level logic.</li>
<li><strong>Splitting Input Files:</strong> Divide large input files into multiple parts to take advantage of multithreading.</li>
</ul>
<p><strong>Lab - Explore the Data Management Workspace (Continued):</strong></p>
<p>This lab continues the exploration of the Data Management workspace, including:</p>
<ul>
<li>Viewing the message "Execution history was successful."</li>
<li>Working with templates (creating, validating).</li>
<li>Viewing data entities.</li>
<li>Exploring entity structure, target fields, and different views (model and category).</li>
<li>Working with the Data task automation tile.</li>
<li>Configuring data sources.</li>
</ul>
<p><strong>Lab - Export Data Using the Data Management Workspace (Continued):</strong></p>
<p>This lab demonstrates how to export data using the Data Management workspace, including:</p>
<ul>
<li>Creating an export project.</li>
<li>Specifying export settings (group name, description, data project operation type, project category, generate data package).</li>
<li>Adding entities to the project.</li>
<li>Defining entity-specific settings (entity name, target data format, skip staging, default refresh type, field selection).</li>
<li>Running the export job.</li>
<li>Downloading the exported data package.</li>
</ul>
<p>In conclusion, this comprehensive overview covers various aspects of reporting, inquiries, and data integration in Dynamics 365 Finance and Operations. It highlights the different reporting tools available, including SSRS, Financial Report Designer, Power BI, and Excel integration. It also explains how to configure and use Electronic Reporting (ER) for managing electronic document formats. The section on integrating with Microsoft Power Platform demonstrates how to connect to Power Automate, Power BI, and Power Apps, and how to embed these applications within Finance and Operations. Additionally, it covers best practices for data migration, testing, and validation, as well as working with the Data Management Framework and its features. The lab exercises provide practical experience with exploring the Data Management workspace and exporting data. This information is essential for effectively managing data, generating reports, and integrating Finance and Operations with other systems and services.</p>
<p>Let's continue our discussion, focusing on Electronic Reporting (ER), configuring ER parameters, creating configuration providers, working with legal entity-specific parameters, using barcode data sources, validating check numbers, and generating electronic documents for payments. We'll also touch upon the lab exercises and provide answers to the knowledge check questions.</p>
<p><strong>Electronic Reporting (ER):</strong></p>
<ul>
<li><strong>Purpose:</strong> Configure formats for incoming and outgoing electronic documents to comply with legal requirements of various countries/regions.</li>
<li><strong>Target Audience:</strong> Business users (configuration, not code).</li>
<li><strong>Features:</strong>
<ul>
<li>Single tool for electronic reporting.</li>
<li>Version-independent formats.</li>
<li>Custom formats based on other formats.</li>
<li>Automatic upgrades of custom formats.</li>
<li>Support for localization.</li>
<li>Distribution of formats through Lifecycle Services.</li>
</ul>
</li>
<li><strong>Components:</strong>
<ul>
<li>Data models and model-mapping components.</li>
<li>Format components for outgoing and incoming documents.</li>
<li>Component versioning, date effectivity, and access.</li>
</ul>
</li>
<li><strong>Configuration:</strong>
<ul>
<li>Provider.</li>
<li>Repository.</li>
</ul>
</li>
</ul>
<p><strong>Configuring ER Parameters:</strong></p>
<ol>
<li><strong>Document Types:</strong> Set up required document types in Document Management:
<ul>
<li>Office documents (templates for reports).</li>
<li>Storage for output in the jobs archive.</li>
<li>Storage for output to be viewed in other programs.</li>
<li>Documents for keeping baselines of configuration output.</li>
<li>Documents for handling files in the framework.</li>
</ul>
</li>
<li><strong>Electronic Reporting Workspace:</strong>
<ul>
<li><strong>General Tab:</strong>
<ul>
<li>Enable design mode.</li>
</ul>
</li>
<li><strong>Attachments Tab:</strong>
<ul>
<li>Define document types for file storage.</li>
</ul>
</li>
<li><strong>Lifecycle Services Tab:</strong>
<ul>
<li>Define the number of parallel threads for loading configurations from Lifecycle Services repositories.</li>
</ul>
</li>
<li><strong>Configuration Providers:</strong>
<ul>
<li>Select a provider and set it as active.</li>
</ul>
</li>
</ul>
</li>
</ol>
<p><strong>Creating a New Configuration Provider:</strong></p>
<ol>
<li>Go to Organization administration &gt; Workspaces &gt; Electronic reporting.</li>
<li>Select "Configuration providers."</li>
<li>Select "New."</li>
<li>Enter the name and internet address.</li>
<li>Save and close the page.</li>
<li>Select the new provider and click "Set active."</li>
</ol>
<p><strong>Legal Entity-Specific Parameters:</strong></p>
<ul>
<li><strong>Purpose:</strong> Define rules for translating terms into supported languages and specify search data sources for legal entity-specific data.</li>
<li><strong>Features:</strong>
<ul>
<li>Specify terms and conditions in the user's chosen language.</li>
<li>Define corporate rules for translating terms.</li>
<li>Configure legal entity-independent rules to return enumeration data types or text values.</li>
</ul>
</li>
</ul>
<p><strong>Barcode Data Sources:</strong></p>
<ul>
<li><strong>Purpose:</strong> Generate barcode images for any text.</li>
<li><strong>Supported Formats:</strong>
<ul>
<li><strong>One-dimensional:</strong> Codabar, Code 39, Code 93, Code 128, EAN-8, EAN-13, ITF-14, Intelligent Mail (IMB), MSI, Plessey, PDF417, UPC-A, UPC-E.</li>
<li><strong>Two-dimensional:</strong> Aztec, Data Matrix, QR Code.</li>
</ul>
</li>
</ul>
<p><strong>Check Number Validation:</strong></p>
<ul>
<li><strong>Purpose:</strong> Eliminate the risk of errors in processing checks.</li>
<li><strong>Options:</strong>
<ul>
<li><strong>Interval Validation:</strong> Set up a default check number interval in Cash and Bank Management parameters.</li>
<li><strong>Character Validation:</strong> Triggers a warning if an invalid character appears in the check number.</li>
</ul>
</li>
</ul>
<p><strong>Lab - Generate Electronic Documents for Payments:</strong></p>
<p>This lab demonstrates how to:</p>
<ol>
<li>Verify the file format setup for electronic payments (e.g., NACHA for US).</li>
<li>Test the format of generated payment files by creating and processing a vendor payment journal.</li>
</ol>
<p><strong>Knowledge Check Questions and Answers:</strong></p>
<ol>
<li><strong>How many required document types must be set up before configuring ER functionality?</strong>
<ul>
<li><strong>Answer:</strong> Four (Office documents, storage for output in jobs archive, storage for output to be viewed in other programs, documents for baselines). The provided options are incorrect. The correct number is 4.</li>
</ul>
</li>
<li><strong>In which module can you access the Electronic Reporting workspace?</strong>
<ul>
<li><strong>Answer:</strong> Organization administration.</li>
</ul>
</li>
<li><strong>In which FastTab can providers be configured in the Electronic Reporting workspace?</strong>
<ul>
<li><strong>Answer:</strong> Configuration providers.</li>
</ul>
</li>
</ol>
<p>In essence, this section provides a comprehensive overview of Electronic Reporting (ER) in Dynamics 365 Finance and Operations. It covers the purpose, features, components, and configuration of ER. It also explains how to create configuration providers, work with legal entity-specific parameters, use barcode data sources, and validate check numbers. The lab exercise demonstrates how to generate electronic documents for payments. This information is essential for understanding and using ER to manage electronic document formats and comply with legal requirements.</p>
    </body>
</html>